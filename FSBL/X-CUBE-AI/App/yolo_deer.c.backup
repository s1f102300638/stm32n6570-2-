/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    yolo_deer.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "e619e8606099384540d70eeaaa8091752b1bebe9"
 * GIT_BRANCH      "STAI-2.2"
 * GIT_DESCRIPTION "atonn-v1.1.1-14-ge619e8606"
 *
 * BUILD_DIR       "/c/local/jenkins_cloud/workspace/2-STEDGEAI_BuildAtonnExe_Win/git/onnx_backend/build"
 * BUILD_DATE      "25/06/2025"
 * BUILD_AUTHOR    "aitest"
 *
 * Command Line options:
 * --load-mdesc-file = "C:/Users/iniad/STM32Cube/Repository/Packs/STMicroelectronics/X-CUBE-AI/10.2.0/scripts/N6_scripts/my_mdescs/stm32n6"
 * --load-mpool-file = "C:/Users/iniad/STM32Cube/Repository/Packs/STMicroelectronics/X-CUBE-AI/10.2.0/scripts/N6_scripts/my_mpools/stm32n6"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --native-float = true
 * --json-quant-file = "C:/Users/iniad/.stm32cubemx/network_output/best_OE_3_3_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --output-info-file = "c_info"
 * --onnx-input = "C:/Users/iniad/.stm32cubemx/network_output/best_OE_3_3_0.onnx"
 * --out-dir-prefix = "C:/Users/iniad/AppData/Local/Temp/mxAI_workspace11367109030250017807818597348598240/neural_art__yolo_deer/"
 * --network-name = "yolo_deer"
 * --all-buffers-info = true
 * --mvei = true
 * --Oauto-sched = true
 *
 * auto* option expanded into:
 *   alt-scheduler = true
 */
#ifndef INFINITY
#define INFINITY HUGE_VALF
#endif

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"
#include <math.h>


#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 1 || LL_ATON_VERSION_DEV != 14
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 7 is 3.12 MB */
/* index=7 file postfix=xSPI1 name=hyperRAM offset=0x90000000  absolute_mode size=33554424 READ_WRITE THROUGHPUT=MID LATENCY=HIGH byte width=2 freq ratio=5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=380 write_power=340 use4initializers=YES score=82  */
/* global pool 8 is 11.53 MB */
/* index=8 file postfix=xSPI2 name=octoFlash offset=0x71000000  absolute_mode size=117440504 READ_ONLY THROUGHPUT=MID LATENCY=HIGH byte width=1 freq ratio=6 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=ON read_power=110 write_power=400 use4initializers=YES score=50  */
/* global pool 1 is 448.00 KB */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is 448.00 KB */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is 448.00 KB */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is 432.00 KB */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */
/* global pool 11 is 2.73 MB */
/* index=11 file postfix=AXISRAM2_AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=cpuRAM2_npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34100000  absolute_mode size=2883576 vpool READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=85  */
/* global pool 4 is 1.00 MB */
/* index=4 file postfix=AXISRAM2 name=cpuRAM2 offset=0x34100000  absolute_mode size=1048576 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=17.324 write_power=15.321 use4initializers=NO score=84  */
/* global pool 5 is ? */
/* index=5 file postfix=AXISRAM1 name=cpuRAM1 offset=0x34064000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=16.616 write_power=14.522 use4initializers=NO score=84  */
/* global pool 6 is ? */
/* index=6 file postfix=AXIFLEXMEM name=flexMEM offset=0x34000000  absolute_mode size=0 READ_WRITE THROUGHPUT=MID LATENCY=MID byte width=8 freq ratio=2.5 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=9.381 write_power=8.569 use4initializers=NO score=84  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_yolo_deer(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_yolo_deer(uint32_t num)
{
  { 
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_yolo_deer(uint32_t num, void* buffer, uint32_t size)
{
  { 
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_yolo_deer(uint32_t num)
{
  { 
    return NULL;
  }
}

bool LL_ATON_EC_Network_Init_yolo_deer(void)
{
  return true;
}

bool LL_ATON_EC_Inference_Init_yolo_deer(void)
{
  return true;
}

/* scheduling epoch=0    nodes=138 ------------------------------------------------------------------- */

/* scheduling epoch=1    nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id473 */

static void LL_ATON_Start_EpochBlock_1(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id473 */
  /* node=Identity_inserted_id473 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id473 input ports=0 range=11[0,1228800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id473_dma_init_in_0_1 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Input_1_out_0_inserted_in473 */
    .offset_start = 0,
    .offset_end = 409600,
    .offset_limit = 1228864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 409600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id473_dma_init_in_0_1, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM3 -> 180224 */
  /* cpuRAM2 -> 1048576 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id473 output ports=0 range=11[1638400,2867200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id473_dma_init_out_0_1 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Input_1_out_0_inserted_out473 */
    .offset_start = 1638400,
    .offset_limit = 2867264,
    .frame_count = 0,
    .fwidth = 320,
    .fheight = 320,
    .batch_depth = 2,
    .batch_offset = 12,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 1228800,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Identity_inserted_id473_dma_init_out_0_1, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM6 <- 442368 */
  /* npuRAM5 <- 458752 */
  /* npuRAM4 <- 327680 */

  static const LL_Switch_InitTypeDef switch_init_in_1[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id473 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
  };


  /* epoch=1 */
  LL_Switch_Init(switch_init_in_1, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2867200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */, 1228800);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_1_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_1_all_units, 2);

}

static void LL_ATON_End_EpochBlock_1(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_1[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id473 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
  };


  /* epoch=1 */
  LL_Switch_Deinit(switch_deinit_in_1, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_1_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_1_all_units, 2);

}


/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_2(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_2 */
  Conv_sw_info conv1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 320,
    .general.input.dim.tensor_w = 320,
    .general.input.dim.tensor_c = 3,
    .general.input.dim.num_elem = 307200,
    .general.input.stride.b = 1228800,
    .general.input.stride.h = 3840,
    .general.input.stride.w = 12,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 16,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 3,
    .weights.dim.num_elem = 432,
    .weights.stride.b = 108,
    .weights.stride.h = 36,
    .weights.stride.w = 12,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12066432))) /* Equivalent hex address = 0x71b81e80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 16,
    .bias.dim.num_elem = 16,
    .bias.stride.b = 64,
    .bias.stride.h = 64,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090496))) /* Equivalent hex address = 0x71b87c80UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 160,
    .general.output.dim.tensor_w = 160,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 409600,
    .general.output.stride.b = 1638400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_2 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1638400);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_3(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_3 */
  Activ_sw_info activ2_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 160,
    .general.input.dim.tensor_w = 160,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 409600,
    .general.input.stride.b = 1638400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 160,
    .general.output.dim.tensor_w = 160,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 409600,
    .general.output.stride.b = 1638400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_3 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ2_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1638400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1638400);

}


/* scheduling epoch=4    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_4(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_4 */
  Arith_sw_info arith3_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 160,
    .general.input.dim.tensor_w = 160,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 409600,
    .general.input.stride.b = 1638400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 160,
    .operand.dim.tensor_w = 160,
    .operand.dim.tensor_c = 16,
    .operand.dim.num_elem = 409600,
    .operand.stride.b = 1638400,
    .operand.stride.h = 10240,
    .operand.stride.w = 64,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 160,
    .general.output.dim.tensor_w = 160,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 409600,
    .general.output.stride.b = 1638400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1638400))) /* Equivalent hex address = 0x90190000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_4 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith3_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1638400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 3276800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1638400))) /* Equivalent hex address = 0x90190000UL */, 1638400);

}


/* scheduling epoch=5    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_5(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_5 */
  Conv_sw_info conv4_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 160,
    .general.input.dim.tensor_w = 160,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 409600,
    .general.input.stride.b = 1638400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1638400))) /* Equivalent hex address = 0x90190000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 16,
    .weights.dim.num_elem = 4608,
    .weights.stride.b = 576,
    .weights.stride.h = 192,
    .weights.stride.w = 64,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11920192))) /* Equivalent hex address = 0x71b5e340UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089152))) /* Equivalent hex address = 0x71b87740UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_5 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv4_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 819200);

}


/* scheduling epoch=6    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_6(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_6 */
  Activ_sw_info activ5_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_6 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ5_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */, 819200);

}


/* scheduling epoch=7    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_7(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_7 */
  Arith_sw_info arith6_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 204800,
    .operand.stride.b = 819200,
    .operand.stride.h = 10240,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_7 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith6_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2457600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */, 819200);

}


/* scheduling epoch=8    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_8(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_8 */
  Conv_sw_info conv7_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 1024,
    .weights.stride.b = 128,
    .weights.stride.h = 128,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12062336))) /* Equivalent hex address = 0x71b80e80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089280))) /* Equivalent hex address = 0x71b877c0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_8 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv7_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 819200);

}


/* scheduling epoch=9    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_9(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_9 */
  Activ_sw_info activ8_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_9 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ8_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */, 819200);

}


/* scheduling epoch=10   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_10(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_10 */
  Arith_sw_info arith9_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 204800,
    .operand.stride.b = 819200,
    .operand.stride.h = 10240,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_10 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith9_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2457600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) /* Equivalent hex address = 0x34290000UL */, 819200);

}


/* scheduling epoch=11   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id475 */

static void LL_ATON_Start_EpochBlock_11(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id475 */
  /* node=Identity_inserted_id475 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id475 input ports=0 range=11[1638400,2457600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id475_dma_init_in_0_11 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Mul_10_out_0_inserted_in475 */
    .offset_start = 1638400,
    .offset_limit = 2457664,
    .frame_count = 0,
    .fwidth = 80,
    .fheight = 80,
    .batch_depth = 2,
    .batch_offset = 128,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 819200,
    .frame_loop_cnt = 32,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Identity_inserted_id475_dma_init_in_0_11, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM6 -> 32768 */
  /* npuRAM5 -> 458752 */
  /* npuRAM4 -> 327680 */

  /* Dma output units from cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id475 output ports=0 range=11[0,819200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id475_dma_init_out_0_11 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Mul_10_out_0_inserted_out475 */
    .offset_start = 0,
    .offset_end = 25600,
    .offset_limit = 819264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 25600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id475_dma_init_out_0_11, 1);


  /* Dma output bandwidth to memory pools: */
  /* cpuRAM2 <- 819200 */

  static const LL_Switch_InitTypeDef switch_init_in_11[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id475 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=11 */
  LL_Switch_Init(switch_init_in_11, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_11_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_11_all_units, 2);

}

static void LL_ATON_End_EpochBlock_11(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_11[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id475 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=11 */
  LL_Switch_Deinit(switch_deinit_in_11, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_11_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_11_all_units, 2);

}


/* scheduling epoch=12   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_19 */

static void LL_ATON_Start_EpochBlock_12(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_19 */
  /* node=Slice_19 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_19 input ports=0 range=11[0,819200] */

  static const LL_Streng_TensorInitTypeDef Slice_19_dma_init_in_0_12 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Mul_10_out_0 */
    .offset_start = 0,
    .offset_end = 25600,
    .offset_limit = 819264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 25600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Slice_19_dma_init_in_0_12, 1);


  /* Dma input bandwidth from memory pools: */
  /* cpuRAM2 -> 819200 */

  /* Dma output units from cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_19 output ports=0 range=11[819200,1638400] */

  static const LL_Streng_TensorInitTypeDef Slice_19_dma_init_out_0_12 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_19_out_0 */
    .offset_start = 819200,
    .offset_end = 844800,
    .offset_limit = 1638464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 25600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Slice_19_dma_init_out_0_12, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 131072 */
  /* npuRAM3 <- 458752 */
  /* cpuRAM2 <- 229376 */

  static const LL_Switch_InitTypeDef switch_init_in_12[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_19 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=12 */
  LL_Switch_Init(switch_init_in_12, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_12_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_12_all_units, 2);

}

static void LL_ATON_End_EpochBlock_12(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_12[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_19 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=12 */
  LL_Switch_Deinit(switch_deinit_in_12, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_12_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_12_all_units, 2);

}


/* scheduling epoch=13   nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id480 */
/* no resources allocated to kind=Identity node=Identity_inserted_id481 */

static void LL_ATON_Start_EpochBlock_13(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id480 */
  /* node=Identity_inserted_id480 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id481 */
  /* node=Identity_inserted_id481 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id480 input ports=0 range=11[819200,1228800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id480_dma_init_in_0_13 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_19_out_0_inserted_in480 */
    .offset_start = 819200,
    .offset_end = 844800,
    .offset_limit = 1228864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 25600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id480_dma_init_in_0_13, 1);

  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id481 input ports=0 range=11[1228800,1638400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id481_dma_init_in_0_13 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_11_out_0_inserted_in481 */
    .offset_start = 1228800,
    .offset_end = 1254400,
    .offset_limit = 1638464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 25600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id481_dma_init_in_0_13, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 131072 */
  /* npuRAM3 -> 458752 */
  /* cpuRAM2 -> 229376 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id480 output ports=0 range=7[0,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id480_dma_init_out_0_13 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .cacheable = 1,
    .cache_allocate = 0,
    .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */}, /* Slice_19_out_0_inserted_out480 */
    .offset_start = 0,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 80,
    .fheight = 80,
    .batch_depth = 2,
    .batch_offset = 64,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 16,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Identity_inserted_id480_dma_init_out_0_13, 1);

  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id481 output ports=0 range=7[409600,819200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id481_dma_init_out_0_13 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .cacheable = 1,
    .cache_allocate = 0,
    .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */}, /* Slice_11_out_0_inserted_out481 */
    .offset_start = 409600,
    .offset_limit = 819264,
    .frame_count = 0,
    .fwidth = 80,
    .fheight = 80,
    .batch_depth = 2,
    .batch_offset = 64,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 16,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id481_dma_init_out_0_13, 1);


  /* Dma output bandwidth to memory pools: */
  /* hyperRAM <- 819200 */

  static const LL_Switch_InitTypeDef switch_init_in_13[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id480 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id481 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=13 */
  LL_Switch_Init(switch_init_in_13, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 409600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 409600))) /* Equivalent hex address = 0x90064000UL */, 409600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_13_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_13_all_units, 4);

}

static void LL_ATON_End_EpochBlock_13(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_13[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id480 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id481 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=13 */
  LL_Switch_Deinit(switch_deinit_in_13, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_13_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_13_all_units, 4);

}


/* scheduling epoch=14   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_14(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_12 */
  Conv_sw_info conv10_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 409600))) /* Equivalent hex address = 0x90064000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 16,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 16,
    .weights.dim.num_elem = 2304,
    .weights.stride.b = 576,
    .weights.stride.h = 192,
    .weights.stride.w = 64,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12037760))) /* Equivalent hex address = 0x71b7ae80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 16,
    .bias.dim.num_elem = 16,
    .bias.stride.b = 64,
    .bias.stride.h = 64,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090560))) /* Equivalent hex address = 0x71b87cc0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_12 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv10_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=15   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_15(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_13 */
  Activ_sw_info activ11_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_13 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ11_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=16   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_16(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_14 */
  Arith_sw_info arith12_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 16,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 5120,
    .operand.stride.w = 64,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_14 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith12_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=17   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_17(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_15 */
  Conv_sw_info conv13_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 16,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 16,
    .weights.dim.num_elem = 2304,
    .weights.stride.b = 576,
    .weights.stride.h = 192,
    .weights.stride.w = 64,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12046976))) /* Equivalent hex address = 0x71b7d280UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 16,
    .bias.dim.num_elem = 16,
    .bias.stride.b = 64,
    .bias.stride.h = 64,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090624))) /* Equivalent hex address = 0x71b87d00UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_15 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv13_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=18   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_18(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_16 */
  Activ_sw_info activ14_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_16 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ14_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=19   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_19(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_17 */
  Arith_sw_info arith15_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 16,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 5120,
    .operand.stride.w = 64,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_17 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith15_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=20   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_20(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** NPU cache clean & invalidate operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  LL_ATON_Cache_NPU_Clean_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) /* Equivalent hex address = 0x900c8000UL */, 409600);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_18 */
  Arith_sw_info arith16_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 409600))) /* Equivalent hex address = 0x90064000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 16,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 5120,
    .operand.stride.w = 64,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) /* Equivalent hex address = 0x900c8000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_18 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith16_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 819200))) /* Equivalent hex address = 0x900c8000UL */, 409600);

}


/* scheduling epoch=21   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_20 */

static void LL_ATON_Start_EpochBlock_21(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_20 */
  /* node=Concat_20 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_20 input ports=0 range=7[0,1228800] */

  static const LL_Streng_TensorInitTypeDef Concat_20_dma_init_in_0_21 = {
    /* from memory with batch=16 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .cacheable = 1,
    .cache_allocate = 0,
    .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */}, /* Slice_19_out_0 */
    .offset_start = 0,
    .offset_end = 409600,
    .offset_limit = 1228864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 409600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Concat_20_dma_init_in_0_21, 1);


  /* Dma input bandwidth from memory pools: */
  /* hyperRAM -> 1228800 */

  /* Dma output units from cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_20 output ports=0 range=11[0,1228800] */

  static const LL_Streng_TensorInitTypeDef Concat_20_dma_init_out_0_21 = {
    /* to memory canonical from batch=16 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Concat_20_out_0 */
    .offset_start = 0,
    .offset_limit = 1228864,
    .frame_count = 0,
    .fwidth = 80,
    .fheight = 80,
    .batch_depth = 32,
    .batch_offset = 192,
    .frame_offset = 64,
    .line_offset = 0,
    .loop_offset = 1228800,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Concat_20_dma_init_out_0_21, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM3 <- 180224 */
  /* cpuRAM2 <- 1048576 */

  static const LL_Switch_InitTypeDef switch_init_in_21[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_20 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=21 */
  LL_Switch_Init(switch_init_in_21, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 1228800);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_21_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_21_all_units, 2);

}

static void LL_ATON_End_EpochBlock_21(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_21[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_20 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=21 */
  LL_Switch_Deinit(switch_deinit_in_21, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_21_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_21_all_units, 2);

}


/* scheduling epoch=22   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_22(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_21 */
  Conv_sw_info conv17_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 48,
    .general.input.dim.num_elem = 307200,
    .general.input.stride.b = 1228800,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 192,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 48,
    .weights.dim.num_elem = 1536,
    .weights.stride.b = 192,
    .weights.stride.h = 192,
    .weights.stride.w = 192,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12056192))) /* Equivalent hex address = 0x71b7f680UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089408))) /* Equivalent hex address = 0x71b87840UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) /* Equivalent hex address = 0x3422c000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_21 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv17_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2048000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) /* Equivalent hex address = 0x3422c000UL */, 819200);

}


/* scheduling epoch=23   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_23(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_22 */
  Activ_sw_info activ18_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) /* Equivalent hex address = 0x3422c000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2048000))) /* Equivalent hex address = 0x342f4000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_22 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ18_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2048000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2867200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2048000))) /* Equivalent hex address = 0x342f4000UL */, 819200);

}


/* scheduling epoch=24   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_24(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_23 */
  Arith_sw_info arith19_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) /* Equivalent hex address = 0x3422c000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 80,
    .operand.dim.tensor_w = 80,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 204800,
    .operand.stride.b = 819200,
    .operand.stride.h = 10240,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2048000))) /* Equivalent hex address = 0x342f4000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 80,
    .general.output.dim.tensor_w = 80,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 204800,
    .general.output.stride.b = 819200,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_23 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith19_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 819200);

}


/* scheduling epoch=25   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_25(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_24 */
  Conv_sw_info conv20_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 80,
    .general.input.dim.tensor_w = 80,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 18432,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11419648))) /* Equivalent hex address = 0x71ae4000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12081728))) /* Equivalent hex address = 0x71b85a40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_24 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv20_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=26   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_26(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_25 */
  Activ_sw_info activ21_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_25 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ21_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=27   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_27(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_26 */
  Arith_sw_info arith22_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_26 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith22_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=28   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_28(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_27 */
  Conv_sw_info conv23_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 4096,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11988608))) /* Equivalent hex address = 0x71b6ee80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12081984))) /* Equivalent hex address = 0x71b85b40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_27 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv23_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=29   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_29(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_28 */
  Activ_sw_info activ24_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_28 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ24_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=30   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_30(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_29 */
  Arith_sw_info arith25_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_29 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith25_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=31   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id483 */

static void LL_ATON_Start_EpochBlock_31(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id483 */
  /* node=Identity_inserted_id483 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id483 input ports=0 range=3[0,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id483_dma_init_in_0_31 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_29_out_0_inserted_in483 */
    .offset_start = 0,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id483_dma_init_in_0_31, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM3 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id483 output ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id483_dma_init_out_0_31 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_29_out_0_inserted_out483 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id483_dma_init_out_0_31, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_31[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id483 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=31 */
  LL_Switch_Init(switch_init_in_31, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_31_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_31_all_units, 2);

}

static void LL_ATON_End_EpochBlock_31(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_31[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id483 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=31 */
  LL_Switch_Deinit(switch_deinit_in_31, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_31_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_31_all_units, 2);

}


/* scheduling epoch=32   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_45 */

static void LL_ATON_Start_EpochBlock_32(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_45 */
  /* node=Slice_45 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_45 input ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Slice_45_dma_init_in_0_32 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_29_out_0 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Slice_45_dma_init_in_0_32, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_45 output ports=0 range=2[0,409600] */

  static const LL_Streng_TensorInitTypeDef Slice_45_dma_init_out_0_32 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_45_out_0 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Slice_45_dma_init_out_0_32, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_32[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_45 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=32 */
  LL_Switch_Init(switch_init_in_32, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_32_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_32_all_units, 2);

}

static void LL_ATON_End_EpochBlock_32(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_32[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_45 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=32 */
  LL_Switch_Deinit(switch_deinit_in_32, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_32_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_32_all_units, 2);

}


/* scheduling epoch=33   nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id488 */
/* no resources allocated to kind=Identity node=Identity_inserted_id489 */

static void LL_ATON_Start_EpochBlock_33(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id488 */
  /* node=Identity_inserted_id488 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id489 */
  /* node=Identity_inserted_id489 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id488 input ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id488_dma_init_in_0_33 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_45_out_0_inserted_in488 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Identity_inserted_id488_dma_init_in_0_33, 1);

  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id489 input ports=0 range=2[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id489_dma_init_in_0_33 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_30_out_0_inserted_in489 */
    .offset_start = 204800,
    .offset_end = 211200,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id489_dma_init_in_0_33, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id488 output ports=0 range=11[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id488_dma_init_out_0_33 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_45_out_0_inserted_out488 */
    .offset_start = 0,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 128,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 32,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Identity_inserted_id488_dma_init_out_0_33, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id489 output ports=0 range=11[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id489_dma_init_out_0_33 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_30_out_0_inserted_out489 */
    .offset_start = 204800,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 128,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 32,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id489_dma_init_out_0_33, 1);


  /* Dma output bandwidth to memory pools: */
  /* cpuRAM2 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_33[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id488 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id489 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=33 */
  LL_Switch_Init(switch_init_in_33, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) /* Equivalent hex address = 0x34132000UL */, 204800);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_33_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_33_all_units, 4);

}

static void LL_ATON_End_EpochBlock_33(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_33[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id488 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id489 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=33 */
  LL_Switch_Deinit(switch_deinit_in_33, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_33_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_33_all_units, 4);

}


/* scheduling epoch=34   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_34(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_31 */
  Conv_sw_info conv26_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) /* Equivalent hex address = 0x34132000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11608064))) /* Equivalent hex address = 0x71b12000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089536))) /* Equivalent hex address = 0x71b878c0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_31 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv26_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=35   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_35(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_32 */
  Activ_sw_info activ27_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_32 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ27_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=36   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_36(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_33 */
  Arith_sw_info arith28_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_33 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith28_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=37   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_37(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_34 */
  Conv_sw_info conv29_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11644928))) /* Equivalent hex address = 0x71b1b000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089664))) /* Equivalent hex address = 0x71b87940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_34 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv29_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=38   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_38(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_35 */
  Activ_sw_info activ30_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_35 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ30_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=39   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_39(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_36 */
  Arith_sw_info arith31_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_36 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith31_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=40   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_40(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_37 */
  Arith_sw_info arith32_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) /* Equivalent hex address = 0x34132000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_37 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith32_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */, 204800);

}


/* scheduling epoch=41   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_41(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_38 */
  Conv_sw_info conv33_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11681792))) /* Equivalent hex address = 0x71b24000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089792))) /* Equivalent hex address = 0x71b879c0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_38 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv33_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=42   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_42(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_39 */
  Activ_sw_info activ34_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_39 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ34_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=43   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_43(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_40 */
  Arith_sw_info arith35_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_40 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith35_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=44   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_44(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_41 */
  Conv_sw_info conv36_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11718656))) /* Equivalent hex address = 0x71b2d000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12089920))) /* Equivalent hex address = 0x71b87a40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_41 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv36_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=45   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_45(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_42 */
  Activ_sw_info activ37_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_42 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ37_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=46   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_46(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_43 */
  Arith_sw_info arith38_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_43 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith38_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=47   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_47(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_44 */
  Arith_sw_info arith39_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) /* Equivalent hex address = 0x34196000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_44 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith39_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) /* Equivalent hex address = 0x34196000UL */, 204800);

}


/* scheduling epoch=48   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_46 */

static void LL_ATON_Start_EpochBlock_48(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_46 */
  /* node=Concat_46 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_46 input ports=0 range=11[0,819200] */

  static const LL_Streng_TensorInitTypeDef Concat_46_dma_init_in_0_48 = {
    /* from memory with batch=32 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_45_out_0 */
    .offset_start = 0,
    .offset_end = 204800,
    .offset_limit = 819264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 204800,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Concat_46_dma_init_in_0_48, 1);


  /* Dma input bandwidth from memory pools: */
  /* cpuRAM2 -> 819200 */

  /* Dma output units from cycle: */
  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_46 output ports=0 range=11[819200,1638400] */

  static const LL_Streng_TensorInitTypeDef Concat_46_dma_init_out_0_48 = {
    /* to memory canonical from batch=32 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Concat_46_out_0 */
    .offset_start = 819200,
    .offset_limit = 1638464,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 64,
    .batch_offset = 512,
    .frame_offset = 128,
    .line_offset = 0,
    .loop_offset = 819200,
    .frame_loop_cnt = 4,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Concat_46_dma_init_out_0_48, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 131072 */
  /* npuRAM3 <- 458752 */
  /* cpuRAM2 <- 229376 */

  static const LL_Switch_InitTypeDef switch_init_in_48[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_46 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=48 */
  LL_Switch_Init(switch_init_in_48, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1638400))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */, 819200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_48_all_units[] = {
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_48_all_units, 2);

}

static void LL_ATON_End_EpochBlock_48(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_48[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_46 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=48 */
  LL_Switch_Deinit(switch_deinit_in_48, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_48_all_units[] = {
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_48_all_units, 2);

}


/* scheduling epoch=49   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_49(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_47 */
  Conv_sw_info conv40_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 204800,
    .general.input.stride.b = 819200,
    .general.input.stride.h = 20480,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) /* Equivalent hex address = 0x341c8000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 8192,
    .weights.stride.b = 512,
    .weights.stride.h = 512,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11862848))) /* Equivalent hex address = 0x71b50340UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12082240))) /* Equivalent hex address = 0x71b85c40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_47 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv40_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=50   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_50(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_48 */
  Activ_sw_info activ41_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_48 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ41_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=51   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_51(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_49 */
  Arith_sw_info arith42_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_49 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith42_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=52   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_52(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_50 */
  Conv_sw_info conv43_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 73728,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 7012352))) /* Equivalent hex address = 0x716b0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12076864))) /* Equivalent hex address = 0x71b84740UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_50 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv43_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=53   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_53(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_51 */
  Activ_sw_info activ44_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_51 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ44_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=54   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_54(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_52 */
  Arith_sw_info arith45_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_52 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith45_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=55   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_55(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_53 */
  Conv_sw_info conv46_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 16384,
    .weights.stride.b = 512,
    .weights.stride.h = 512,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11493376))) /* Equivalent hex address = 0x71af6000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12077376))) /* Equivalent hex address = 0x71b84940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_53 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv46_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=56   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_56(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_54 */
  Activ_sw_info activ47_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_54 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ47_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=57   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_57(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_55 */
  Arith_sw_info arith48_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_55 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith48_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=58   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id491 */

static void LL_ATON_Start_EpochBlock_58(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id491 */
  /* node=Identity_inserted_id491 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id491 input ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id491_dma_init_in_0_58 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Mul_55_out_0_inserted_in491 */
    .offset_start = 0,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id491_dma_init_in_0_58, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id491 output ports=0 range=1[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id491_dma_init_out_0_58 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_55_out_0_inserted_out491 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id491_dma_init_out_0_58, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_58[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id491 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=58 */
  LL_Switch_Init(switch_init_in_58, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_58_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_58_all_units, 2);

}

static void LL_ATON_End_EpochBlock_58(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_58[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id491 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=58 */
  LL_Switch_Deinit(switch_deinit_in_58, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_58_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_58_all_units, 2);

}


/* scheduling epoch=59   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_71 */

static void LL_ATON_Start_EpochBlock_59(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_71 */
  /* node=Slice_71 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_71 input ports=0 range=1[0,204800] */

  static const LL_Streng_TensorInitTypeDef Slice_71_dma_init_in_0_59 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_55_out_0 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Slice_71_dma_init_in_0_59, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_71 output ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Slice_71_dma_init_out_0_59 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_71_out_0 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Slice_71_dma_init_out_0_59, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_59[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_71 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=59 */
  LL_Switch_Init(switch_init_in_59, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_59_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_59_all_units, 2);

}

static void LL_ATON_End_EpochBlock_59(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_59[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_71 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=59 */
  LL_Switch_Deinit(switch_deinit_in_59, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_59_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_59_all_units, 2);

}


/* scheduling epoch=60   nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id496 */
/* no resources allocated to kind=Identity node=Identity_inserted_id497 */

static void LL_ATON_Start_EpochBlock_60(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id496 */
  /* node=Identity_inserted_id496 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id497 */
  /* node=Identity_inserted_id497 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id496 input ports=0 range=2[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id496_dma_init_in_0_60 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_71_out_0_inserted_in496 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Identity_inserted_id496_dma_init_in_0_60, 1);

  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id497 input ports=0 range=2[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id497_dma_init_in_0_60 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_56_out_0_inserted_in497 */
    .offset_start = 102400,
    .offset_end = 104000,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id497_dma_init_in_0_60, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id496 output ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id496_dma_init_out_0_60 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_71_out_0_inserted_out496 */
    .offset_start = 0,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Identity_inserted_id496_dma_init_out_0_60, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id497 output ports=0 range=1[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id497_dma_init_out_0_60 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_56_out_0_inserted_out497 */
    .offset_start = 102400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id497_dma_init_out_0_60, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_60[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id496 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id497 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=60 */
  LL_Switch_Init(switch_init_in_60, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_60_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_60_all_units, 4);

}

static void LL_ATON_End_EpochBlock_60(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_60[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id496 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id497 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=60 */
  LL_Switch_Deinit(switch_deinit_in_60, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_60_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_60_all_units, 4);

}


/* scheduling epoch=61   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_61(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_57 */
  Conv_sw_info conv49_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8945664))) /* Equivalent hex address = 0x71888000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12084544))) /* Equivalent hex address = 0x71b86540UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_57 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv49_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=62   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_62(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_58 */
  Activ_sw_info activ50_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_58 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ50_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=63   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_63(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_59 */
  Arith_sw_info arith51_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_59 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith51_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */, 102400);

}


/* scheduling epoch=64   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_64(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_60 */
  Conv_sw_info conv52_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9093120))) /* Equivalent hex address = 0x718ac000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12084800))) /* Equivalent hex address = 0x71b86640UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_60 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv52_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */, 102400);

}


/* scheduling epoch=65   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_65(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_61 */
  Activ_sw_info activ53_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_61 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ53_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=66   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_66(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_62 */
  Arith_sw_info arith54_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_62 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith54_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=67   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_67(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_63 */
  Arith_sw_info arith55_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_63 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith55_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=68   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_68(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_64 */
  Conv_sw_info conv56_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9240576))) /* Equivalent hex address = 0x718d0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12085056))) /* Equivalent hex address = 0x71b86740UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_64 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv56_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=69   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_69(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_65 */
  Activ_sw_info activ57_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_65 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ57_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=70   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_70(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_66 */
  Arith_sw_info arith58_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_66 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith58_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */, 102400);

}


/* scheduling epoch=71   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_71(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_67 */
  Conv_sw_info conv59_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) /* Equivalent hex address = 0x342a2000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9388032))) /* Equivalent hex address = 0x718f4000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12085312))) /* Equivalent hex address = 0x71b86840UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_67 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv59_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */, 102400);

}


/* scheduling epoch=72   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_72(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_68 */
  Activ_sw_info activ60_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_68 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ60_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=73   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_73(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_69 */
  Arith_sw_info arith61_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_69 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith61_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=74   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_74(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_70 */
  Arith_sw_info arith62_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_70 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith62_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=75   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_72 */

static void LL_ATON_Start_EpochBlock_75(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_72 */
  /* node=Concat_72 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_72 input ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Concat_72_dma_init_in_0_75 = {
    /* from memory with batch=64 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_71_out_0 */
    .offset_start = 0,
    .offset_end = 102400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 102400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Concat_72_dma_init_in_0_75, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_72 output ports=0 range=2[0,409600] */

  static const LL_Streng_TensorInitTypeDef Concat_72_dma_init_out_0_75 = {
    /* to memory canonical from batch=64 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Concat_72_out_0 */
    .offset_start = 0,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 128,
    .batch_offset = 1024,
    .frame_offset = 256,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 4,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Concat_72_dma_init_out_0_75, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_75[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_72 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=75 */
  LL_Switch_Init(switch_init_in_75, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_75_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_75_all_units, 2);

}

static void LL_ATON_End_EpochBlock_75(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_75[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_72 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=75 */
  LL_Switch_Deinit(switch_deinit_in_75, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_75_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_75_all_units, 2);

}


/* scheduling epoch=76   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_76(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_73 */
  Conv_sw_info conv63_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 20480,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 256,
    .weights.dim.num_elem = 32768,
    .weights.stride.b = 1024,
    .weights.stride.h = 1024,
    .weights.stride.w = 1024,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10862592))) /* Equivalent hex address = 0x71a5c000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12077888))) /* Equivalent hex address = 0x71b84b40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_73 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv63_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=77   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_77(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_74 */
  Activ_sw_info activ64_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_74 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ64_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=78   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_78(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_75 */
  Arith_sw_info arith65_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_75 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith65_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 204800);

}


/* scheduling epoch=79   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_79(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_76 */
  Conv_sw_info conv66_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 294912,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 0))) /* Equivalent hex address = 0x71000000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12068160))) /* Equivalent hex address = 0x71b82540UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_76 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv66_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=80   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_80(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_77 */
  Activ_sw_info activ67_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_77 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ67_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=81   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_81(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_78 */
  Arith_sw_info arith68_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_78 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith68_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=82   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_82(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_79 */
  Conv_sw_info conv69_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 256,
    .weights.dim.num_elem = 65536,
    .weights.stride.b = 1024,
    .weights.stride.h = 1024,
    .weights.stride.w = 1024,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 7897088))) /* Equivalent hex address = 0x71788000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12073280))) /* Equivalent hex address = 0x71b83940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_79 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv69_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=83   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_83(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_80 */
  Activ_sw_info activ70_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_80 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ70_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=84   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_84(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_81 */
  Arith_sw_info arith71_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_81 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith71_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=85   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id499 */

static void LL_ATON_Start_EpochBlock_85(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id499 */
  /* node=Identity_inserted_id499 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id499 input ports=0 range=1[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id499_dma_init_in_0_85 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_81_out_0_inserted_in499 */
    .offset_start = 102400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 1024,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 256,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id499_dma_init_in_0_85, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id499 output ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id499_dma_init_out_0_85 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_81_out_0_inserted_out499 */
    .offset_start = 0,
    .offset_end = 400,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Identity_inserted_id499_dma_init_out_0_85, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 102400 */

  static const LL_Switch_InitTypeDef switch_init_in_85[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id499 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=85 */
  LL_Switch_Init(switch_init_in_85, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_85_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_85_all_units, 2);

}

static void LL_ATON_End_EpochBlock_85(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_85[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id499 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=85 */
  LL_Switch_Deinit(switch_deinit_in_85, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_85_all_units[] = {
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_85_all_units, 2);

}


/* scheduling epoch=86   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_90 */

static void LL_ATON_Start_EpochBlock_86(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_90 */
  /* node=Slice_90 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_90 input ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Slice_90_dma_init_in_0_86 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_81_out_0 */
    .offset_start = 0,
    .offset_end = 400,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Slice_90_dma_init_in_0_86, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_90 output ports=0 range=1[153600,256000] */

  static const LL_Streng_TensorInitTypeDef Slice_90_dma_init_out_0_86 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_90_out_0 */
    .offset_start = 153600,
    .offset_end = 154000,
    .offset_limit = 256064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Slice_90_dma_init_out_0_86, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 102400 */

  static const LL_Switch_InitTypeDef switch_init_in_86[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_90 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=86 */
  LL_Switch_Init(switch_init_in_86, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_86_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_86_all_units, 2);

}

static void LL_ATON_End_EpochBlock_86(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_86[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_90 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=86 */
  LL_Switch_Deinit(switch_deinit_in_86, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_86_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_86_all_units, 2);

}


/* scheduling epoch=87   nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id504 */
/* no resources allocated to kind=Identity node=Identity_inserted_id505 */

static void LL_ATON_Start_EpochBlock_87(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id504 */
  /* node=Identity_inserted_id504 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id505 */
  /* node=Identity_inserted_id505 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id504 input ports=0 range=1[153600,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id504_dma_init_in_0_87 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_90_out_0_inserted_in504 */
    .offset_start = 153600,
    .offset_end = 154000,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id504_dma_init_in_0_87, 1);

  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id505 input ports=0 range=1[204800,256000] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id505_dma_init_in_0_87 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_82_out_0_inserted_in505 */
    .offset_start = 204800,
    .offset_end = 205200,
    .offset_limit = 256064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Identity_inserted_id505_dma_init_in_0_87, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id504 output ports=0 range=1[0,51200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id504_dma_init_out_0_87 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_90_out_0_inserted_out504 */
    .offset_start = 0,
    .offset_limit = 51264,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 51200,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id504_dma_init_out_0_87, 1);

  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id505 output ports=0 range=1[51200,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id505_dma_init_out_0_87 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_82_out_0_inserted_out505 */
    .offset_start = 51200,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 51200,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Identity_inserted_id505_dma_init_out_0_87, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 102400 */

  static const LL_Switch_InitTypeDef switch_init_in_87[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id504 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id505 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
  };


  /* epoch=87 */
  LL_Switch_Init(switch_init_in_87, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 51200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_87_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_87_all_units, 4);

}

static void LL_ATON_End_EpochBlock_87(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_87[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id504 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id505 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
  };


  /* epoch=87 */
  LL_Switch_Deinit(switch_deinit_in_87, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_87_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_87_all_units, 4);

}


/* scheduling epoch=88   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_88(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_83 */
  Conv_sw_info conv72_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 1179648))) /* Equivalent hex address = 0x71120000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12074304))) /* Equivalent hex address = 0x71b83d40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_83 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv72_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 51200);

}


/* scheduling epoch=89   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_89(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_84 */
  Activ_sw_info activ73_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_84 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ73_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 51200);

}


/* scheduling epoch=90   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_90(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_85 */
  Arith_sw_info arith74_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_85 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith74_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */, 51200);

}


/* scheduling epoch=91   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_91(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_86 */
  Conv_sw_info conv75_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 1769472))) /* Equivalent hex address = 0x711b0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12074816))) /* Equivalent hex address = 0x71b83f40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_86 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv75_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 51200);

}


/* scheduling epoch=92   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_92(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_87 */
  Activ_sw_info activ76_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_87 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ76_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 51200);

}


/* scheduling epoch=93   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_93(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_88 */
  Arith_sw_info arith77_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_88 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith77_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */, 51200);

}


/* scheduling epoch=94   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_94(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_89 */
  Arith_sw_info arith78_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_89 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith78_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 51200);

}


/* scheduling epoch=95   nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_91 */

static void LL_ATON_Start_EpochBlock_95(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_91 */
  /* node=Concat_91 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_91 input ports=0 range=1[0,153600] */

  static const LL_Streng_TensorInitTypeDef Concat_91_dma_init_in_0_95 = {
    /* from memory with batch=128 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_90_out_0 */
    .offset_start = 0,
    .offset_end = 51200,
    .offset_limit = 153664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 51200,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Concat_91_dma_init_in_0_95, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 153600 */

  /* Dma output units from cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_91 output ports=0 range=1[153600,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_91_dma_init_out_0_95 = {
    /* to memory canonical from batch=128 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_91_out_0 */
    .offset_start = 153600,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 256,
    .batch_offset = 1536,
    .frame_offset = 512,
    .line_offset = 0,
    .loop_offset = 153600,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Concat_91_dma_init_out_0_95, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 153600 */

  static const LL_Switch_InitTypeDef switch_init_in_95[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_91 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=95 */
  LL_Switch_Init(switch_init_in_95, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 153600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_95_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_95_all_units, 2);

}

static void LL_ATON_End_EpochBlock_95(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_95[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_91 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=95 */
  LL_Switch_Deinit(switch_deinit_in_95, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_95_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_95_all_units, 2);

}


/* scheduling epoch=96   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_96(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_92 */
  Conv_sw_info conv79_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 384,
    .general.input.dim.num_elem = 38400,
    .general.input.stride.b = 153600,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 1536,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 384,
    .weights.dim.num_elem = 98304,
    .weights.stride.b = 1536,
    .weights.stride.h = 1536,
    .weights.stride.w = 1536,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 5832704))) /* Equivalent hex address = 0x71590000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12070208))) /* Equivalent hex address = 0x71b82d40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_92 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv79_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=97   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_97(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_93 */
  Activ_sw_info activ80_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_93 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ80_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=98   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_98(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_94 */
  Arith_sw_info arith81_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_94 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith81_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=99   nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_99(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_95 */
  Conv_sw_info conv82_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 256,
    .weights.dim.num_elem = 32768,
    .weights.stride.b = 1024,
    .weights.stride.h = 1024,
    .weights.stride.w = 1024,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10993664))) /* Equivalent hex address = 0x71a7c000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12080448))) /* Equivalent hex address = 0x71b85540UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_95 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv82_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 51200);

}


/* scheduling epoch=100  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_100(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_96 */
  Activ_sw_info activ83_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_96 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ83_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */, 51200);

}


/* scheduling epoch=101  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_101(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_97 */
  Arith_sw_info arith84_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_97 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith84_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 51200);

}


/* scheduling epoch=102  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_102(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=MaxPool_98_decomposed_pad */
  unsigned char* MaxPool_98_decomposed_pad_input_start_addr_102 = ((unsigned char *)((0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */;
  unsigned char* MaxPool_98_decomposed_pad_output_start_addr_102 = ((unsigned char *)((0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */;
  unsigned char* MaxPool_98_decomposed_pad_input_addr_limit_102 = ((unsigned char *)((0x342e0000UL + 51264))) /* Equivalent hex address = 0x342ec840UL */;
  unsigned char* MaxPool_98_decomposed_pad_output_addr_limit_102 = ((unsigned char *)((0x342e0000UL + 305216))) /* Equivalent hex address = 0x3432a840UL */;

  static const uint32_t MaxPool_98_decomposed_pad_min_shape_102[] = { 1, 10, 10, 128 };

  static const int32_t MaxPool_98_decomposed_pad_pad_in_offsets_start_102[] = { 0, 10240, 1024, 0 };
  static const int32_t MaxPool_98_decomposed_pad_pad_in_offsets_end_102[] = { 0, 10240, 1024, 0 };

  static const int32_t MaxPool_98_decomposed_pad_pad_out_offsets_start_102[] = { 0, 14336, 1024, 0 };
  static const int32_t MaxPool_98_decomposed_pad_pad_out_offsets_end_102[] = { 0, 14336, 1024, 0 };

  static const int32_t MaxPool_98_decomposed_pad_out_shape_102[] = { 1, 14, 14, 128 };
  static const int32_t MaxPool_98_decomposed_pad_out_offsets_102[] = { 100352, 7168, 512, 4 };

  const float pad_const_value_native = -HUGE_VALF;

  LL_ATON_LIB_Pad(MaxPool_98_decomposed_pad_input_start_addr_102, MaxPool_98_decomposed_pad_output_start_addr_102, MaxPool_98_decomposed_pad_input_addr_limit_102, MaxPool_98_decomposed_pad_output_addr_limit_102, MaxPool_98_decomposed_pad_min_shape_102, 0, 4, 25088, *((int32_t*)&pad_const_value_native), 2, 1280, MaxPool_98_decomposed_pad_pad_in_offsets_start_102, MaxPool_98_decomposed_pad_pad_in_offsets_end_102, MaxPool_98_decomposed_pad_pad_out_offsets_start_102, MaxPool_98_decomposed_pad_pad_out_offsets_end_102, MaxPool_98_decomposed_pad_out_shape_102, MaxPool_98_decomposed_pad_out_offsets_102, 4, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

}


/* scheduling epoch=103  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_103(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_98_decomposed_0 */
  Pool_sw_info pool85_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 12,
    .general.output.dim.tensor_w = 12,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_98_decomposed_0 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool85_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 378880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */, 73728);

}


/* scheduling epoch=104  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_104(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_98_decomposed_1 */
  Pool_sw_info pool86_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 12,
    .general.input.dim.tensor_w = 12,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_98_decomposed_1 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool86_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 51200);

}


/* scheduling epoch=105  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_105(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=MaxPool_99_decomposed_pad */
  unsigned char* MaxPool_99_decomposed_pad_input_start_addr_105 = ((unsigned char *)((0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */;
  unsigned char* MaxPool_99_decomposed_pad_output_start_addr_105 = ((unsigned char *)((0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */;
  unsigned char* MaxPool_99_decomposed_pad_input_addr_limit_105 = ((unsigned char *)((0x342e0000UL + 102464))) /* Equivalent hex address = 0x342f9040UL */;
  unsigned char* MaxPool_99_decomposed_pad_output_addr_limit_105 = ((unsigned char *)((0x342e0000UL + 305216))) /* Equivalent hex address = 0x3432a840UL */;

  static const uint32_t MaxPool_99_decomposed_pad_min_shape_105[] = { 1, 10, 10, 128 };

  static const int32_t MaxPool_99_decomposed_pad_pad_in_offsets_start_105[] = { 0, 10240, 1024, 0 };
  static const int32_t MaxPool_99_decomposed_pad_pad_in_offsets_end_105[] = { 0, 10240, 1024, 0 };

  static const int32_t MaxPool_99_decomposed_pad_pad_out_offsets_start_105[] = { 0, 14336, 1024, 0 };
  static const int32_t MaxPool_99_decomposed_pad_pad_out_offsets_end_105[] = { 0, 14336, 1024, 0 };

  static const int32_t MaxPool_99_decomposed_pad_out_shape_105[] = { 1, 14, 14, 128 };
  static const int32_t MaxPool_99_decomposed_pad_out_offsets_105[] = { 100352, 7168, 512, 4 };

  const float pad_const_value_native = -HUGE_VALF;

  LL_ATON_LIB_Pad(MaxPool_99_decomposed_pad_input_start_addr_105, MaxPool_99_decomposed_pad_output_start_addr_105, MaxPool_99_decomposed_pad_input_addr_limit_105, MaxPool_99_decomposed_pad_output_addr_limit_105, MaxPool_99_decomposed_pad_min_shape_105, 0, 4, 25088, *((int32_t*)&pad_const_value_native), 2, 1280, MaxPool_99_decomposed_pad_pad_in_offsets_start_105, MaxPool_99_decomposed_pad_pad_in_offsets_end_105, MaxPool_99_decomposed_pad_pad_out_offsets_start_105, MaxPool_99_decomposed_pad_pad_out_offsets_end_105, MaxPool_99_decomposed_pad_out_shape_105, MaxPool_99_decomposed_pad_out_offsets_105, 4, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

}


/* scheduling epoch=106  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_106(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_99_decomposed_0 */
  Pool_sw_info pool87_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 12,
    .general.output.dim.tensor_w = 12,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_99_decomposed_0 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool87_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 378880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */, 73728);

}


/* scheduling epoch=107  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_107(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_99_decomposed_1 */
  Pool_sw_info pool88_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 12,
    .general.input.dim.tensor_w = 12,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_99_decomposed_1 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool88_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 51200);

}


/* scheduling epoch=108  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_108(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Pad node=MaxPool_100_decomposed_pad */
  unsigned char* MaxPool_100_decomposed_pad_input_start_addr_108 = ((unsigned char *)((0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */;
  unsigned char* MaxPool_100_decomposed_pad_output_start_addr_108 = ((unsigned char *)((0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */;
  unsigned char* MaxPool_100_decomposed_pad_input_addr_limit_108 = ((unsigned char *)((0x342e0000UL + 153664))) /* Equivalent hex address = 0x34305840UL */;
  unsigned char* MaxPool_100_decomposed_pad_output_addr_limit_108 = ((unsigned char *)((0x342e0000UL + 305216))) /* Equivalent hex address = 0x3432a840UL */;

  static const uint32_t MaxPool_100_decomposed_pad_min_shape_108[] = { 1, 10, 10, 128 };

  static const int32_t MaxPool_100_decomposed_pad_pad_in_offsets_start_108[] = { 0, 10240, 1024, 0 };
  static const int32_t MaxPool_100_decomposed_pad_pad_in_offsets_end_108[] = { 0, 10240, 1024, 0 };

  static const int32_t MaxPool_100_decomposed_pad_pad_out_offsets_start_108[] = { 0, 14336, 1024, 0 };
  static const int32_t MaxPool_100_decomposed_pad_pad_out_offsets_end_108[] = { 0, 14336, 1024, 0 };

  static const int32_t MaxPool_100_decomposed_pad_out_shape_108[] = { 1, 14, 14, 128 };
  static const int32_t MaxPool_100_decomposed_pad_out_offsets_108[] = { 100352, 7168, 512, 4 };

  const float pad_const_value_native = -HUGE_VALF;

  LL_ATON_LIB_Pad(MaxPool_100_decomposed_pad_input_start_addr_108, MaxPool_100_decomposed_pad_output_start_addr_108, MaxPool_100_decomposed_pad_input_addr_limit_108, MaxPool_100_decomposed_pad_output_addr_limit_108, MaxPool_100_decomposed_pad_min_shape_108, 0, 4, 25088, *((int32_t*)&pad_const_value_native), 2, 1280, MaxPool_100_decomposed_pad_pad_in_offsets_start_108, MaxPool_100_decomposed_pad_pad_in_offsets_end_108, MaxPool_100_decomposed_pad_pad_out_offsets_start_108, MaxPool_100_decomposed_pad_pad_out_offsets_end_108, MaxPool_100_decomposed_pad_out_shape_108, MaxPool_100_decomposed_pad_out_offsets_108, 4, 6, 7);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 100352);

}


/* scheduling epoch=109  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_109(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_100_decomposed_0 */
  Pool_sw_info pool89_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 14,
    .general.input.dim.tensor_w = 14,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 25088,
    .general.input.stride.b = 100352,
    .general.input.stride.h = 7168,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 12,
    .general.output.dim.tensor_w = 12,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 18432,
    .general.output.stride.b = 73728,
    .general.output.stride.h = 6144,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_100_decomposed_0 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool89_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 378880))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */, 73728);

}


/* scheduling epoch=110  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_110(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=MaxPool node=MaxPool_100_decomposed_1 */
  Pool_sw_info pool90_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 12,
    .general.input.dim.tensor_w = 12,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 18432,
    .general.input.stride.b = 73728,
    .general.input.stride.h = 6144,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 305152))) /* Equivalent hex address = 0x3432a800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .k_shape = {3, 3},
    .general.type = LL_SW_MAXPOOL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node MaxPool_100_decomposed_1 mapped on EmbedNets (FLOAT) as MaxPool | Category: Computational */
  ll_sw_forward_pool(&pool90_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 51200);

}


/* scheduling epoch=111  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_101 */

static void LL_ATON_Start_EpochBlock_111(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_101 */
  /* node=Concat_101 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_101 input ports=0 range=1[0,204800] */

  static const LL_Streng_TensorInitTypeDef Concat_101_dma_init_in_0_111 = {
    /* from memory with batch=128 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_97_out_0 */
    .offset_start = 0,
    .offset_end = 51200,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 51200,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Concat_101_dma_init_in_0_111, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_101 output ports=0 range=1[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Concat_101_dma_init_out_0_111 = {
    /* to memory canonical from batch=128 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_101_out_0 */
    .offset_start = 204800,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 256,
    .batch_offset = 2048,
    .frame_offset = 512,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 4,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Concat_101_dma_init_out_0_111, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_111[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_101 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=111 */
  LL_Switch_Init(switch_init_in_111, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_111_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_111_all_units, 2);

}

static void LL_ATON_End_EpochBlock_111(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_111[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_101 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=111 */
  LL_Switch_Deinit(switch_deinit_in_111, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_111_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_111_all_units, 2);

}


/* scheduling epoch=112  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_112(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_102 */
  Conv_sw_info conv91_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 512,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 20480,
    .general.input.stride.w = 2048,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 512,
    .weights.dim.num_elem = 131072,
    .weights.stride.b = 2048,
    .weights.stride.h = 2048,
    .weights.stride.w = 2048,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 5308416))) /* Equivalent hex address = 0x71510000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12069184))) /* Equivalent hex address = 0x71b82940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_102 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv91_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=113  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_113(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_103 */
  Activ_sw_info activ92_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_103 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ92_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=114  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_114(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** NPU cache clean & invalidate operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1536000))) */
  LL_ATON_Cache_NPU_Clean_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) /* Equivalent hex address = 0x9015e000UL */, 102400);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_104 */
  Arith_sw_info arith93_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) /* Equivalent hex address = 0x9015e000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_104 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith93_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1536000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) /* Equivalent hex address = 0x9015e000UL */, 102400);

}


/* scheduling epoch=115  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Resize_105_resize_NN_expansion_concat_9 */

static void LL_ATON_Start_EpochBlock_115(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Resize_105_resize_NN_expansion_concat_9 */
  /* node=Resize_105_resize_NN_expansion_concat_9 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Resize_105_resize_NN_expansion_concat_9 input ports=0 range=7[1433600,1536000] */

  static const LL_Streng_TensorInitTypeDef Resize_105_resize_NN_expansion_concat_9_dma_init_in_0_115 = {
    /* from memory with batch=256
iterating outer iter=0 num_higher_elem=4
spanning across 409600 bytes */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .cacheable = 1,
    .cache_allocate = 1,
    .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */}, /* Mul_104_out_0 */
    .offset_start = 1433600,
    .offset_end = 1536000,
    .offset_limit = 1536064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 102400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 1,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Resize_105_resize_NN_expansion_concat_9_dma_init_in_0_115, 1);


  /* Dma input bandwidth from memory pools: */
  /* hyperRAM -> 102400 */
  /* CACHE -> 307200 */

  /* Dma output units from cycle: */
  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Resize_105_resize_NN_expansion_concat_9 output ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Resize_105_resize_NN_expansion_concat_9_dma_init_out_0_115 = {
    /* to memory canonical from batch=256 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Resize_105_resize_NN_expansion_concat_9_out_10 */
    .offset_start = 0,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 512,
    .batch_offset = 4096,
    .frame_offset = 1024,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 4,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Resize_105_resize_NN_expansion_concat_9_dma_init_out_0_115, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_115[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Resize_105_resize_NN_expansion_concat_9 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=115 */
  LL_Switch_Init(switch_init_in_115, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_115_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_115_all_units, 2);

}

static void LL_ATON_End_EpochBlock_115(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_115[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Resize_105_resize_NN_expansion_concat_9 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=115 */
  LL_Switch_Deinit(switch_deinit_in_115, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_115_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_115_all_units, 2);

}


/* scheduling epoch=116  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_116(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=DepthToSpace node=Resize_105_resize_NN_to_expansion_dts_11 */
  static const uint32_t Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116__shape_1_1024_10_10[] = { 1, 10, 10, 1024 };
  static const uint32_t Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116__mem_shape_L_1_1024_10_10[] = { 1, 10, 10, 1024 };
  static const LL_Buffer_InfoTypeDef Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116[] = {
    {
      .name = "Resize_105_resize_NN_expansion_concat_9_out_10",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 116,
      .batch = 1024,
      .mem_shape = Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116__mem_shape_L_1_1024_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116__shape_1_1024_10_10,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116__shape_1_256_20_20[] = { 1, 20, 20, 256 };
  static const uint32_t Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116__mem_shape_L_1_256_20_20[] = { 1, 20, 20, 256 };
  static const LL_Buffer_InfoTypeDef Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116[] = {
    {
      .name = "Resize_105_resize_NN_expansion_concat_9_out_12",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 116,
      .batch = 256,
      .mem_shape = Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116__mem_shape_L_1_256_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116__shape_1_256_20_20,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_DMA_DepthToSpace(Resize_105_resize_NN_to_expansion_dts_11_tensor_info_in_116, 1, Resize_105_resize_NN_to_expansion_dts_11_tensor_info_out_116, 2, 2, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=117  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_117(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 614400);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_106 */
  static const uint32_t Concat_106_tensor_info_in_117__shape_1_256_20_20[] = { 1, 20, 20, 256 };
  static const uint32_t Concat_106_tensor_info_in_117__mem_shape_L_1_256_20_20[] = { 1, 20, 20, 256 };
  static const uint32_t Concat_106_tensor_info_in_117__shape_1_128_20_20[] = { 1, 20, 20, 128 };
  static const uint32_t Concat_106_tensor_info_in_117__mem_shape_L_1_128_20_20[] = { 1, 20, 20, 128 };
  static const LL_Buffer_InfoTypeDef Concat_106_tensor_info_in_117[] = {
    {
      .name = "Resize_105_resize_NN_expansion_concat_9_out_12",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 256,
      .mem_shape = Concat_106_tensor_info_in_117__mem_shape_L_1_256_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_106_tensor_info_in_117__shape_1_256_20_20,
    },
    {
      .name = "Mul_75_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 128,
      .mem_shape = Concat_106_tensor_info_in_117__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_106_tensor_info_in_117__shape_1_128_20_20,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_106_tensor_info_out_117__shape_1_384_20_20[] = { 1, 20, 20, 384 };
  static const uint32_t Concat_106_tensor_info_out_117__mem_shape_L_1_384_20_20[] = { 1, 20, 20, 384 };
  static const LL_Buffer_InfoTypeDef Concat_106_tensor_info_out_117[] = {
    {
      .name = "Concat_106_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 614400,
      .offset_limit = 614464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 384,
      .mem_shape = Concat_106_tensor_info_out_117__mem_shape_L_1_384_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_106_tensor_info_out_117__shape_1_384_20_20,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_106_tensor_info_in_117, 2, Concat_106_tensor_info_out_117, 1, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 614400);

}


/* scheduling epoch=118  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_118(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_107 */
  Conv_sw_info conv94_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 384,
    .general.input.dim.num_elem = 153600,
    .general.input.stride.b = 614400,
    .general.input.stride.h = 30720,
    .general.input.stride.w = 1536,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 384,
    .weights.dim.num_elem = 49152,
    .weights.stride.b = 1536,
    .weights.stride.h = 1536,
    .weights.stride.w = 1536,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8159232))) /* Equivalent hex address = 0x717c8000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12078400))) /* Equivalent hex address = 0x71b84d40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_107 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv94_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=119  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_119(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_108 */
  Activ_sw_info activ95_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_108 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ95_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=120  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_120(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_109 */
  Arith_sw_info arith96_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_109 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith96_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=121  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id507 */

static void LL_ATON_Start_EpochBlock_121(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id507 */
  /* node=Identity_inserted_id507 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id507 input ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id507_dma_init_in_0_121 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Mul_109_out_0_inserted_in507 */
    .offset_start = 0,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id507_dma_init_in_0_121, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id507 output ports=0 range=1[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id507_dma_init_out_0_121 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_109_out_0_inserted_out507 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id507_dma_init_out_0_121, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_121[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id507 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=121 */
  LL_Switch_Init(switch_init_in_121, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_121_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_121_all_units, 2);

}

static void LL_ATON_End_EpochBlock_121(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_121[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id507 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=121 */
  LL_Switch_Deinit(switch_deinit_in_121, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_121_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_121_all_units, 2);

}


/* scheduling epoch=122  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_117 */

static void LL_ATON_Start_EpochBlock_122(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_117 */
  /* node=Slice_117 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_117 input ports=0 range=1[0,204800] */

  static const LL_Streng_TensorInitTypeDef Slice_117_dma_init_in_0_122 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_109_out_0 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Slice_117_dma_init_in_0_122, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_117 output ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Slice_117_dma_init_out_0_122 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_117_out_0 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Slice_117_dma_init_out_0_122, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_122[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_117 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=122 */
  LL_Switch_Init(switch_init_in_122, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_122_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_122_all_units, 2);

}

static void LL_ATON_End_EpochBlock_122(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_122[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_117 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=122 */
  LL_Switch_Deinit(switch_deinit_in_122, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_122_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_122_all_units, 2);

}


/* scheduling epoch=123  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id511 */
/* no resources allocated to kind=Identity node=Identity_inserted_id512 */

static void LL_ATON_Start_EpochBlock_123(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id511 */
  /* node=Identity_inserted_id511 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id512 */
  /* node=Identity_inserted_id512 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id511 input ports=0 range=2[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id511_dma_init_in_0_123 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_117_out_0_inserted_in511 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id511_dma_init_in_0_123, 1);

  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id512 input ports=0 range=2[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id512_dma_init_in_0_123 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_110_out_0_inserted_in512 */
    .offset_start = 102400,
    .offset_end = 104000,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id512_dma_init_in_0_123, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id511 output ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id511_dma_init_out_0_123 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_117_out_0_inserted_out511 */
    .offset_start = 0,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id511_dma_init_out_0_123, 1);

  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id512 output ports=0 range=1[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id512_dma_init_out_0_123 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_110_out_0_inserted_out512 */
    .offset_start = 102400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Identity_inserted_id512_dma_init_out_0_123, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_123[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id511 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id512 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
  };


  /* epoch=123 */
  LL_Switch_Init(switch_init_in_123, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_123_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_123_all_units, 4);

}

static void LL_ATON_End_EpochBlock_123(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_123[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id511 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id512 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
  };


  /* epoch=123 */
  LL_Switch_Deinit(switch_deinit_in_123, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_123_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_123_all_units, 4);

}


/* scheduling epoch=124  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_124(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_111 */
  Conv_sw_info conv97_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9535488))) /* Equivalent hex address = 0x71918000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12085568))) /* Equivalent hex address = 0x71b86940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_111 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv97_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=125  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_125(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_112 */
  Activ_sw_info activ98_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_112 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ98_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=126  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_126(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_113 */
  Arith_sw_info arith99_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_113 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith99_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=127  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_127(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_114 */
  Conv_sw_info conv100_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9682944))) /* Equivalent hex address = 0x7193c000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12085824))) /* Equivalent hex address = 0x71b86a40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_114 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv100_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=128  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_128(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_115 */
  Activ_sw_info activ101_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_115 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ101_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=129  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_129(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_116 */
  Arith_sw_info arith102_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_116 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith102_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=130  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_118 */

static void LL_ATON_Start_EpochBlock_130(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_118 */
  /* node=Concat_118 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_118 input ports=0 range=1[0,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_118_dma_init_in_0_130 = {
    /* from memory with batch=64 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_117_out_0 */
    .offset_start = 0,
    .offset_end = 102400,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 102400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Concat_118_dma_init_in_0_130, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 307200 */

  /* Dma output units from cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_118 output ports=0 range=2[0,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_118_dma_init_out_0_130 = {
    /* to memory canonical from batch=64 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Concat_118_out_0 */
    .offset_start = 0,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 128,
    .batch_offset = 768,
    .frame_offset = 256,
    .line_offset = 0,
    .loop_offset = 307200,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Concat_118_dma_init_out_0_130, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 307200 */

  static const LL_Switch_InitTypeDef switch_init_in_130[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_118 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=130 */
  LL_Switch_Init(switch_init_in_130, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 307200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_130_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_130_all_units, 2);

}

static void LL_ATON_End_EpochBlock_130(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_130[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_118 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=130 */
  LL_Switch_Deinit(switch_deinit_in_130, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_130_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_130_all_units, 2);

}


/* scheduling epoch=131  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_131(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_119 */
  Conv_sw_info conv103_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 192,
    .general.input.dim.num_elem = 76800,
    .general.input.stride.b = 307200,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 768,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 192,
    .weights.dim.num_elem = 24576,
    .weights.stride.b = 768,
    .weights.stride.h = 768,
    .weights.stride.w = 768,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11124736))) /* Equivalent hex address = 0x71a9c000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12078912))) /* Equivalent hex address = 0x71b84f40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_119 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv103_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=132  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_132(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_120 */
  Activ_sw_info activ104_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_120 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ104_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=133  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_133(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** NPU cache clean & invalidate operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) */
  LL_ATON_Cache_NPU_Clean_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) /* Equivalent hex address = 0x9012c000UL */, 204800);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_121 */
  Arith_sw_info arith105_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) /* Equivalent hex address = 0x9012c000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_121 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith105_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1433600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) /* Equivalent hex address = 0x9012c000UL */, 204800);

}


/* scheduling epoch=134  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Resize_122_resize_NN_expansion_concat_13 */

static void LL_ATON_Start_EpochBlock_134(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Resize_122_resize_NN_expansion_concat_13 */
  /* node=Resize_122_resize_NN_expansion_concat_13 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Resize_122_resize_NN_expansion_concat_13 input ports=0 range=7[1228800,1433600] */

  static const LL_Streng_TensorInitTypeDef Resize_122_resize_NN_expansion_concat_13_dma_init_in_0_134 = {
    /* from memory with batch=128
iterating outer iter=0 num_higher_elem=4
spanning across 819200 bytes */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .cacheable = 1,
    .cache_allocate = 1,
    .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */}, /* Mul_121_out_0 */
    .offset_start = 1228800,
    .offset_end = 1433600,
    .offset_limit = 1433664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 204800,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 1,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Resize_122_resize_NN_expansion_concat_13_dma_init_in_0_134, 1);


  /* Dma input bandwidth from memory pools: */
  /* hyperRAM -> 204800 */
  /* CACHE -> 614400 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Resize_122_resize_NN_expansion_concat_13 output ports=0 range=11[0,819200] */

  static const LL_Streng_TensorInitTypeDef Resize_122_resize_NN_expansion_concat_13_dma_init_out_0_134 = {
    /* to memory canonical from batch=128 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Resize_122_resize_NN_expansion_concat_13_out_14 */
    .offset_start = 0,
    .offset_limit = 819264,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 256,
    .batch_offset = 2048,
    .frame_offset = 512,
    .line_offset = 0,
    .loop_offset = 819200,
    .frame_loop_cnt = 4,
    .frame_tot_cnt = 4,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Resize_122_resize_NN_expansion_concat_13_dma_init_out_0_134, 1);


  /* Dma output bandwidth to memory pools: */
  /* cpuRAM2 <- 819200 */

  static const LL_Switch_InitTypeDef switch_init_in_134[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Resize_122_resize_NN_expansion_concat_13 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=134 */
  LL_Switch_Init(switch_init_in_134, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 819200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 819200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_134_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_134_all_units, 2);

}

static void LL_ATON_End_EpochBlock_134(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_134[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Resize_122_resize_NN_expansion_concat_13 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
  };


  /* epoch=134 */
  LL_Switch_Deinit(switch_deinit_in_134, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_134_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_134_all_units, 2);

}


/* scheduling epoch=135  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_135(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1458176))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2277376))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1458176))) /* Equivalent hex address = 0x34264000UL */, 819200);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=DepthToSpace node=Resize_122_resize_NN_to_expansion_dts_15 */
  static const uint32_t Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135__shape_1_512_20_20[] = { 1, 20, 20, 512 };
  static const uint32_t Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135__mem_shape_L_1_512_20_20[] = { 1, 20, 20, 512 };
  static const LL_Buffer_InfoTypeDef Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135[] = {
    {
      .name = "Resize_122_resize_NN_expansion_concat_13_out_14",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 135,
      .batch = 512,
      .mem_shape = Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135__mem_shape_L_1_512_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135__shape_1_512_20_20,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135__shape_1_128_40_40[] = { 1, 40, 40, 128 };
  static const uint32_t Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135__mem_shape_L_1_128_40_40[] = { 1, 40, 40, 128 };
  static const LL_Buffer_InfoTypeDef Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135[] = {
    {
      .name = "Resize_122_resize_NN_expansion_concat_13_out_16",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1458176,
      .offset_end = 2277376,
      .offset_limit = 2277440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 135,
      .batch = 128,
      .mem_shape = Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135__mem_shape_L_1_128_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135__shape_1_128_40_40,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_DMA_DepthToSpace(Resize_122_resize_NN_to_expansion_dts_15_tensor_info_in_135, 1, Resize_122_resize_NN_to_expansion_dts_15_tensor_info_out_135, 2, 2, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1458176))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2277376))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1458176))) /* Equivalent hex address = 0x34264000UL */, 819200);

}


/* scheduling epoch=136  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_136(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1228800);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_123 */
  static const uint32_t Concat_123_tensor_info_in_136__shape_1_128_40_40[] = { 1, 40, 40, 128 };
  static const uint32_t Concat_123_tensor_info_in_136__mem_shape_L_1_128_40_40[] = { 1, 40, 40, 128 };
  static const uint32_t Concat_123_tensor_info_in_136__shape_1_64_40_40[] = { 1, 40, 40, 64 };
  static const uint32_t Concat_123_tensor_info_in_136__mem_shape_L_1_64_40_40[] = { 1, 40, 40, 64 };
  static const LL_Buffer_InfoTypeDef Concat_123_tensor_info_in_136[] = {
    {
      .name = "Resize_122_resize_NN_expansion_concat_13_out_16",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1458176,
      .offset_end = 2277376,
      .offset_limit = 2277440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 136,
      .batch = 128,
      .mem_shape = Concat_123_tensor_info_in_136__mem_shape_L_1_128_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_123_tensor_info_in_136__shape_1_128_40_40,
    },
    {
      .name = "Mul_49_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 136,
      .batch = 64,
      .mem_shape = Concat_123_tensor_info_in_136__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_123_tensor_info_in_136__shape_1_64_40_40,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_123_tensor_info_out_136__shape_1_192_40_40[] = { 1, 40, 40, 192 };
  static const uint32_t Concat_123_tensor_info_out_136__mem_shape_L_1_192_40_40[] = { 1, 40, 40, 192 };
  static const LL_Buffer_InfoTypeDef Concat_123_tensor_info_out_136[] = {
    {
      .name = "Concat_123_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 136,
      .batch = 192,
      .mem_shape = Concat_123_tensor_info_out_136__mem_shape_L_1_192_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_123_tensor_info_out_136__shape_1_192_40_40,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_123_tensor_info_in_136, 2, Concat_123_tensor_info_out_136, 1, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 7 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 1228800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */, 1228800);

}


/* scheduling epoch=137  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_137(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_124 */
  Conv_sw_info conv106_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 192,
    .general.input.dim.num_elem = 307200,
    .general.input.stride.b = 1228800,
    .general.input.stride.h = 30720,
    .general.input.stride.w = 768,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x90000000UL + 0))) /* Equivalent hex address = 0x90000000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 192,
    .weights.dim.num_elem = 12288,
    .weights.stride.b = 768,
    .weights.stride.h = 768,
    .weights.stride.w = 768,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11558912))) /* Equivalent hex address = 0x71b06000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12082496))) /* Equivalent hex address = 0x71b85d40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_124 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv106_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=138  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_138(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_125 */
  Activ_sw_info activ107_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_125 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ107_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=139  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_139(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_126 */
  Arith_sw_info arith108_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_126 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith108_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=140  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id514 */

static void LL_ATON_Start_EpochBlock_140(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id514 */
  /* node=Identity_inserted_id514 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id514 input ports=0 range=3[0,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id514_dma_init_in_0_140 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_126_out_0_inserted_in514 */
    .offset_start = 0,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 409600,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Identity_inserted_id514_dma_init_in_0_140, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM3 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id514 output ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id514_dma_init_out_0_140 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_126_out_0_inserted_out514 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Identity_inserted_id514_dma_init_out_0_140, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_140[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id514 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=140 */
  LL_Switch_Init(switch_init_in_140, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_140_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_140_all_units, 2);

}

static void LL_ATON_End_EpochBlock_140(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_140[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id514 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=140 */
  LL_Switch_Deinit(switch_deinit_in_140, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_140_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_140_all_units, 2);

}


/* scheduling epoch=141  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_134 */

static void LL_ATON_Start_EpochBlock_141(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_134 */
  /* node=Slice_134 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_134 input ports=0 range=1[0,409600] */

  static const LL_Streng_TensorInitTypeDef Slice_134_dma_init_in_0_141 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_126_out_0 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Slice_134_dma_init_in_0_141, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_134 output ports=0 range=2[0,409600] */

  static const LL_Streng_TensorInitTypeDef Slice_134_dma_init_out_0_141 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_134_out_0 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Slice_134_dma_init_out_0_141, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_141[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_134 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=141 */
  LL_Switch_Init(switch_init_in_141, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_141_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_141_all_units, 2);

}

static void LL_ATON_End_EpochBlock_141(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_141[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_134 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=141 */
  LL_Switch_Deinit(switch_deinit_in_141, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_141_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_141_all_units, 2);

}


/* scheduling epoch=142  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id518 */
/* no resources allocated to kind=Identity node=Identity_inserted_id519 */

static void LL_ATON_Start_EpochBlock_142(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id518 */
  /* node=Identity_inserted_id518 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id519 */
  /* node=Identity_inserted_id519 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id518 input ports=0 range=2[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id518_dma_init_in_0_142 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_134_out_0_inserted_in518 */
    .offset_start = 0,
    .offset_end = 6400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id518_dma_init_in_0_142, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id519 input ports=0 range=2[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id519_dma_init_in_0_142 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Slice_127_out_0_inserted_in519 */
    .offset_start = 204800,
    .offset_end = 211200,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 6400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id519_dma_init_in_0_142, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM4 -> 409600 */

  /* Dma output units from cycle: */
  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id518 output ports=0 range=11[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id518_dma_init_out_0_142 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_134_out_0_inserted_out518 */
    .offset_start = 0,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 128,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 32,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Identity_inserted_id518_dma_init_out_0_142, 1);

  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id519 output ports=0 range=11[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id519_dma_init_out_0_142 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_127_out_0_inserted_out519 */
    .offset_start = 204800,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 128,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 32,
    .frame_tot_cnt = 32,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id519_dma_init_out_0_142, 1);


  /* Dma output bandwidth to memory pools: */
  /* cpuRAM2 <- 409600 */

  static const LL_Switch_InitTypeDef switch_init_in_142[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id518 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id519 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=142 */
  LL_Switch_Init(switch_init_in_142, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) /* Equivalent hex address = 0x34132000UL */, 204800);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_142_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_142_all_units, 4);

}

static void LL_ATON_End_EpochBlock_142(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_142[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id518 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id519 OUT: in unit=STREAM_ENG_V2 0 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=142 */
  LL_Switch_Deinit(switch_deinit_in_142, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_142_all_units[] = {
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_142_all_units, 4);

}


/* scheduling epoch=143  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_143(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_128 */
  Conv_sw_info conv109_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 204800))) /* Equivalent hex address = 0x34132000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11755520))) /* Equivalent hex address = 0x71b36000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090048))) /* Equivalent hex address = 0x71b87ac0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_128 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv109_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=144  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_144(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_129 */
  Activ_sw_info activ110_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_129 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ110_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=145  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_145(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_130 */
  Arith_sw_info arith111_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_130 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith111_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=146  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_146(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_131 */
  Conv_sw_info conv112_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 32,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 32,
    .weights.dim.num_elem = 9216,
    .weights.stride.b = 1152,
    .weights.stride.h = 384,
    .weights.stride.w = 128,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11792384))) /* Equivalent hex address = 0x71b3f000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 32,
    .bias.dim.num_elem = 32,
    .bias.stride.b = 128,
    .bias.stride.h = 128,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090176))) /* Equivalent hex address = 0x71b87b40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_131 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv112_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=147  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_147(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_132 */
  Activ_sw_info activ113_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_132 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ113_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=148  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_148(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_133 */
  Arith_sw_info arith114_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 32,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 128,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 32,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 5120,
    .operand.stride.w = 128,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 32,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 128,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_133 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith114_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 409600))) /* Equivalent hex address = 0x34164000UL */, 204800);

}


/* scheduling epoch=149  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_135 */

static void LL_ATON_Start_EpochBlock_149(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_135 */
  /* node=Concat_135 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_135 input ports=0 range=11[0,614400] */

  static const LL_Streng_TensorInitTypeDef Concat_135_dma_init_in_0_149 = {
    /* from memory with batch=32 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Slice_134_out_0 */
    .offset_start = 0,
    .offset_end = 204800,
    .offset_limit = 614464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 204800,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Concat_135_dma_init_in_0_149, 1);


  /* Dma input bandwidth from memory pools: */
  /* cpuRAM2 -> 614400 */

  /* Dma output units from cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_135 output ports=0 range=11[614400,1228800] */

  static const LL_Streng_TensorInitTypeDef Concat_135_dma_init_out_0_149 = {
    /* to memory canonical from batch=32 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Concat_135_out_0 */
    .offset_start = 614400,
    .offset_limit = 1228864,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 64,
    .batch_offset = 384,
    .frame_offset = 128,
    .line_offset = 0,
    .loop_offset = 614400,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Concat_135_dma_init_out_0_149, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM3 <- 180224 */
  /* cpuRAM2 <- 434176 */

  static const LL_Switch_InitTypeDef switch_init_in_149[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_135 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=149 */
  LL_Switch_Init(switch_init_in_149, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 1228800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) /* Equivalent hex address = 0x34196000UL */, 614400);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_149_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_149_all_units, 2);

}

static void LL_ATON_End_EpochBlock_149(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_149[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_135 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=149 */
  LL_Switch_Deinit(switch_deinit_in_149, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_149_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_149_all_units, 2);

}


/* scheduling epoch=150  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_150(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_136 */
  Conv_sw_info conv115_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 96,
    .general.input.dim.num_elem = 153600,
    .general.input.stride.b = 614400,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 384,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 614400))) /* Equivalent hex address = 0x34196000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 96,
    .weights.dim.num_elem = 6144,
    .weights.stride.b = 384,
    .weights.stride.h = 384,
    .weights.stride.w = 384,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11895616))) /* Equivalent hex address = 0x71b58340UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12082752))) /* Equivalent hex address = 0x71b85e40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_136 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv115_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=151  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_151(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_137 */
  Activ_sw_info activ116_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_137 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ116_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=152  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_152(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_138 */
  Arith_sw_info arith117_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_138 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith117_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 409600);

}


/* scheduling epoch=153  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_153(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_139 */
  Conv_sw_info conv118_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8355840))) /* Equivalent hex address = 0x717f8000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12083008))) /* Equivalent hex address = 0x71b85f40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_139 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv118_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=154  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_154(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_140 */
  Activ_sw_info activ119_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_140 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ119_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=155  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_155(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_141 */
  Arith_sw_info arith120_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_141 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith120_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 409600);

}


/* scheduling epoch=156  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_156(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_142 */
  Conv_sw_info conv121_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8503296))) /* Equivalent hex address = 0x7181c000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12083264))) /* Equivalent hex address = 0x71b86040UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_142 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv121_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=157  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_157(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_143 */
  Activ_sw_info activ122_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_143 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ122_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=158  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_158(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_144 */
  Arith_sw_info arith123_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_144 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith123_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 409600);

}


/* scheduling epoch=159  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_159(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_145 */
  Conv_sw_info conv124_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 1,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 64,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12088384))) /* Equivalent hex address = 0x71b87440UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1,
    .bias.dim.num_elem = 1,
    .bias.stride.b = 4,
    .bias.stride.h = 4,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090816))) /* Equivalent hex address = 0x71b87dc0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 1600,
    .general.output.stride.b = 6400,
    .general.output.stride.h = 160,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 416000))) /* Equivalent hex address = 0x34345900UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_145 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv124_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 416000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 422400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 416000))) /* Equivalent hex address = 0x34345900UL */, 6400);

}


/* scheduling epoch=160  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_160(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_146 */
  Conv_sw_info conv125_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8650752))) /* Equivalent hex address = 0x71840000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12083520))) /* Equivalent hex address = 0x71b86140UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_146 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv125_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=161  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_161(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_147 */
  Activ_sw_info activ126_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_147 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ126_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=162  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_162(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_148 */
  Arith_sw_info arith127_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_148 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith127_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 409600);

}


/* scheduling epoch=163  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_163(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_149 */
  Conv_sw_info conv128_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 8798208))) /* Equivalent hex address = 0x71864000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12083776))) /* Equivalent hex address = 0x71b86240UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_149 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv128_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 409600);

}


/* scheduling epoch=164  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_164(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_150 */
  Activ_sw_info activ129_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_150 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ129_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=165  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_165(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_151 */
  Arith_sw_info arith130_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 40,
    .operand.dim.tensor_w = 40,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 102400,
    .operand.stride.b = 409600,
    .operand.stride.h = 10240,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_151 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith130_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 0 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */, 409600);

}


/* scheduling epoch=166  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_166(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_152 */
  Conv_sw_info conv131_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34350000UL + 0))) /* Equivalent hex address = 0x34350000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 4096,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11972224))) /* Equivalent hex address = 0x71b6ae80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12081472))) /* Equivalent hex address = 0x71b85940UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 40,
    .general.output.dim.tensor_w = 40,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 102400,
    .general.output.stride.b = 409600,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_152 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv131_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 409600);

}


/* scheduling epoch=167  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_167(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 416000))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 416000);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_153 */
  static const uint32_t Concat_153_tensor_info_in_167__shape_1_64_40_40[] = { 1, 40, 40, 64 };
  static const uint32_t Concat_153_tensor_info_in_167__mem_shape_L_1_64_40_40[] = { 1, 40, 40, 64 };
  static const uint32_t Concat_153_tensor_info_in_167__shape_1_1_40_40[] = { 1, 40, 40, 1 };
  static const uint32_t Concat_153_tensor_info_in_167__mem_shape_F_1_1_40_40[] = { 1, 1, 40, 40 };
  static const LL_Buffer_InfoTypeDef Concat_153_tensor_info_in_167[] = {
    {
      .name = "Conv2D_152_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 167,
      .batch = 64,
      .mem_shape = Concat_153_tensor_info_in_167__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_153_tensor_info_in_167__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_145_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 416000,
      .offset_end = 422400,
      .offset_limit = 422464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 167,
      .batch = 1,
      .mem_shape = Concat_153_tensor_info_in_167__mem_shape_F_1_1_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_153_tensor_info_in_167__shape_1_1_40_40,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_153_tensor_info_out_167__shape_1_65_40_40[] = { 1, 40, 40, 65 };
  static const uint32_t Concat_153_tensor_info_out_167__mem_shape_L_1_65_40_40[] = { 1, 40, 40, 65 };
  static const LL_Buffer_InfoTypeDef Concat_153_tensor_info_out_167[] = {
    {
      .name = "Concat_153_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 416000,
      .offset_limit = 416064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 167,
      .batch = 65,
      .mem_shape = Concat_153_tensor_info_out_167__mem_shape_L_1_65_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_153_tensor_info_out_167__shape_1_65_40_40,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_153_tensor_info_in_167, 2, Concat_153_tensor_info_out_167, 1, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 416000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 416000);

}


/* scheduling epoch=168  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_168(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_155 */
  Conv_sw_info conv132_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 40,
    .general.input.dim.tensor_w = 40,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 102400,
    .general.input.stride.b = 409600,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9830400))) /* Equivalent hex address = 0x71960000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12086080))) /* Equivalent hex address = 0x71b86b40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_155 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv132_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 102400);

}


/* scheduling epoch=169  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_169(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_156 */
  Activ_sw_info activ133_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_156 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ133_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */, 102400);

}


/* scheduling epoch=170  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_170(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_157 */
  Arith_sw_info arith134_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 102400))) /* Equivalent hex address = 0x34289000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_157 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith134_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) /* Equivalent hex address = 0x342bb000UL */, 102400);

}


/* scheduling epoch=171  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_171(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 307200);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_158 */
  static const uint32_t Concat_158_tensor_info_in_171__shape_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t Concat_158_tensor_info_in_171__mem_shape_L_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t Concat_158_tensor_info_in_171__shape_1_128_20_20[] = { 1, 20, 20, 128 };
  static const uint32_t Concat_158_tensor_info_in_171__mem_shape_L_1_128_20_20[] = { 1, 20, 20, 128 };
  static const LL_Buffer_InfoTypeDef Concat_158_tensor_info_in_171[] = {
    {
      .name = "Mul_157_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 171,
      .batch = 64,
      .mem_shape = Concat_158_tensor_info_in_171__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_158_tensor_info_in_171__shape_1_64_20_20,
    },
    {
      .name = "Mul_121_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1228800,
      .offset_end = 1433600,
      .offset_limit = 1433664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 171,
      .batch = 128,
      .mem_shape = Concat_158_tensor_info_in_171__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_158_tensor_info_in_171__shape_1_128_20_20,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_158_tensor_info_out_171__shape_1_192_20_20[] = { 1, 20, 20, 192 };
  static const uint32_t Concat_158_tensor_info_out_171__mem_shape_L_1_192_20_20[] = { 1, 20, 20, 192 };
  static const LL_Buffer_InfoTypeDef Concat_158_tensor_info_out_171[] = {
    {
      .name = "Concat_158_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 171,
      .batch = 192,
      .mem_shape = Concat_158_tensor_info_out_171__mem_shape_L_1_192_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_158_tensor_info_out_171__shape_1_192_20_20,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_158_tensor_info_in_171, 2, Concat_158_tensor_info_out_171, 1, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 307200);

}


/* scheduling epoch=172  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_172(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_159 */
  Conv_sw_info conv135_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 192,
    .general.input.dim.num_elem = 76800,
    .general.input.stride.b = 307200,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 768,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 192,
    .weights.dim.num_elem = 24576,
    .weights.stride.b = 768,
    .weights.stride.h = 768,
    .weights.stride.w = 768,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11223040))) /* Equivalent hex address = 0x71ab4000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12079424))) /* Equivalent hex address = 0x71b85140UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_159 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv135_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 204800);

}


/* scheduling epoch=173  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_173(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_160 */
  Activ_sw_info activ136_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_160 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ136_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 204800);

}


/* scheduling epoch=174  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_174(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_161 */
  Arith_sw_info arith137_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) /* Equivalent hex address = 0x34232000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_161 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith137_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) /* Equivalent hex address = 0x34232000UL */, 204800);

}


/* scheduling epoch=175  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Reshape node=Reshape_154 */
/* no resources allocated to kind=Identity node=Identity_inserted_id521 */

static void LL_ATON_Start_EpochBlock_175(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Reshape node=Reshape_154 */
  /* node=Reshape_154 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id521 */
  /* node=Identity_inserted_id521 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_154 input ports=0 range=1[0,416000] */

  static const LL_Streng_TensorInitTypeDef Reshape_154_dma_init_in_0_175 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_153_out_0 */
    .offset_start = 0,
    .offset_limit = 416064,
    .frame_count = 0,
    .fwidth = 40,
    .fheight = 40,
    .batch_depth = 2,
    .batch_offset = 260,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 416000,
    .frame_loop_cnt = 65,
    .frame_tot_cnt = 65,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Reshape_154_dma_init_in_0_175, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id521 input ports=0 range=3[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id521_dma_init_in_0_175 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_161_out_0_inserted_in521 */
    .offset_start = 204800,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 204800,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id521_dma_init_in_0_175, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 416000 */
  /* npuRAM3 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_154 output ports=0 range=2[0,416000] */

  static const LL_Streng_TensorInitTypeDef Reshape_154_dma_init_out_0_175 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */}, /* Reshape_154_out_0 */
    .offset_start = 0,
    .offset_end = 416000,
    .offset_limit = 416064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 416000,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Reshape_154_dma_init_out_0_175, 1);

  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id521 output ports=0 range=3[0,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id521_dma_init_out_0_175 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_161_out_0_inserted_out521 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id521_dma_init_out_0_175, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM4 <- 416000 */
  /* npuRAM3 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_175[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_154 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id521 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=175 */
  LL_Switch_Init(switch_init_in_175, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 2 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 416000))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34270000UL + 0))) /* Equivalent hex address = 0x34270000UL */, 416000);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_175_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_175_all_units, 4);

}

static void LL_ATON_End_EpochBlock_175(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_175[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_154 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id521 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=175 */
  LL_Switch_Deinit(switch_deinit_in_175, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_175_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_175_all_units, 4);

}


/* scheduling epoch=176  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_169 */

static void LL_ATON_Start_EpochBlock_176(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_169 */
  /* node=Slice_169 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_169 input ports=0 range=3[0,204800] */

  static const LL_Streng_TensorInitTypeDef Slice_169_dma_init_in_0_176 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_161_out_0 */
    .offset_start = 0,
    .offset_end = 1600,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Slice_169_dma_init_in_0_176, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM3 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_169 output ports=0 range=3[204800,409600] */

  static const LL_Streng_TensorInitTypeDef Slice_169_dma_init_out_0_176 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Slice_169_out_0 */
    .offset_start = 204800,
    .offset_end = 206400,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Slice_169_dma_init_out_0_176, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM3 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_176[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_169 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
  };


  /* epoch=176 */
  LL_Switch_Init(switch_init_in_176, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_176_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_176_all_units, 2);

}

static void LL_ATON_End_EpochBlock_176(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_176[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_169 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 8 out port=0 */
  };


  /* epoch=176 */
  LL_Switch_Deinit(switch_deinit_in_176, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_176_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_176_all_units, 2);

}


/* scheduling epoch=177  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id525 */
/* no resources allocated to kind=Identity node=Identity_inserted_id526 */

static void LL_ATON_Start_EpochBlock_177(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id525 */
  /* node=Identity_inserted_id525 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id526 */
  /* node=Identity_inserted_id526 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id525 input ports=0 range=3[204800,307200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id525_dma_init_in_0_177 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Slice_169_out_0_inserted_in525 */
    .offset_start = 204800,
    .offset_end = 206400,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Identity_inserted_id525_dma_init_in_0_177, 1);

  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id526 input ports=0 range=3[307200,409600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id526_dma_init_in_0_177 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Slice_162_out_0_inserted_in526 */
    .offset_start = 307200,
    .offset_end = 308800,
    .offset_limit = 409664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 1600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Identity_inserted_id526_dma_init_in_0_177, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM3 -> 204800 */

  /* Dma output units from cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id525 output ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id525_dma_init_out_0_177 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_169_out_0_inserted_out525 */
    .offset_start = 0,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id525_dma_init_out_0_177, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id526 output ports=0 range=1[102400,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id526_dma_init_out_0_177 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_162_out_0_inserted_out526 */
    .offset_start = 102400,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 256,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 64,
    .frame_tot_cnt = 64,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id526_dma_init_out_0_177, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 204800 */

  static const LL_Switch_InitTypeDef switch_init_in_177[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id525 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id526 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=177 */
  LL_Switch_Init(switch_init_in_177, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_177_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_177_all_units, 4);

}

static void LL_ATON_End_EpochBlock_177(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_177[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id525 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 4 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id526 OUT: in unit=STREAM_ENG_V2 6 in port=0 out unit=STREAM_ENG_V2 9 out port=0 */
  };


  /* epoch=177 */
  LL_Switch_Deinit(switch_deinit_in_177, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_177_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_177_all_units, 4);

}


/* scheduling epoch=178  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_178(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_163 */
  Conv_sw_info conv138_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 9977856))) /* Equivalent hex address = 0x71984000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12086336))) /* Equivalent hex address = 0x71b86c40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_163 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv138_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=179  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_179(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_164 */
  Activ_sw_info activ139_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_164 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ139_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 102400);

}


/* scheduling epoch=180  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_180(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_165 */
  Arith_sw_info arith140_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) /* Equivalent hex address = 0x34219000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_165 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith140_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) /* Equivalent hex address = 0x34219000UL */, 102400);

}


/* scheduling epoch=181  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_181(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_166 */
  Conv_sw_info conv141_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) /* Equivalent hex address = 0x34219000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10125312))) /* Equivalent hex address = 0x719a8000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12086592))) /* Equivalent hex address = 0x71b86d40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_166 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv141_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=182  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_182(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_167 */
  Activ_sw_info activ142_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_167 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ142_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 102400);

}


/* scheduling epoch=183  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_183(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_168 */
  Arith_sw_info arith143_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_168 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith143_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=184  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_170 */

static void LL_ATON_Start_EpochBlock_184(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_170 */
  /* node=Concat_170 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_170 input ports=0 range=1[0,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_170_dma_init_in_0_184 = {
    /* from memory with batch=64 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_169_out_0 */
    .offset_start = 0,
    .offset_end = 102400,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 102400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Concat_170_dma_init_in_0_184, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 307200 */

  /* Dma output units from cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_170 output ports=0 range=3[0,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_170_dma_init_out_0_184 = {
    /* to memory canonical from batch=64 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Concat_170_out_0 */
    .offset_start = 0,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 128,
    .batch_offset = 768,
    .frame_offset = 256,
    .line_offset = 0,
    .loop_offset = 307200,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Concat_170_dma_init_out_0_184, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM3 <- 307200 */

  static const LL_Switch_InitTypeDef switch_init_in_184[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_170 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=184 */
  LL_Switch_Init(switch_init_in_184, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 307200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 307200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_184_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_184_all_units, 2);

}

static void LL_ATON_End_EpochBlock_184(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_184[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_170 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=184 */
  LL_Switch_Deinit(switch_deinit_in_184, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_184_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_184_all_units, 2);

}


/* scheduling epoch=185  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_185(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_171 */
  Conv_sw_info conv144_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 192,
    .general.input.dim.num_elem = 76800,
    .general.input.stride.b = 307200,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 768,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 192,
    .weights.dim.num_elem = 24576,
    .weights.stride.b = 768,
    .weights.stride.h = 768,
    .weights.stride.w = 768,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11321344))) /* Equivalent hex address = 0x71acc000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12079936))) /* Equivalent hex address = 0x71b85340UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_171 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv144_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 204800);

}


/* scheduling epoch=186  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_186(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_172 */
  Activ_sw_info activ145_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_172 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ145_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 204800);

}


/* scheduling epoch=187  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_187(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_173 */
  Arith_sw_info arith146_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 51200,
    .operand.stride.b = 204800,
    .operand.stride.h = 10240,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 51200,
    .general.output.stride.b = 204800,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_173 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith146_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 204800);

}


/* scheduling epoch=188  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_188(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_174 */
  Conv_sw_info conv147_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 73728,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 7307264))) /* Equivalent hex address = 0x716f8000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12084032))) /* Equivalent hex address = 0x71b86340UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_174 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv147_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=189  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_189(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_175 */
  Activ_sw_info activ148_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_175 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ148_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=190  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_190(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_176 */
  Arith_sw_info arith149_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_176 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith149_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=191  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_191(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_177 */
  Conv_sw_info conv150_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10272768))) /* Equivalent hex address = 0x719cc000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12086848))) /* Equivalent hex address = 0x71b86e40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_177 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv150_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=192  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_192(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_178 */
  Activ_sw_info activ151_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_178 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ151_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=193  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_193(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_179 */
  Arith_sw_info arith152_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_179 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith152_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=194  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_194(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_180 */
  Conv_sw_info conv153_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 1,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 64,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12088640))) /* Equivalent hex address = 0x71b87540UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1,
    .bias.dim.num_elem = 1,
    .bias.stride.b = 4,
    .bias.stride.h = 4,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090832))) /* Equivalent hex address = 0x71b87dd0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 400,
    .general.output.stride.b = 1600,
    .general.output.stride.h = 80,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) /* Equivalent hex address = 0x34344000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_180 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv153_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) /* Equivalent hex address = 0x34344000UL */, 1600);

}


/* scheduling epoch=195  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_195(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_181 */
  Conv_sw_info conv154_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 73728,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 7602176))) /* Equivalent hex address = 0x71740000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12084288))) /* Equivalent hex address = 0x71b86440UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_181 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv154_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=196  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_196(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_182 */
  Activ_sw_info activ155_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_182 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ155_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=197  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_197(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_183 */
  Arith_sw_info arith156_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_183 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith156_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=198  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_198(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_184 */
  Conv_sw_info conv157_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10420224))) /* Equivalent hex address = 0x719f0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12087104))) /* Equivalent hex address = 0x71b86f40UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_184 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv157_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 409600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 102400);

}


/* scheduling epoch=199  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_199(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_185 */
  Activ_sw_info activ158_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_185 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ158_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=200  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_200(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_186 */
  Arith_sw_info arith159_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 20,
    .operand.dim.tensor_w = 20,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 5120,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_186 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith159_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=201  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_201(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_187 */
  Conv_sw_info conv160_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 4096,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12004992))) /* Equivalent hex address = 0x71b72e80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12087872))) /* Equivalent hex address = 0x71b87240UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 20,
    .general.output.dim.tensor_w = 20,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_187 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv160_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=202  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_202(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 104000);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_188 */
  static const uint32_t Concat_188_tensor_info_in_202__shape_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t Concat_188_tensor_info_in_202__mem_shape_L_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t Concat_188_tensor_info_in_202__shape_1_1_20_20[] = { 1, 20, 20, 1 };
  static const uint32_t Concat_188_tensor_info_in_202__mem_shape_F_1_1_20_20[] = { 1, 1, 20, 20 };
  static const LL_Buffer_InfoTypeDef Concat_188_tensor_info_in_202[] = {
    {
      .name = "Conv2D_187_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 202,
      .batch = 64,
      .mem_shape = Concat_188_tensor_info_in_202__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_188_tensor_info_in_202__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_180_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 409600,
      .offset_end = 411200,
      .offset_limit = 411264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 202,
      .batch = 1,
      .mem_shape = Concat_188_tensor_info_in_202__mem_shape_F_1_1_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_188_tensor_info_in_202__shape_1_1_20_20,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_188_tensor_info_out_202__shape_1_65_20_20[] = { 1, 20, 20, 65 };
  static const uint32_t Concat_188_tensor_info_out_202__mem_shape_L_1_65_20_20[] = { 1, 20, 20, 65 };
  static const LL_Buffer_InfoTypeDef Concat_188_tensor_info_out_202[] = {
    {
      .name = "Concat_188_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 257600,
      .offset_limit = 257664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 202,
      .batch = 65,
      .mem_shape = Concat_188_tensor_info_out_202__mem_shape_L_1_65_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_188_tensor_info_out_202__shape_1_65_20_20,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_188_tensor_info_in_202, 2, Concat_188_tensor_info_out_202, 1, 8, 9);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 104000);

}


/* scheduling epoch=203  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_203(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_190 */
  Conv_sw_info conv161_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 20,
    .general.input.dim.tensor_w = 20,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 51200,
    .general.input.stride.b = 204800,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 2359296))) /* Equivalent hex address = 0x71240000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12075328))) /* Equivalent hex address = 0x71b84140UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {2, 2},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_190 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv161_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 51200);

}


/* scheduling epoch=204  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_204(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_191 */
  Activ_sw_info activ162_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_191 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ162_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 51200);

}


/* scheduling epoch=205  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_205(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_192 */
  Arith_sw_info arith163_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_192 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith163_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 308800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */, 51200);

}


/* scheduling epoch=206  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_206(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 153600);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_193 */
  static const uint32_t Concat_193_tensor_info_in_206__shape_1_128_10_10[] = { 1, 10, 10, 128 };
  static const uint32_t Concat_193_tensor_info_in_206__mem_shape_L_1_128_10_10[] = { 1, 10, 10, 128 };
  static const uint32_t Concat_193_tensor_info_in_206__shape_1_256_10_10[] = { 1, 10, 10, 256 };
  static const uint32_t Concat_193_tensor_info_in_206__mem_shape_L_1_256_10_10[] = { 1, 10, 10, 256 };
  static const LL_Buffer_InfoTypeDef Concat_193_tensor_info_in_206[] = {
    {
      .name = "Mul_192_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 257600,
      .offset_end = 308800,
      .offset_limit = 308864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 206,
      .batch = 128,
      .mem_shape = Concat_193_tensor_info_in_206__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_193_tensor_info_in_206__shape_1_128_10_10,
    },
    {
      .name = "Mul_104_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1433600,
      .offset_end = 1536000,
      .offset_limit = 1536064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 206,
      .batch = 256,
      .mem_shape = Concat_193_tensor_info_in_206__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_193_tensor_info_in_206__shape_1_256_10_10,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_193_tensor_info_out_206__shape_1_384_10_10[] = { 1, 10, 10, 384 };
  static const uint32_t Concat_193_tensor_info_out_206__mem_shape_L_1_384_10_10[] = { 1, 10, 10, 384 };
  static const LL_Buffer_InfoTypeDef Concat_193_tensor_info_out_206[] = {
    {
      .name = "Concat_193_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 153600,
      .offset_limit = 153664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 206,
      .batch = 384,
      .mem_shape = Concat_193_tensor_info_out_206__mem_shape_L_1_384_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_193_tensor_info_out_206__shape_1_384_10_10,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_193_tensor_info_in_206, 2, Concat_193_tensor_info_out_206, 1, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 153600);

}


/* scheduling epoch=207  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_207(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_194 */
  Conv_sw_info conv164_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 384,
    .general.input.dim.num_elem = 38400,
    .general.input.stride.b = 153600,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 1536,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 384,
    .weights.dim.num_elem = 98304,
    .weights.stride.b = 1536,
    .weights.stride.h = 1536,
    .weights.stride.w = 1536,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 6225920))) /* Equivalent hex address = 0x715f0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12071232))) /* Equivalent hex address = 0x71b83140UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_194 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv164_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 360000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */, 102400);

}


/* scheduling epoch=208  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_208(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_195 */
  Activ_sw_info activ165_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_195 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ165_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=209  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_209(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_196 */
  Arith_sw_info arith166_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 257600))) /* Equivalent hex address = 0x3431ee40UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_196 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith166_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 3 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 0))) /* Equivalent hex address = 0x34200000UL */, 102400);

}


/* scheduling epoch=210  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Reshape node=Reshape_189 */
/* no resources allocated to kind=Identity node=Identity_inserted_id528 */

static void LL_ATON_Start_EpochBlock_210(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Reshape node=Reshape_189 */
  /* node=Reshape_189 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id528 */
  /* node=Identity_inserted_id528 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_189 input ports=0 range=1[153600,257600] */

  static const LL_Streng_TensorInitTypeDef Reshape_189_dma_init_in_0_210 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_188_out_0 */
    .offset_start = 153600,
    .offset_limit = 257664,
    .frame_count = 0,
    .fwidth = 20,
    .fheight = 20,
    .batch_depth = 2,
    .batch_offset = 260,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 104000,
    .frame_loop_cnt = 65,
    .frame_tot_cnt = 65,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Reshape_189_dma_init_in_0_210, 1);

  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id528 input ports=0 range=3[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id528_dma_init_in_0_210 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */}, /* Mul_196_out_0_inserted_in528 */
    .offset_start = 0,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 1024,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 102400,
    .frame_loop_cnt = 256,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Identity_inserted_id528_dma_init_in_0_210, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 104000 */
  /* npuRAM3 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_189 output ports=0 range=1[307200,411200] */

  static const LL_Streng_TensorInitTypeDef Reshape_189_dma_init_out_0_210 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Reshape_189_out_0 */
    .offset_start = 307200,
    .offset_end = 411200,
    .offset_limit = 411264,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 104000,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Reshape_189_dma_init_out_0_210, 1);

  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id528 output ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id528_dma_init_out_0_210 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_196_out_0_inserted_out528 */
    .offset_start = 0,
    .offset_end = 400,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Identity_inserted_id528_dma_init_out_0_210, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 206400 */

  static const LL_Switch_InitTypeDef switch_init_in_210[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_189 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id528 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=210 */
  LL_Switch_Init(switch_init_in_210, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) /* Equivalent hex address = 0x3432b000UL */, 104000);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_210_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_210_all_units, 4);

}

static void LL_ATON_End_EpochBlock_210(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_210[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_189 OUT: in unit=STREAM_ENG_V2 7 in port=0 out unit=STREAM_ENG_V2 3 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id528 OUT: in unit=STREAM_ENG_V2 2 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=210 */
  LL_Switch_Deinit(switch_deinit_in_210, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_210_all_units[] = {
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_210_all_units, 4);

}


/* scheduling epoch=211  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_204 */

static void LL_ATON_Start_EpochBlock_211(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_204 */
  /* node=Slice_204 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 6 [STREAM_ENG_V2 6] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_204 input ports=0 range=1[0,102400] */

  static const LL_Streng_TensorInitTypeDef Slice_204_dma_init_in_0_211 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_196_out_0 */
    .offset_start = 0,
    .offset_end = 400,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(6, &Slice_204_dma_init_in_0_211, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_204 output ports=0 range=1[153600,256000] */

  static const LL_Streng_TensorInitTypeDef Slice_204_dma_init_out_0_211 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_204_out_0 */
    .offset_start = 153600,
    .offset_end = 154000,
    .offset_limit = 256064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 256,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Slice_204_dma_init_out_0_211, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 102400 */

  static const LL_Switch_InitTypeDef switch_init_in_211[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_204 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=211 */
  LL_Switch_Init(switch_init_in_211, 1);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_211_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_211_all_units, 2);

}

static void LL_ATON_End_EpochBlock_211(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_211[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 6, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_204 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 6 out port=0 */
  };


  /* epoch=211 */
  LL_Switch_Deinit(switch_deinit_in_211, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_211_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 6} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_211_all_units, 2);

}


/* scheduling epoch=212  nodes=2   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id532 */
/* no resources allocated to kind=Identity node=Identity_inserted_id533 */

static void LL_ATON_Start_EpochBlock_212(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id532 */
  /* node=Identity_inserted_id532 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id533 */
  /* node=Identity_inserted_id533 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id532 input ports=0 range=1[153600,204800] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id532_dma_init_in_0_212 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_204_out_0_inserted_in532 */
    .offset_start = 153600,
    .offset_end = 154000,
    .offset_limit = 204864,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Identity_inserted_id532_dma_init_in_0_212, 1);

  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id533 input ports=0 range=1[204800,256000] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id533_dma_init_in_0_212 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_197_out_0_inserted_in533 */
    .offset_start = 204800,
    .offset_end = 205200,
    .offset_limit = 256064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 400,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Identity_inserted_id533_dma_init_in_0_212, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 102400 */

  /* Dma output units from cycle: */
  /* Unit= 3 [STREAM_ENG_V2 3] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id532 output ports=0 range=1[0,51200] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id532_dma_init_out_0_212 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_204_out_0_inserted_out532 */
    .offset_start = 0,
    .offset_limit = 51264,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 51200,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(3, &Identity_inserted_id532_dma_init_out_0_212, 1);

  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id533 output ports=0 range=1[51200,102400] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id533_dma_init_out_0_212 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_197_out_0_inserted_out533 */
    .offset_start = 51200,
    .offset_limit = 102464,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 512,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 51200,
    .frame_loop_cnt = 128,
    .frame_tot_cnt = 128,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Identity_inserted_id533_dma_init_out_0_212, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 102400 */

  static const LL_Switch_InitTypeDef switch_init_in_212[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id532 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id533 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=212 */
  LL_Switch_Init(switch_init_in_212, 2);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 51200);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_212_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_212_all_units, 4);

}

static void LL_ATON_End_EpochBlock_212(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_212[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 3, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id532 OUT: in unit=STREAM_ENG_V2 3 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id533 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=212 */
  LL_Switch_Deinit(switch_deinit_in_212, 2);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_212_all_units[] = {
    { {STRENG, 3} }, /* STREAM_ENG_V2 */
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_212_all_units, 4);

}


/* scheduling epoch=213  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_213(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_198 */
  Conv_sw_info conv167_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 2949120))) /* Equivalent hex address = 0x712d0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12075840))) /* Equivalent hex address = 0x71b84340UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_198 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv167_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 51200);

}


/* scheduling epoch=214  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_214(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_199 */
  Activ_sw_info activ168_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_199 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ168_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 51200);

}


/* scheduling epoch=215  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_215(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_200 */
  Arith_sw_info arith169_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_200 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith169_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */, 51200);

}


/* scheduling epoch=216  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_216(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_201 */
  Conv_sw_info conv170_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) /* Equivalent hex address = 0x3431e800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 128,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 128,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 4608,
    .weights.stride.h = 1536,
    .weights.stride.w = 512,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 3538944))) /* Equivalent hex address = 0x71360000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 128,
    .bias.dim.num_elem = 128,
    .bias.stride.b = 512,
    .bias.stride.h = 512,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12076352))) /* Equivalent hex address = 0x71b84540UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_201 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv170_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 51200);

}


/* scheduling epoch=217  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_217(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_202 */
  Activ_sw_info activ171_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_202 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ171_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 256000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 51200);

}


/* scheduling epoch=218  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_218(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_203 */
  Arith_sw_info arith172_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 128,
    .general.input.dim.num_elem = 12800,
    .general.input.stride.b = 51200,
    .general.input.stride.h = 5120,
    .general.input.stride.w = 512,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 128,
    .operand.dim.num_elem = 12800,
    .operand.stride.b = 51200,
    .operand.stride.h = 5120,
    .operand.stride.w = 512,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 128,
    .general.output.dim.num_elem = 12800,
    .general.output.stride.b = 51200,
    .general.output.stride.h = 5120,
    .general.output.stride.w = 512,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_203 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith172_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 51200);

}


/* scheduling epoch=219  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_205 */

static void LL_ATON_Start_EpochBlock_219(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_205 */
  /* node=Concat_205 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_205 input ports=0 range=1[0,153600] */

  static const LL_Streng_TensorInitTypeDef Concat_205_dma_init_in_0_219 = {
    /* from memory with batch=128 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_204_out_0 */
    .offset_start = 0,
    .offset_end = 51200,
    .offset_limit = 153664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 51200,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Concat_205_dma_init_in_0_219, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 153600 */

  /* Dma output units from cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_205 output ports=0 range=1[153600,307200] */

  static const LL_Streng_TensorInitTypeDef Concat_205_dma_init_out_0_219 = {
    /* to memory canonical from batch=128 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_205_out_0 */
    .offset_start = 153600,
    .offset_limit = 307264,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 256,
    .batch_offset = 1536,
    .frame_offset = 512,
    .line_offset = 0,
    .loop_offset = 153600,
    .frame_loop_cnt = 3,
    .frame_tot_cnt = 3,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Concat_205_dma_init_out_0_219, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 153600 */

  static const LL_Switch_InitTypeDef switch_init_in_219[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_205 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 5 out port=0 */
  };


  /* epoch=219 */
  LL_Switch_Init(switch_init_in_219, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */, 153600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_219_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_219_all_units, 2);

}

static void LL_ATON_End_EpochBlock_219(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_219[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_205 OUT: in unit=STREAM_ENG_V2 1 in port=0 out unit=STREAM_ENG_V2 5 out port=0 */
  };


  /* epoch=219 */
  LL_Switch_Deinit(switch_deinit_in_219, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_219_all_units[] = {
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_219_all_units, 2);

}


/* scheduling epoch=220  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_220(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_206 */
  Conv_sw_info conv173_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 384,
    .general.input.dim.num_elem = 38400,
    .general.input.stride.b = 153600,
    .general.input.stride.h = 15360,
    .general.input.stride.w = 1536,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 153600))) /* Equivalent hex address = 0x34305800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 256,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 384,
    .weights.dim.num_elem = 98304,
    .weights.stride.b = 1536,
    .weights.stride.h = 1536,
    .weights.stride.w = 1536,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 6619136))) /* Equivalent hex address = 0x71650000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 256,
    .bias.dim.num_elem = 256,
    .bias.stride.b = 1024,
    .bias.stride.h = 1024,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12072256))) /* Equivalent hex address = 0x71b83540UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_206 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv173_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 102400);

}


/* scheduling epoch=221  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_221(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_207 */
  Activ_sw_info activ174_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_207 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ174_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */, 102400);

}


/* scheduling epoch=222  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_222(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_208 */
  Arith_sw_info arith175_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 256,
    .operand.dim.num_elem = 25600,
    .operand.stride.b = 102400,
    .operand.stride.h = 10240,
    .operand.stride.w = 1024,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) /* Equivalent hex address = 0x342f9000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 256,
    .general.output.dim.num_elem = 25600,
    .general.output.stride.b = 102400,
    .general.output.stride.h = 10240,
    .general.output.stride.w = 1024,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_208 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith175_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 307200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */, 102400);

}


/* scheduling epoch=223  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_223(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_209 */
  Conv_sw_info conv176_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 256,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 9216,
    .weights.stride.h = 3072,
    .weights.stride.w = 1024,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 4128768))) /* Equivalent hex address = 0x713f0000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12080960))) /* Equivalent hex address = 0x71b85740UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_209 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv176_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 25600);

}


/* scheduling epoch=224  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_224(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_210 */
  Activ_sw_info activ177_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_210 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ177_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */, 25600);

}


/* scheduling epoch=225  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_225(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_211 */
  Arith_sw_info arith178_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 6400,
    .operand.stride.b = 25600,
    .operand.stride.h = 2560,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_211 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith178_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 25600);

}


/* scheduling epoch=226  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_226(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_212 */
  Conv_sw_info conv179_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10567680))) /* Equivalent hex address = 0x71a14000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12087360))) /* Equivalent hex address = 0x71b87040UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_212 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv179_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 25600);

}


/* scheduling epoch=227  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_227(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_213 */
  Activ_sw_info activ180_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_213 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ180_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */, 25600);

}


/* scheduling epoch=228  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_228(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_214 */
  Arith_sw_info arith181_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 6400,
    .operand.stride.b = 25600,
    .operand.stride.h = 2560,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_214 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith181_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 25600);

}


/* scheduling epoch=229  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_229(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411616))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411584))) /* Equivalent hex address = 0x343447c0UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_215 */
  Conv_sw_info conv182_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 1,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 64,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12088896))) /* Equivalent hex address = 0x71b87640UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 1,
    .bias.dim.num_elem = 1,
    .bias.stride.b = 4,
    .bias.stride.h = 4,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090848))) /* Equivalent hex address = 0x71b87de0UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 100,
    .general.output.stride.b = 400,
    .general.output.stride.h = 40,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411200))) /* Equivalent hex address = 0x34344640UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_215 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv182_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411616))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 411200))) /* Equivalent hex address = 0x34344640UL */, 416);

}


/* scheduling epoch=230  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_230(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_216 */
  Conv_sw_info conv183_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 256,
    .general.input.dim.num_elem = 25600,
    .general.input.stride.b = 102400,
    .general.input.stride.h = 10240,
    .general.input.stride.w = 1024,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 204800))) /* Equivalent hex address = 0x34312000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 256,
    .weights.dim.num_elem = 147456,
    .weights.stride.b = 9216,
    .weights.stride.h = 3072,
    .weights.stride.w = 1024,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 4718592))) /* Equivalent hex address = 0x71480000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12081216))) /* Equivalent hex address = 0x71b85840UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_216 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv183_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 25600);

}


/* scheduling epoch=231  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_231(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_217 */
  Activ_sw_info activ184_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_217 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ184_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */, 25600);

}


/* scheduling epoch=232  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_232(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_218 */
  Arith_sw_info arith185_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 6400,
    .operand.stride.b = 25600,
    .operand.stride.h = 2560,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_218 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith185_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 25600);

}


/* scheduling epoch=233  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_233(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_219 */
  Conv_sw_info conv186_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 3,
    .weights.dim.tensor_w = 3,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 36864,
    .weights.stride.b = 2304,
    .weights.stride.h = 768,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 10715136))) /* Equivalent hex address = 0x71a38000UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12087616))) /* Equivalent hex address = 0x71b87140UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {1, 1, 1, 1},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_219 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv186_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 25600);

}


/* scheduling epoch=234  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_234(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_220 */
  Activ_sw_info activ187_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_220 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ187_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */, 25600);

}


/* scheduling epoch=235  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_235(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_221 */
  Arith_sw_info arith188_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 10,
    .operand.dim.tensor_w = 10,
    .operand.dim.tensor_c = 64,
    .operand.dim.num_elem = 6400,
    .operand.stride.b = 25600,
    .operand.stride.h = 2560,
    .operand.stride.w = 256,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25600))) /* Equivalent hex address = 0x342e6400UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_221 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith188_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */, 25600);

}


/* scheduling epoch=236  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_236(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_222 */
  Conv_sw_info conv189_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 10,
    .general.input.dim.tensor_w = 10,
    .general.input.dim.tensor_c = 64,
    .general.input.dim.num_elem = 6400,
    .general.input.stride.b = 25600,
    .general.input.stride.h = 2560,
    .general.input.stride.w = 256,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 51200))) /* Equivalent hex address = 0x342ec800UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 64,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 64,
    .weights.dim.num_elem = 4096,
    .weights.stride.b = 256,
    .weights.stride.h = 256,
    .weights.stride.w = 256,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12021376))) /* Equivalent hex address = 0x71b76e80UL */,
    .weights.format.is_signed = 0,
    /* "bias" tensor-related info: */
    .bias.dim.tensor_b = 1,
    .bias.dim.tensor_h = 1,
    .bias.dim.tensor_w = 1,
    .bias.dim.tensor_c = 64,
    .bias.dim.num_elem = 64,
    .bias.stride.b = 256,
    .bias.stride.h = 256,
    .bias.stride.w = 4,
    .bias.stride.c = 4,
    .bias.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12088128))) /* Equivalent hex address = 0x71b87340UL */,
    .bias.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 10,
    .general.output.dim.tensor_w = 10,
    .general.output.dim.tensor_c = 64,
    .general.output.dim.num_elem = 6400,
    .general.output.stride.b = 25600,
    .general.output.stride.h = 2560,
    .general.output.stride.w = 256,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) /* Equivalent hex address = 0x342f2c00UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_222 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv189_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 102400))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 76800))) /* Equivalent hex address = 0x342f2c00UL */, 25600);

}


/* scheduling epoch=237  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_237(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 26016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 26016);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_223 */
  static const uint32_t Concat_223_tensor_info_in_237__shape_1_64_10_10[] = { 1, 10, 10, 64 };
  static const uint32_t Concat_223_tensor_info_in_237__mem_shape_L_1_64_10_10[] = { 1, 10, 10, 64 };
  static const uint32_t Concat_223_tensor_info_in_237__shape_1_1_10_10[] = { 1, 10, 10, 1 };
  static const uint32_t Concat_223_tensor_info_in_237__mem_shape_F_1_1_10_10[] = { 1, 1, 10, 10 };
  static const LL_Buffer_InfoTypeDef Concat_223_tensor_info_in_237[] = {
    {
      .name = "Conv2D_222_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 76800,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 237,
      .batch = 64,
      .mem_shape = Concat_223_tensor_info_in_237__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_223_tensor_info_in_237__shape_1_64_10_10,
    },
    {
      .name = "Conv2D_215_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 411200,
      .offset_end = 411600,
      .offset_limit = 411664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 237,
      .batch = 1,
      .mem_shape = Concat_223_tensor_info_in_237__mem_shape_F_1_1_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_223_tensor_info_in_237__shape_1_1_10_10,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_223_tensor_info_out_237__shape_1_65_10_10[] = { 1, 10, 10, 65 };
  static const uint32_t Concat_223_tensor_info_out_237__mem_shape_L_1_65_10_10[] = { 1, 10, 10, 65 };
  static const LL_Buffer_InfoTypeDef Concat_223_tensor_info_out_237[] = {
    {
      .name = "Concat_223_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 26000,
      .offset_limit = 26064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 237,
      .batch = 65,
      .mem_shape = Concat_223_tensor_info_out_237__mem_shape_L_1_65_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_223_tensor_info_out_237__shape_1_65_10_10,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_223_tensor_info_in_237, 2, Concat_223_tensor_info_out_237, 1, 4, 5);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 26016))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 26016);

}


/* scheduling epoch=238  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Reshape node=Reshape_224 */

static void LL_ATON_Start_EpochBlock_238(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Reshape node=Reshape_224 */
  /* node=Reshape_224 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 2 [STREAM_ENG_V2 2] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_224 input ports=0 range=1[0,26000] */

  static const LL_Streng_TensorInitTypeDef Reshape_224_dma_init_in_0_238 = {
    /* memory canonical to batch=1 */
    .dir = 0,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_223_out_0 */
    .offset_start = 0,
    .offset_limit = 26064,
    .frame_count = 0,
    .fwidth = 10,
    .fheight = 10,
    .batch_depth = 2,
    .batch_offset = 260,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 26000,
    .frame_loop_cnt = 65,
    .frame_tot_cnt = 65,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(2, &Reshape_224_dma_init_in_0_238, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 26000 */

  /* Dma output units from cycle: */
  /* Unit= 8 [STREAM_ENG_V2 8] */
  /* Emit conf for STREAM_ENG_V2 node=Reshape_224 output ports=0 range=1[26000,52000] */

  static const LL_Streng_TensorInitTypeDef Reshape_224_dma_init_out_0_238 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Reshape_224_out_0 */
    .offset_start = 26000,
    .offset_end = 52000,
    .offset_limit = 52064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 26000,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(8, &Reshape_224_dma_init_out_0_238, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 26000 */

  static const LL_Switch_InitTypeDef switch_init_in_238[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_224 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=238 */
  LL_Switch_Init(switch_init_in_238, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25984))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 52000))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 25984))) /* Equivalent hex address = 0x342e6580UL */, 26016);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_238_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_238_all_units, 2);

}

static void LL_ATON_End_EpochBlock_238(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_238[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 8, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 2, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Reshape_224 OUT: in unit=STREAM_ENG_V2 8 in port=0 out unit=STREAM_ENG_V2 2 out port=0 */
  };


  /* epoch=238 */
  LL_Switch_Deinit(switch_deinit_in_238, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_238_all_units[] = {
    { {STRENG, 8} }, /* STREAM_ENG_V2 */
    { {STRENG, 2} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_238_all_units, 2);

}


/* scheduling epoch=239  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_239(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 546016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 546016);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Concat node=Concat_225 */
  static const uint32_t Concat_225_tensor_info_in_239__shape_1_65_1600[] = { 1, 65, 1600, 1 };
  static const uint32_t Concat_225_tensor_info_in_239__mem_shape_F_1_65_1600[] = { 1, 65, 1600 };
  static const uint32_t Concat_225_tensor_info_in_239__shape_1_65_400[] = { 1, 65, 400, 1 };
  static const uint32_t Concat_225_tensor_info_in_239__mem_shape_F_1_65_400[] = { 1, 65, 400 };
  static const uint32_t Concat_225_tensor_info_in_239__shape_1_65_100[] = { 1, 65, 100, 1 };
  static const uint32_t Concat_225_tensor_info_in_239__mem_shape_F_1_65_100[] = { 1, 65, 100 };
  static const LL_Buffer_InfoTypeDef Concat_225_tensor_info_in_239[] = {
    {
      .name = "Reshape_154_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 416000,
      .offset_limit = 416064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 239,
      .batch = 1,
      .mem_shape = Concat_225_tensor_info_in_239__mem_shape_F_1_65_1600,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_225_tensor_info_in_239__shape_1_65_1600,
    },
    {
      .name = "Reshape_189_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 411200,
      .offset_limit = 411264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 239,
      .batch = 1,
      .mem_shape = Concat_225_tensor_info_in_239__mem_shape_F_1_65_400,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_225_tensor_info_in_239__shape_1_65_400,
    },
    {
      .name = "Reshape_224_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 26000,
      .offset_end = 52000,
      .offset_limit = 52064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 239,
      .batch = 1,
      .mem_shape = Concat_225_tensor_info_in_239__mem_shape_F_1_65_100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_225_tensor_info_in_239__shape_1_65_100,
    },
    {
      .name = NULL,
    }
  };

  static const uint32_t Concat_225_tensor_info_out_239__shape_1_65_2100[] = { 1, 65, 2100, 1 };
  static const uint32_t Concat_225_tensor_info_out_239__mem_shape_F_1_65_2100[] = { 1, 65, 2100 };
  static const LL_Buffer_InfoTypeDef Concat_225_tensor_info_out_239[] = {
    {
      .name = "Concat_225_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 546000,
      .offset_limit = 546064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 239,
      .batch = 1,
      .mem_shape = Concat_225_tensor_info_out_239__mem_shape_F_1_65_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = Concat_225_tensor_info_out_239__shape_1_65_2100,
    },
    {
      .name = NULL,
    }
  };

  LL_ATON_LIB_Concat(Concat_225_tensor_info_in_239, 3, Concat_225_tensor_info_out_239, 3, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 546016))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 546016);

}


/* scheduling epoch=240  nodes=2   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_240(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) /* Equivalent hex address = 0x342f2740UL */, 8416);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2587680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) /* Equivalent hex address = 0x342f4820UL */, 537600);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Split node=Slice_228 */
  static const uint32_t Slice_228_tensor_shape_in_240_shape_0[] = { 1, 65, 2100 };
  static const LL_LIB_TensorShape_TypeDef Slice_228_tensor_shape_in_240[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 546000,
      .offset_limit = 546064,
      .ndims = 3,
      .nbits = 32,
      .shape = Slice_228_tensor_shape_in_240_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Slice_228_tensor_shape_out_240_shape_0[] = { 1, 64, 2100 };
  static const uint32_t Slice_228_tensor_shape_out_240_shape_1[] = { 1, 1, 2100 };
  static const LL_LIB_TensorShape_TypeDef Slice_228_tensor_shape_out_240[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2050080,
      .offset_end = 2587680,
      .offset_limit = 2587744,
      .ndims = 3,
      .nbits = 32,
      .shape = Slice_228_tensor_shape_out_240_shape_0,
      .batch = 1,
    },
    {
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 75600,
      .offset_end = 84000,
      .offset_limit = 84064,
      .ndims = 3,
      .nbits = 32,
      .shape = Slice_228_tensor_shape_out_240_shape_1,
      .batch = 1,
    }
  };

  LL_ATON_LIB_Split(&Slice_228_tensor_shape_in_240[0], true, &Slice_228_tensor_shape_out_240, 2, 3, 1, 0, 7, 0, 1);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) /* Equivalent hex address = 0x342f2740UL */, 8416);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2587680))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) /* Equivalent hex address = 0x342f4820UL */, 537600);

}


/* scheduling epoch=241  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_241(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 537600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 537600);

  /* Reset the stream switch */
  LL_Switch_Init(NULL, 0);

/* Unit= 27 [PROCESSOR 0] */
/* kind=Transpose node=Transpose_230 */
  static const uint32_t Transpose_230_tensor_shape_in_241_shape_0[] = { 1, 4, 16, 2100 };
  static const LL_LIB_TensorShape_TypeDef Transpose_230_tensor_shape_in_241[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2050080,
      .offset_end = 2587680,
      .offset_limit = 2587744,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_230_tensor_shape_in_241_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_230_tensor_axes_offsets_in_241_0[] = { 537600, 134400, 8400, 4 };
  static const uint32_t* Transpose_230_tensor_axes_offsets_in_241[] = {
    Transpose_230_tensor_axes_offsets_in_241_0
  };

  static const uint32_t Transpose_230_tensor_shape_out_241_shape_0[] = { 1, 16, 4, 2100 };
  static const LL_LIB_TensorShape_TypeDef Transpose_230_tensor_shape_out_241[] = {
    {
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 537600,
      .offset_limit = 537664,
      .ndims = 4,
      .nbits = 32,
      .shape = Transpose_230_tensor_shape_out_241_shape_0,
      .batch = 1,
    }
  };

  static const uint32_t Transpose_230_tensor_axes_offsets_out_241_0[] = { 537600, 33600, 8400, 4 };
  static const uint32_t* Transpose_230_tensor_axes_offsets_out_241[] = {
    Transpose_230_tensor_axes_offsets_out_241_0
  };

  static const uint8_t Transpose_230_perm_to_use_array_in_241[] = { 0, 2, 1, 3 };
  static const uint8_t Transpose_230_target_pos_array_in_241[] = { 0, 2, 1, 3 };
  LL_ATON_LIB_DMA_Transpose(&Transpose_230_tensor_shape_in_241[0], Transpose_230_tensor_axes_offsets_in_241[0], &Transpose_230_tensor_shape_out_241[0], Transpose_230_tensor_axes_offsets_out_241[0], Transpose_230_target_pos_array_in_241, Transpose_230_perm_to_use_array_in_241, 2, 3);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 537600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 537600);

}


/* scheduling epoch=242  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_242(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75616))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) /* Equivalent hex address = 0x342f2740UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sigmoid node=Sigmoid_227 */
  Activ_sw_info activ190_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 2100,
    .general.input.stride.b = 8400,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75600))) /* Equivalent hex address = 0x342f2750UL */,
    .general.input.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 2100,
    .general.output.stride.b = 8400,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75600))) /* Equivalent hex address = 0x342f2750UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_SIGMOID,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sigmoid_227 mapped on EmbedNets (FLOAT) as Sigmoid | Category: Computational */
  ll_sw_forward_activ(&activ190_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) /* Equivalent hex address = 0x342f2740UL */, 8416);

}


/* scheduling epoch=243  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Identity node=Identity_inserted_id536 */

static void LL_ATON_Start_EpochBlock_243(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Identity node=Identity_inserted_id536 */
  /* node=Identity_inserted_id536 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 7 [STREAM_ENG_V2 7] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id536 input ports=0 range=11[0,537600] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id536_dma_init_in_0_243 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Transpose_230_out_0_inserted_in536 */
    .offset_start = 0,
    .offset_end = 33600,
    .offset_limit = 537664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 33600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(7, &Identity_inserted_id536_dma_init_in_0_243, 1);


  /* Dma input bandwidth from memory pools: */
  /* cpuRAM2 -> 537600 */

  /* Dma output units from cycle: */
  /* Unit= 4 [STREAM_ENG_V2 4] */
  /* Emit conf for STREAM_ENG_V2 node=Identity_inserted_id536 output ports=0 range=11[2050080,2587680] */

  static const LL_Streng_TensorInitTypeDef Identity_inserted_id536_dma_init_out_0_243 = {
    /* to memory canonical from batch=1 */
    .dir = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */}, /* Transpose_230_out_0_inserted_out536 */
    .offset_start = 2050080,
    .offset_limit = 2587744,
    .frame_count = 0,
    .fwidth = 2100,
    .fheight = 4,
    .batch_depth = 2,
    .batch_offset = 64,
    .frame_offset = 4,
    .line_offset = 0,
    .loop_offset = 537600,
    .frame_loop_cnt = 16,
    .frame_tot_cnt = 16,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(4, &Identity_inserted_id536_dma_init_out_0_243, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM6 <- 162848 */
  /* npuRAM5 <- 374752 */

  static const LL_Switch_InitTypeDef switch_init_in_243[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id536 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=243 */
  LL_Switch_Init(switch_init_in_243, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2587680))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) /* Equivalent hex address = 0x342f4820UL */, 537600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_243_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_243_all_units, 2);

}

static void LL_ATON_End_EpochBlock_243(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_243[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 4, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 7, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Identity_inserted_id536 OUT: in unit=STREAM_ENG_V2 4 in port=0 out unit=STREAM_ENG_V2 7 out port=0 */
  };


  /* epoch=243 */
  LL_Switch_Deinit(switch_deinit_in_243, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_243_all_units[] = {
    { {STRENG, 4} }, /* STREAM_ENG_V2 */
    { {STRENG, 7} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_243_all_units, 2);

}


/* scheduling epoch=244  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_244(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 34624))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 34656))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 34624))) /* Equivalent hex address = 0x342e8740UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Softmax node=Softmax_231 */
  Softmax_sw_info softmax191_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 4,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 134400,
    .general.input.stride.b = 537600,
    .general.input.stride.h = 134400,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 2050080))) /* Equivalent hex address = 0x342f4820UL */,
    .general.input.format.is_signed = 0,
    /* "scratch" tensor-related info: */
    .scratch.dim.tensor_b = 1,
    .scratch.dim.tensor_h = 1,
    .scratch.dim.tensor_w = 257,
    .scratch.dim.tensor_c = 1,
    .scratch.dim.num_elem = 257,
    .scratch.stride.b = 1028,
    .scratch.stride.h = 1028,
    .scratch.stride.w = 4,
    .scratch.stride.c = 4,
    .scratch.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 33600))) /* Equivalent hex address = 0x342e8340UL */,
    .scratch.format.is_signed = 1,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 4,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 16,
    .general.output.dim.num_elem = 134400,
    .general.output.stride.b = 537600,
    .general.output.stride.h = 134400,
    .general.output.stride.w = 64,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.output.format.is_signed = 0,
    /* Node-specific Hyper-parameters: */
    .axis = 1,
    .general.type = LL_SW_SOFTMAX,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Softmax_231 mapped on EmbedNets (FLOAT) as Softmax | Category: Computational */
  ll_sw_forward_softmax(&softmax191_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 33600))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 34656))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 33600))) /* Equivalent hex address = 0x342e8340UL */, 1056);

  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 11 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 537600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */, 537600);

}


/* scheduling epoch=245  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_245(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Conv node=Conv2D_232 */
  Conv_sw_info conv192_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 4,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 16,
    .general.input.dim.num_elem = 134400,
    .general.input.stride.b = 537600,
    .general.input.stride.h = 134400,
    .general.input.stride.w = 64,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34100000UL + 0))) /* Equivalent hex address = 0x34100000UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = 1,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = 16,
    .weights.dim.num_elem = 16,
    .weights.stride.b = 64,
    .weights.stride.h = 64,
    .weights.stride.w = 64,
    .weights.stride.c = 4,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090688))) /* Equivalent hex address = 0x71b87d40UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 4,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 8400,
    .general.output.stride.b = 33600,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Conv2D_232 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv192_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 33600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 33600);

}


/* scheduling epoch=246  nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=247  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_247(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Mul node=Mul_237 */
  Arith_sw_info arith193_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 4,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 8400,
    .general.input.stride.b = 33600,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 4,
    .operand.dim.tensor_w = 2100,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 8400,
    .operand.stride.b = 33600,
    .operand.stride.h = 8400,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11829248))) /* Equivalent hex address = 0x71b48000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 4,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 8400,
    .general.output.stride.b = 33600,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHMUL,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Mul_237 mapped on EmbedNets (FLOAT) as Mul | Category: Computational */
  ll_sw_forward_arith(&arith193_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 117600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */, 33600);

}


/* scheduling epoch=248  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Split node=Slice_240 */

static void LL_ATON_Start_EpochBlock_248(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Split node=Slice_240 */
  /* node=Slice_240 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 0 [STREAM_ENG_V2 0] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_240 input ports=0 range=1[84000,117600] */

  static const LL_Streng_TensorInitTypeDef Slice_240_dma_init_in_0_248 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Mul_237_out_0 */
    .offset_start = 84000,
    .offset_end = 117600,
    .offset_limit = 117664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 33600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(0, &Slice_240_dma_init_in_0_248, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 33600 */

  /* Dma output units from cycle: */
  /* Unit= 9 [STREAM_ENG_V2 9] */
  /* Emit conf for STREAM_ENG_V2 node=Slice_240 output ports=0 range=1[0,33600] */

  static const LL_Streng_TensorInitTypeDef Slice_240_dma_init_out_0_248 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Slice_240_out_0 */
    .offset_start = 0,
    .offset_end = 33600,
    .offset_limit = 33664,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 33600,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(9, &Slice_240_dma_init_out_0_248, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 33600 */

  static const LL_Switch_InitTypeDef switch_init_in_248[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_240 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=248 */
  LL_Switch_Init(switch_init_in_248, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 33600))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 33600);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_248_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_248_all_units, 2);

}

static void LL_ATON_End_EpochBlock_248(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_248[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 9, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 0, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Slice_240 OUT: in unit=STREAM_ENG_V2 9 in port=0 out unit=STREAM_ENG_V2 0 out port=0 */
  };


  /* epoch=248 */
  LL_Switch_Deinit(switch_deinit_in_248, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_248_all_units[] = {
    { {STRENG, 9} }, /* STREAM_ENG_V2 */
    { {STRENG, 0} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_248_all_units, 2);

}


/* scheduling epoch=249  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_249(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_239 */
  Arith_sw_info arith194_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 2,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 4200,
    .general.input.stride.b = 16800,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16800))) /* Equivalent hex address = 0x342e41a0UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 2,
    .operand.dim.tensor_w = 2100,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 4200,
    .operand.stride.b = 16800,
    .operand.stride.h = 8400,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11938624))) /* Equivalent hex address = 0x71b62b40UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 2,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 4200,
    .general.output.stride.b = 16800,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_239 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith194_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */, 16800);

}


/* scheduling epoch=250  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_250(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sub node=Sub_241 */
  Arith_sw_info arith195_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 2,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 4200,
    .general.input.stride.b = 16800,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 11955424))) /* Equivalent hex address = 0x71b66ce0UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 2,
    .operand.dim.tensor_w = 2100,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 4200,
    .operand.stride.b = 16800,
    .operand.stride.h = 8400,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 2,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 4200,
    .general.output.stride.b = 16800,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) /* Equivalent hex address = 0x342f89c0UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHSUB,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sub_241 mapped on EmbedNets (FLOAT) as Sub | Category: Computational */
  ll_sw_forward_arith(&arith195_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 117600))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) /* Equivalent hex address = 0x342f89c0UL */, 16800);

}


/* scheduling epoch=251  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_251(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Add node=Add_243 */
  Arith_sw_info arith196_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 2,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 4200,
    .general.input.stride.b = 16800,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) /* Equivalent hex address = 0x342f89c0UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 2,
    .operand.dim.tensor_w = 2100,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 4200,
    .operand.stride.b = 16800,
    .operand.stride.h = 8400,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 2,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 4200,
    .general.output.stride.b = 16800,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.output.format.is_signed = 0,
    .general.type = LL_SW_ARITHADD,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Add_243 mapped on EmbedNets (FLOAT) as Add | Category: Computational */
  ll_sw_forward_arith(&arith196_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 16800))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 16800);

}


/* scheduling epoch=252  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_252(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 41984))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 42016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 41984))) /* Equivalent hex address = 0x342ea400UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58816))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) /* Equivalent hex address = 0x342ee5a0UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Div node=Div_244 */
  Arith_sw_info arith197_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 2,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 4200,
    .general.input.stride.b = 16800,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 1,
    .operand.dim.tensor_w = 1,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 1,
    .operand.stride.b = 4,
    .operand.stride.h = 4,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x71000000UL + 12090800))) /* Equivalent hex address = 0x71b87db0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 2,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 4200,
    .general.output.stride.b = 16800,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 42000))) /* Equivalent hex address = 0x342ea410UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHDIV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Div_244 mapped on EmbedNets (FLOAT) as Div | Category: Computational */
  ll_sw_forward_arith(&arith197_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 41984))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58816))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 41984))) /* Equivalent hex address = 0x342ea400UL */, 16832);

}


/* scheduling epoch=253  nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_253(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* *** MCU cache invalidate (only) operation for unaligned buffer start address (first line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58816))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) /* Equivalent hex address = 0x342ee5a0UL */, 32);

  /* *** MCU cache invalidate (only) operation for unaligned buffer end address (last line) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75616))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75584))) /* Equivalent hex address = 0x342f2740UL */, 32);


/* Unit= 27 [PROCESSOR 0] */
/* kind=Sub node=Sub_242 */
  Arith_sw_info arith198_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 2,
    .general.input.dim.tensor_w = 2100,
    .general.input.dim.tensor_c = 1,
    .general.input.dim.num_elem = 4200,
    .general.input.stride.b = 16800,
    .general.input.stride.h = 8400,
    .general.input.stride.w = 4,
    .general.input.stride.c = 4,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 84000))) /* Equivalent hex address = 0x342f4820UL */,
    .general.input.format.is_signed = 0,
    /* "operand" tensor-related info: */
    .operand.dim.tensor_b = 1,
    .operand.dim.tensor_h = 2,
    .operand.dim.tensor_w = 2100,
    .operand.dim.tensor_c = 1,
    .operand.dim.num_elem = 4200,
    .operand.stride.b = 16800,
    .operand.stride.h = 8400,
    .operand.stride.w = 4,
    .operand.stride.c = 4,
    .operand.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 100800))) /* Equivalent hex address = 0x342f89c0UL */,
    .operand.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 2,
    .general.output.dim.tensor_w = 2100,
    .general.output.dim.tensor_c = 1,
    .general.output.dim.num_elem = 4200,
    .general.output.stride.b = 16800,
    .general.output.stride.h = 8400,
    .general.output.stride.w = 4,
    .general.output.stride.c = 4,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58800))) /* Equivalent hex address = 0x342ee5b0UL */,
    .general.output.format.is_signed = 1,
    .general.type = LL_SW_ARITHSUB,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Sub_242 mapped on EmbedNets (FLOAT) as Sub | Category: Computational */
  ll_sw_forward_arith(&arith198_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 75616))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 58784))) /* Equivalent hex address = 0x342ee5a0UL */, 16832);

}


/* scheduling epoch=254  nodes=1   ------------------------------------------------------------------- */
/* no resources allocated to kind=Concat node=Concat_245 */

static void LL_ATON_Start_EpochBlock_254(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  /* Unit= 28 [NULL_UNIT 0] */
  /* kind=Concat node=Concat_245 */
  /* node=Concat_245 satisfies input and output adjacency (DMA->DMA) and can be omitted */

  /* Dma inputs units to cycle: */
  /* Unit= 1 [STREAM_ENG_V2 1] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_245 input ports=0 range=1[42000,84000] */

  static const LL_Streng_TensorInitTypeDef Concat_245_dma_init_in_0_254 = {
    /* from memory with batch=1 */
    .dir = 0,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Div_244_out_0 */
    .offset_start = 42000,
    .offset_end = 84000,
    .offset_limit = 84064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 42000,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(1, &Concat_245_dma_init_in_0_254, 1);


  /* Dma input bandwidth from memory pools: */
  /* npuRAM5 -> 42000 */

  /* Dma output units from cycle: */
  /* Unit= 5 [STREAM_ENG_V2 5] */
  /* Emit conf for STREAM_ENG_V2 node=Concat_245 output ports=0 range=1[0,42000] */

  static const LL_Streng_TensorInitTypeDef Concat_245_dma_init_out_0_254 = {
    /* to memory with batch=1 */
    .dir = 1,
    .raw = 1,
    .noblk = 0,
    .align_right = 0,
    .nbits_unsigned = 0,
    .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */}, /* Concat_245_out_0 */
    .offset_start = 0,
    .offset_end = 42000,
    .offset_limit = 42064,
    .frame_count = 0,
    .fwidth = 0,
    .fheight = 0,
    .batch_depth = 0,
    .batch_offset = 0,
    .frame_offset = 42000,
    .line_offset = 0,
    .loop_offset = 0,
    .frame_loop_cnt = 0,
    .frame_tot_cnt = 1,
    .nbits_in = 16,
    .nbits_out = 16,
  };

  /* Unit=STREAM_ENG_V2 */
  LL_Streng_TensorInit(5, &Concat_245_dma_init_out_0_254, 1);


  /* Dma output bandwidth to memory pools: */
  /* npuRAM5 <- 42000 */

  static const LL_Switch_InitTypeDef switch_init_in_254[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_245 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=254 */
  LL_Switch_Init(switch_init_in_254, 1);

  /* *** MCU cache invalidate (only) operation (HW, whole range) *** */
  /*     memory pool: 1 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 42016))) */
  LL_ATON_Cache_MCU_Invalidate_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x342e0000UL + 0))) /* Equivalent hex address = 0x342e0000UL */, 42016);

  static const LL_ATON_EnableUnits_InitTypeDef Enable_epoch_254_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_EnableUnits_Init(Enable_epoch_254_all_units, 2);

}

static void LL_ATON_End_EpochBlock_254(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);

  static const LL_Switch_DeinitTypeDef switch_deinit_in_254[] = {
    { LL_Switch_Init_Dest() = ATONN_DSTPORT(STRSWITCH, 0, STRENG, 5, 0), LL_Switch_Init_Source(0) = ATONN_SRCPORT(STRSWITCH, 0, STRENG, 1, 0), LL_Switch_Init_Context(0) = 1, LL_Switch_Init_Frames(0) = 0, }, /* Concat_245 OUT: in unit=STREAM_ENG_V2 5 in port=0 out unit=STREAM_ENG_V2 1 out port=0 */
  };


  /* epoch=254 */
  LL_Switch_Deinit(switch_deinit_in_254, 1);

  static const LL_ATON_DisableUnits_InitTypeDef Disable_epoch_254_all_units[] = {
    { {STRENG, 5} }, /* STREAM_ENG_V2 */
    { {STRENG, 1} }, /* STREAM_ENG_V2 */
  };


  LL_ATON_DisableUnits_Init(Disable_epoch_254_all_units, 2);

}


/* scheduling epoch=255  nodes=1   ------------------------------------------------------------------- */

/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_yolo_deer(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_1,
      .end_epoch_block = LL_ATON_End_EpochBlock_1,
      .wait_mask = 0x00000020,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 1,
      .last_epoch_num = 1,
      .in_streng_mask = 0x00000008,
      .out_streng_mask = 0x00000020,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_2,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_3,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 3,
      .last_epoch_num = 3,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_4,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 4,
      .last_epoch_num = 4,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_5,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 5,
      .last_epoch_num = 5,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_6,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 6,
      .last_epoch_num = 6,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_7,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 7,
      .last_epoch_num = 7,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_8,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 8,
      .last_epoch_num = 8,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_9,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 9,
      .last_epoch_num = 9,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_10,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 10,
      .last_epoch_num = 10,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_11,
      .end_epoch_block = LL_ATON_End_EpochBlock_11,
      .wait_mask = 0x00000008,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 11,
      .last_epoch_num = 11,
      .in_streng_mask = 0x00000080,
      .out_streng_mask = 0x00000008,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_12,
      .end_epoch_block = LL_ATON_End_EpochBlock_12,
      .wait_mask = 0x00000008,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 12,
      .last_epoch_num = 12,
      .in_streng_mask = 0x00000080,
      .out_streng_mask = 0x00000008,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_13,
      .end_epoch_block = LL_ATON_End_EpochBlock_13,
      .wait_mask = 0x00000028,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 13,
      .last_epoch_num = 13,
      .in_streng_mask = 0x00000201,
      .out_streng_mask = 0x00000028,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_14,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 14,
      .last_epoch_num = 14,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_15,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 15,
      .last_epoch_num = 15,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_16,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 16,
      .last_epoch_num = 16,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_17,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 17,
      .last_epoch_num = 17,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_18,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 18,
      .last_epoch_num = 18,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_19,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 19,
      .last_epoch_num = 19,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_20,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 20,
      .last_epoch_num = 20,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_21,
      .end_epoch_block = LL_ATON_End_EpochBlock_21,
      .wait_mask = 0x00000080,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 21,
      .last_epoch_num = 21,
      .in_streng_mask = 0x00000001,
      .out_streng_mask = 0x00000080,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_22,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 22,
      .last_epoch_num = 22,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_23,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 23,
      .last_epoch_num = 23,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_24,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 24,
      .last_epoch_num = 24,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_25,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 25,
      .last_epoch_num = 25,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_26,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 26,
      .last_epoch_num = 26,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_27,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 27,
      .last_epoch_num = 27,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_28,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 28,
      .last_epoch_num = 28,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_29,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 29,
      .last_epoch_num = 29,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_30,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 30,
      .last_epoch_num = 30,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_31,
      .end_epoch_block = LL_ATON_End_EpochBlock_31,
      .wait_mask = 0x00000001,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 31,
      .last_epoch_num = 31,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000001,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_32,
      .end_epoch_block = LL_ATON_End_EpochBlock_32,
      .wait_mask = 0x00000080,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 32,
      .last_epoch_num = 32,
      .in_streng_mask = 0x00000010,
      .out_streng_mask = 0x00000080,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_33,
      .end_epoch_block = LL_ATON_End_EpochBlock_33,
      .wait_mask = 0x00000042,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 33,
      .last_epoch_num = 33,
      .in_streng_mask = 0x00000104,
      .out_streng_mask = 0x00000042,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_34,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 34,
      .last_epoch_num = 34,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_35,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 35,
      .last_epoch_num = 35,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_36,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 36,
      .last_epoch_num = 36,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_37,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 37,
      .last_epoch_num = 37,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_38,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 38,
      .last_epoch_num = 38,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_39,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 39,
      .last_epoch_num = 39,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_40,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 40,
      .last_epoch_num = 40,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_41,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 41,
      .last_epoch_num = 41,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_42,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 42,
      .last_epoch_num = 42,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_43,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 43,
      .last_epoch_num = 43,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_44,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 44,
      .last_epoch_num = 44,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_45,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 45,
      .last_epoch_num = 45,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_46,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 46,
      .last_epoch_num = 46,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_47,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 47,
      .last_epoch_num = 47,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_48,
      .end_epoch_block = LL_ATON_End_EpochBlock_48,
      .wait_mask = 0x00000040,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 48,
      .last_epoch_num = 48,
      .in_streng_mask = 0x00000002,
      .out_streng_mask = 0x00000040,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_49,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 49,
      .last_epoch_num = 49,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_50,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 50,
      .last_epoch_num = 50,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_51,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 51,
      .last_epoch_num = 51,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_52,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 52,
      .last_epoch_num = 52,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_53,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 53,
      .last_epoch_num = 53,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_54,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 54,
      .last_epoch_num = 54,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_55,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 55,
      .last_epoch_num = 55,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_56,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 56,
      .last_epoch_num = 56,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_57,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 57,
      .last_epoch_num = 57,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_58,
      .end_epoch_block = LL_ATON_End_EpochBlock_58,
      .wait_mask = 0x00000004,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 58,
      .last_epoch_num = 58,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000004,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_59,
      .end_epoch_block = LL_ATON_End_EpochBlock_59,
      .wait_mask = 0x00000200,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 59,
      .last_epoch_num = 59,
      .in_streng_mask = 0x00000002,
      .out_streng_mask = 0x00000200,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_60,
      .end_epoch_block = LL_ATON_End_EpochBlock_60,
      .wait_mask = 0x00000060,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 60,
      .last_epoch_num = 60,
      .in_streng_mask = 0x00000011,
      .out_streng_mask = 0x00000060,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_61,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 61,
      .last_epoch_num = 61,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_62,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 62,
      .last_epoch_num = 62,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_63,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 63,
      .last_epoch_num = 63,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_64,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 64,
      .last_epoch_num = 64,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_65,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 65,
      .last_epoch_num = 65,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_66,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 66,
      .last_epoch_num = 66,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_67,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 67,
      .last_epoch_num = 67,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_68,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 68,
      .last_epoch_num = 68,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_69,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 69,
      .last_epoch_num = 69,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_70,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 70,
      .last_epoch_num = 70,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_71,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 71,
      .last_epoch_num = 71,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_72,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 72,
      .last_epoch_num = 72,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_73,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 73,
      .last_epoch_num = 73,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_74,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 74,
      .last_epoch_num = 74,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_75,
      .end_epoch_block = LL_ATON_End_EpochBlock_75,
      .wait_mask = 0x00000200,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 75,
      .last_epoch_num = 75,
      .in_streng_mask = 0x00000001,
      .out_streng_mask = 0x00000200,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_76,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 76,
      .last_epoch_num = 76,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_77,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 77,
      .last_epoch_num = 77,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_78,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 78,
      .last_epoch_num = 78,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_79,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 79,
      .last_epoch_num = 79,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_80,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 80,
      .last_epoch_num = 80,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_81,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 81,
      .last_epoch_num = 81,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_82,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 82,
      .last_epoch_num = 82,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_83,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 83,
      .last_epoch_num = 83,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_84,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 84,
      .last_epoch_num = 84,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_85,
      .end_epoch_block = LL_ATON_End_EpochBlock_85,
      .wait_mask = 0x00000080,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 85,
      .last_epoch_num = 85,
      .in_streng_mask = 0x00000001,
      .out_streng_mask = 0x00000080,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_86,
      .end_epoch_block = LL_ATON_End_EpochBlock_86,
      .wait_mask = 0x00000200,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 86,
      .last_epoch_num = 86,
      .in_streng_mask = 0x00000004,
      .out_streng_mask = 0x00000200,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_87,
      .end_epoch_block = LL_ATON_End_EpochBlock_87,
      .wait_mask = 0x00000014,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 87,
      .last_epoch_num = 87,
      .in_streng_mask = 0x00000300,
      .out_streng_mask = 0x00000014,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_88,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 88,
      .last_epoch_num = 88,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_89,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 89,
      .last_epoch_num = 89,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_90,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 90,
      .last_epoch_num = 90,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_91,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 91,
      .last_epoch_num = 91,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_92,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 92,
      .last_epoch_num = 92,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_93,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 93,
      .last_epoch_num = 93,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_94,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 94,
      .last_epoch_num = 94,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_95,
      .end_epoch_block = LL_ATON_End_EpochBlock_95,
      .wait_mask = 0x00000004,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 95,
      .last_epoch_num = 95,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000004,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_96,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 96,
      .last_epoch_num = 96,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_97,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 97,
      .last_epoch_num = 97,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_98,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 98,
      .last_epoch_num = 98,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_99,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 99,
      .last_epoch_num = 99,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_100,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 100,
      .last_epoch_num = 100,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_101,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 101,
      .last_epoch_num = 101,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_102,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 102,
      .last_epoch_num = 102,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_103,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 103,
      .last_epoch_num = 103,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_104,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 104,
      .last_epoch_num = 104,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_105,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 105,
      .last_epoch_num = 105,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_106,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 106,
      .last_epoch_num = 106,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_107,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 107,
      .last_epoch_num = 107,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_108,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 108,
      .last_epoch_num = 108,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_109,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 109,
      .last_epoch_num = 109,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_110,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 110,
      .last_epoch_num = 110,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_111,
      .end_epoch_block = LL_ATON_End_EpochBlock_111,
      .wait_mask = 0x00000200,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 111,
      .last_epoch_num = 111,
      .in_streng_mask = 0x00000010,
      .out_streng_mask = 0x00000200,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_112,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 112,
      .last_epoch_num = 112,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_113,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 113,
      .last_epoch_num = 113,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_114,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 114,
      .last_epoch_num = 114,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_115,
      .end_epoch_block = LL_ATON_End_EpochBlock_115,
      .wait_mask = 0x00000100,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 115,
      .last_epoch_num = 115,
      .in_streng_mask = 0x00000001,
      .out_streng_mask = 0x00000100,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_116,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 116,
      .last_epoch_num = 116,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_117,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 117,
      .last_epoch_num = 117,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_118,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 118,
      .last_epoch_num = 118,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_119,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 119,
      .last_epoch_num = 119,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_120,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 120,
      .last_epoch_num = 120,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_121,
      .end_epoch_block = LL_ATON_End_EpochBlock_121,
      .wait_mask = 0x00000004,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 121,
      .last_epoch_num = 121,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000004,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_122,
      .end_epoch_block = LL_ATON_End_EpochBlock_122,
      .wait_mask = 0x00000001,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 122,
      .last_epoch_num = 122,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000001,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_123,
      .end_epoch_block = LL_ATON_End_EpochBlock_123,
      .wait_mask = 0x00000300,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 123,
      .last_epoch_num = 123,
      .in_streng_mask = 0x0000000c,
      .out_streng_mask = 0x00000300,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_124,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 124,
      .last_epoch_num = 124,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_125,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 125,
      .last_epoch_num = 125,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_126,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 126,
      .last_epoch_num = 126,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_127,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 127,
      .last_epoch_num = 127,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_128,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 128,
      .last_epoch_num = 128,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_129,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 129,
      .last_epoch_num = 129,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_130,
      .end_epoch_block = LL_ATON_End_EpochBlock_130,
      .wait_mask = 0x00000001,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 130,
      .last_epoch_num = 130,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000001,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_131,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 131,
      .last_epoch_num = 131,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_132,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 132,
      .last_epoch_num = 132,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_133,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 133,
      .last_epoch_num = 133,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_134,
      .end_epoch_block = LL_ATON_End_EpochBlock_134,
      .wait_mask = 0x00000020,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 134,
      .last_epoch_num = 134,
      .in_streng_mask = 0x00000010,
      .out_streng_mask = 0x00000020,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_135,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 135,
      .last_epoch_num = 135,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_136,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 136,
      .last_epoch_num = 136,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_137,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 137,
      .last_epoch_num = 137,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_138,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 138,
      .last_epoch_num = 138,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_139,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 139,
      .last_epoch_num = 139,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_140,
      .end_epoch_block = LL_ATON_End_EpochBlock_140,
      .wait_mask = 0x00000020,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 140,
      .last_epoch_num = 140,
      .in_streng_mask = 0x00000002,
      .out_streng_mask = 0x00000020,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_141,
      .end_epoch_block = LL_ATON_End_EpochBlock_141,
      .wait_mask = 0x00000008,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 141,
      .last_epoch_num = 141,
      .in_streng_mask = 0x00000040,
      .out_streng_mask = 0x00000008,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_142,
      .end_epoch_block = LL_ATON_End_EpochBlock_142,
      .wait_mask = 0x00000101,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 142,
      .last_epoch_num = 142,
      .in_streng_mask = 0x00000048,
      .out_streng_mask = 0x00000101,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_143,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 143,
      .last_epoch_num = 143,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_144,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 144,
      .last_epoch_num = 144,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_145,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 145,
      .last_epoch_num = 145,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_146,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 146,
      .last_epoch_num = 146,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_147,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 147,
      .last_epoch_num = 147,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_148,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 148,
      .last_epoch_num = 148,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_149,
      .end_epoch_block = LL_ATON_End_EpochBlock_149,
      .wait_mask = 0x00000002,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 149,
      .last_epoch_num = 149,
      .in_streng_mask = 0x00000200,
      .out_streng_mask = 0x00000002,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_150,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 150,
      .last_epoch_num = 150,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_151,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 151,
      .last_epoch_num = 151,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_152,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 152,
      .last_epoch_num = 152,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_153,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 153,
      .last_epoch_num = 153,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_154,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 154,
      .last_epoch_num = 154,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_155,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 155,
      .last_epoch_num = 155,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_156,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 156,
      .last_epoch_num = 156,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_157,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 157,
      .last_epoch_num = 157,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_158,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 158,
      .last_epoch_num = 158,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_159,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 159,
      .last_epoch_num = 159,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_160,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 160,
      .last_epoch_num = 160,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_161,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 161,
      .last_epoch_num = 161,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_162,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 162,
      .last_epoch_num = 162,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_163,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 163,
      .last_epoch_num = 163,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_164,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 164,
      .last_epoch_num = 164,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_165,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 165,
      .last_epoch_num = 165,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_166,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 166,
      .last_epoch_num = 166,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_167,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 167,
      .last_epoch_num = 167,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_168,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 168,
      .last_epoch_num = 168,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_169,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 169,
      .last_epoch_num = 169,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_170,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 170,
      .last_epoch_num = 170,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_171,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 171,
      .last_epoch_num = 171,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_172,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 172,
      .last_epoch_num = 172,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_173,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 173,
      .last_epoch_num = 173,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_174,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 174,
      .last_epoch_num = 174,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_175,
      .end_epoch_block = LL_ATON_End_EpochBlock_175,
      .wait_mask = 0x00000028,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 175,
      .last_epoch_num = 175,
      .in_streng_mask = 0x00000050,
      .out_streng_mask = 0x00000028,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_176,
      .end_epoch_block = LL_ATON_End_EpochBlock_176,
      .wait_mask = 0x00000010,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 176,
      .last_epoch_num = 176,
      .in_streng_mask = 0x00000100,
      .out_streng_mask = 0x00000010,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_177,
      .end_epoch_block = LL_ATON_End_EpochBlock_177,
      .wait_mask = 0x00000044,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 177,
      .last_epoch_num = 177,
      .in_streng_mask = 0x00000210,
      .out_streng_mask = 0x00000044,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_178,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 178,
      .last_epoch_num = 178,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_179,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 179,
      .last_epoch_num = 179,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_180,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 180,
      .last_epoch_num = 180,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_181,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 181,
      .last_epoch_num = 181,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_182,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 182,
      .last_epoch_num = 182,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_183,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 183,
      .last_epoch_num = 183,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_184,
      .end_epoch_block = LL_ATON_End_EpochBlock_184,
      .wait_mask = 0x00000010,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 184,
      .last_epoch_num = 184,
      .in_streng_mask = 0x00000080,
      .out_streng_mask = 0x00000010,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_185,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 185,
      .last_epoch_num = 185,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_186,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 186,
      .last_epoch_num = 186,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_187,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 187,
      .last_epoch_num = 187,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_188,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 188,
      .last_epoch_num = 188,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_189,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 189,
      .last_epoch_num = 189,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_190,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 190,
      .last_epoch_num = 190,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_191,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 191,
      .last_epoch_num = 191,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_192,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 192,
      .last_epoch_num = 192,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_193,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 193,
      .last_epoch_num = 193,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_194,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 194,
      .last_epoch_num = 194,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_195,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 195,
      .last_epoch_num = 195,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_196,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 196,
      .last_epoch_num = 196,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_197,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 197,
      .last_epoch_num = 197,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_198,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 198,
      .last_epoch_num = 198,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_199,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 199,
      .last_epoch_num = 199,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_200,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 200,
      .last_epoch_num = 200,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_201,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 201,
      .last_epoch_num = 201,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_202,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 202,
      .last_epoch_num = 202,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_203,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 203,
      .last_epoch_num = 203,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_204,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 204,
      .last_epoch_num = 204,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_205,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 205,
      .last_epoch_num = 205,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_206,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 206,
      .last_epoch_num = 206,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_207,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 207,
      .last_epoch_num = 207,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_208,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 208,
      .last_epoch_num = 208,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_209,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 209,
      .last_epoch_num = 209,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_210,
      .end_epoch_block = LL_ATON_End_EpochBlock_210,
      .wait_mask = 0x00000084,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 210,
      .last_epoch_num = 210,
      .in_streng_mask = 0x00000048,
      .out_streng_mask = 0x00000084,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_211,
      .end_epoch_block = LL_ATON_End_EpochBlock_211,
      .wait_mask = 0x00000010,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 211,
      .last_epoch_num = 211,
      .in_streng_mask = 0x00000040,
      .out_streng_mask = 0x00000010,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_212,
      .end_epoch_block = LL_ATON_End_EpochBlock_212,
      .wait_mask = 0x00000108,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 212,
      .last_epoch_num = 212,
      .in_streng_mask = 0x00000081,
      .out_streng_mask = 0x00000108,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_213,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 213,
      .last_epoch_num = 213,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_214,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 214,
      .last_epoch_num = 214,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_215,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 215,
      .last_epoch_num = 215,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_216,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 216,
      .last_epoch_num = 216,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_217,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 217,
      .last_epoch_num = 217,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_218,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 218,
      .last_epoch_num = 218,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_219,
      .end_epoch_block = LL_ATON_End_EpochBlock_219,
      .wait_mask = 0x00000002,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 219,
      .last_epoch_num = 219,
      .in_streng_mask = 0x00000020,
      .out_streng_mask = 0x00000002,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_220,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 220,
      .last_epoch_num = 220,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_221,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 221,
      .last_epoch_num = 221,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_222,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 222,
      .last_epoch_num = 222,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_223,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 223,
      .last_epoch_num = 223,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_224,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 224,
      .last_epoch_num = 224,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_225,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 225,
      .last_epoch_num = 225,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_226,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 226,
      .last_epoch_num = 226,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_227,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 227,
      .last_epoch_num = 227,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_228,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 228,
      .last_epoch_num = 228,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_229,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 229,
      .last_epoch_num = 229,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_230,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 230,
      .last_epoch_num = 230,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_231,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 231,
      .last_epoch_num = 231,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_232,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 232,
      .last_epoch_num = 232,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_233,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 233,
      .last_epoch_num = 233,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_234,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 234,
      .last_epoch_num = 234,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_235,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 235,
      .last_epoch_num = 235,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_236,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 236,
      .last_epoch_num = 236,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_237,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 237,
      .last_epoch_num = 237,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_238,
      .end_epoch_block = LL_ATON_End_EpochBlock_238,
      .wait_mask = 0x00000100,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 238,
      .last_epoch_num = 238,
      .in_streng_mask = 0x00000004,
      .out_streng_mask = 0x00000100,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_239,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 239,
      .last_epoch_num = 239,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_240,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 240,
      .last_epoch_num = 240,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_241,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_hybrid,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 241,
      .last_epoch_num = 241,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_242,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 242,
      .last_epoch_num = 242,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_243,
      .end_epoch_block = LL_ATON_End_EpochBlock_243,
      .wait_mask = 0x00000010,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 243,
      .last_epoch_num = 243,
      .in_streng_mask = 0x00000080,
      .out_streng_mask = 0x00000010,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_244,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 244,
      .last_epoch_num = 244,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_245,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 245,
      .last_epoch_num = 245,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_247,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 247,
      .last_epoch_num = 247,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_248,
      .end_epoch_block = LL_ATON_End_EpochBlock_248,
      .wait_mask = 0x00000200,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 248,
      .last_epoch_num = 248,
      .in_streng_mask = 0x00000001,
      .out_streng_mask = 0x00000200,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_249,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 249,
      .last_epoch_num = 249,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_250,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 250,
      .last_epoch_num = 250,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_251,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 251,
      .last_epoch_num = 251,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_252,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 252,
      .last_epoch_num = 252,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_253,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 253,
      .last_epoch_num = 253,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .start_epoch_block = LL_ATON_Start_EpochBlock_254,
      .end_epoch_block = LL_ATON_End_EpochBlock_254,
      .wait_mask = 0x00000020,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_hw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 254,
      .last_epoch_num = 254,
      .in_streng_mask = 0x00000002,
      .out_streng_mask = 0x00000020,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_yolo_deer(void)
{
  static const uint32_t buff_info__shape_1_3_320_320[] = { 1, 320, 320, 3 };
  static const uint32_t buff_info__mem_shape_F_1_3_320_320[] = { 1, 3, 320, 320 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_16_3_3_3[] = { 16, 3, 3, 3 };
  static const uint32_t buff_info__mem_shape_L_16_3_3_3[] = { 16, 3, 3, 3 };
  static const uint32_t buff_info__shape_16[] = { 1, 1, 16, 1 };
  static const uint32_t buff_info__mem_shape_U_16[] = { 16 };
  static const uint32_t buff_info__shape_32_16_3_3[] = { 32, 3, 3, 16 };
  static const uint32_t buff_info__mem_shape_L_32_16_3_3[] = { 32, 3, 3, 16 };
  static const uint32_t buff_info__shape_32[] = { 1, 1, 32, 1 };
  static const uint32_t buff_info__mem_shape_U_32[] = { 32 };
  static const uint32_t buff_info__shape_32_32_1_1[] = { 32, 1, 1, 32 };
  static const uint32_t buff_info__mem_shape_F_32_32_1_1[] = { 32, 32, 1, 1 };
  static const uint32_t buff_info__shape_16_16_3_3[] = { 16, 3, 3, 16 };
  static const uint32_t buff_info__mem_shape_L_16_16_3_3[] = { 16, 3, 3, 16 };
  static const uint32_t buff_info__shape_32_48_1_1[] = { 32, 1, 1, 48 };
  static const uint32_t buff_info__mem_shape_F_32_48_1_1[] = { 32, 48, 1, 1 };
  static const uint32_t buff_info__shape_64_32_3_3[] = { 64, 3, 3, 32 };
  static const uint32_t buff_info__mem_shape_L_64_32_3_3[] = { 64, 3, 3, 32 };
  static const uint32_t buff_info__shape_64[] = { 1, 1, 64, 1 };
  static const uint32_t buff_info__mem_shape_U_64[] = { 64 };
  static const uint32_t buff_info__shape_64_64_1_1[] = { 64, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_F_64_64_1_1[] = { 64, 64, 1, 1 };
  static const uint32_t buff_info__shape_32_32_3_3[] = { 32, 3, 3, 32 };
  static const uint32_t buff_info__mem_shape_L_32_32_3_3[] = { 32, 3, 3, 32 };
  static const uint32_t buff_info__shape_64_128_1_1[] = { 64, 1, 1, 128 };
  static const uint32_t buff_info__mem_shape_F_64_128_1_1[] = { 64, 128, 1, 1 };
  static const uint32_t buff_info__shape_128_64_3_3[] = { 128, 3, 3, 64 };
  static const uint32_t buff_info__mem_shape_L_128_64_3_3[] = { 128, 3, 3, 64 };
  static const uint32_t buff_info__shape_128[] = { 1, 1, 128, 1 };
  static const uint32_t buff_info__mem_shape_U_128[] = { 128 };
  static const uint32_t buff_info__shape_128_128_1_1[] = { 128, 1, 1, 128 };
  static const uint32_t buff_info__mem_shape_F_128_128_1_1[] = { 128, 128, 1, 1 };
  static const uint32_t buff_info__shape_64_64_3_3[] = { 64, 3, 3, 64 };
  static const uint32_t buff_info__mem_shape_L_64_64_3_3[] = { 64, 3, 3, 64 };
  static const uint32_t buff_info__shape_128_256_1_1[] = { 128, 1, 1, 256 };
  static const uint32_t buff_info__mem_shape_F_128_256_1_1[] = { 128, 256, 1, 1 };
  static const uint32_t buff_info__shape_256_128_3_3[] = { 256, 3, 3, 128 };
  static const uint32_t buff_info__mem_shape_L_256_128_3_3[] = { 256, 3, 3, 128 };
  static const uint32_t buff_info__shape_256[] = { 1, 1, 256, 1 };
  static const uint32_t buff_info__mem_shape_U_256[] = { 256 };
  static const uint32_t buff_info__shape_256_256_1_1[] = { 256, 1, 1, 256 };
  static const uint32_t buff_info__mem_shape_F_256_256_1_1[] = { 256, 256, 1, 1 };
  static const uint32_t buff_info__shape_128_128_3_3[] = { 128, 3, 3, 128 };
  static const uint32_t buff_info__mem_shape_L_128_128_3_3[] = { 128, 3, 3, 128 };
  static const uint32_t buff_info__shape_256_384_1_1[] = { 256, 1, 1, 384 };
  static const uint32_t buff_info__mem_shape_F_256_384_1_1[] = { 256, 384, 1, 1 };
  static const uint32_t buff_info__shape_256_512_1_1[] = { 256, 1, 1, 512 };
  static const uint32_t buff_info__mem_shape_F_256_512_1_1[] = { 256, 512, 1, 1 };
  static const uint32_t buff_info__shape_128_384_1_1[] = { 128, 1, 1, 384 };
  static const uint32_t buff_info__mem_shape_F_128_384_1_1[] = { 128, 384, 1, 1 };
  static const uint32_t buff_info__shape_128_192_1_1[] = { 128, 1, 1, 192 };
  static const uint32_t buff_info__mem_shape_F_128_192_1_1[] = { 128, 192, 1, 1 };
  static const uint32_t buff_info__shape_64_192_1_1[] = { 64, 1, 1, 192 };
  static const uint32_t buff_info__mem_shape_F_64_192_1_1[] = { 64, 192, 1, 1 };
  static const uint32_t buff_info__shape_64_96_1_1[] = { 64, 1, 1, 96 };
  static const uint32_t buff_info__mem_shape_F_64_96_1_1[] = { 64, 96, 1, 1 };
  static const uint32_t buff_info__shape_1_64_1_1[] = { 1, 1, 1, 64 };
  static const uint32_t buff_info__mem_shape_F_1_64_1_1[] = { 1, 64, 1, 1 };
  static const uint32_t buff_info__shape_1[] = { 1, 1, 1, 1 };
  static const uint32_t buff_info__mem_shape_U_1[] = { 1 };
  static const uint32_t buff_info__shape_64_128_3_3[] = { 64, 3, 3, 128 };
  static const uint32_t buff_info__mem_shape_L_64_128_3_3[] = { 64, 3, 3, 128 };
  static const uint32_t buff_info__shape_64_256_3_3[] = { 64, 3, 3, 256 };
  static const uint32_t buff_info__mem_shape_L_64_256_3_3[] = { 64, 3, 3, 256 };
  static const uint32_t buff_info__shape_1_16_1_1[] = { 1, 1, 1, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_1[] = { 1, 16, 1, 1 };
  static const uint32_t buff_info__shape_1_4_2100[] = { 1, 4, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_4_2100[] = { 1, 4, 2100 };
  static const uint32_t buff_info__shape_1_2_2100[] = { 1, 2, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_2_2100[] = { 1, 2, 2100 };
  static const uint32_t buff_info__shape_8[] = { 1, 1, 8, 1 };
  static const uint32_t buff_info__mem_shape_U_8[] = { 8 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_1_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_3_320_320,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_320_320,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Conv2D_2_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12066432,
      .offset_end = 12068160,
      .offset_limit = 12068224,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_16_3_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16_3_3_3,
    },
    {
      .name = "Conv2D_2_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090496,
      .offset_end = 12090560,
      .offset_limit = 12090624,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_16,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16,
    },
    {
      .name = "Conv2D_5_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11920192,
      .offset_end = 11938624,
      .offset_limit = 11938688,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_32_16_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_16_3_3,
    },
    {
      .name = "Conv2D_5_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089152,
      .offset_end = 12089280,
      .offset_limit = 12089344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_8_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12062336,
      .offset_end = 12066432,
      .offset_limit = 12066496,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_F_32_32_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_1_1,
    },
    {
      .name = "Conv2D_8_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089280,
      .offset_end = 12089408,
      .offset_limit = 12089472,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_12_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12037760,
      .offset_end = 12046976,
      .offset_limit = 12047040,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_16_16_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16_16_3_3,
    },
    {
      .name = "Conv2D_12_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090560,
      .offset_end = 12090624,
      .offset_limit = 12090688,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_16,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16,
    },
    {
      .name = "Conv2D_15_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12046976,
      .offset_end = 12056192,
      .offset_limit = 12056256,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_16_16_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16_16_3_3,
    },
    {
      .name = "Conv2D_15_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090624,
      .offset_end = 12090688,
      .offset_limit = 12090752,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_16,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16,
    },
    {
      .name = "Conv2D_21_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12056192,
      .offset_end = 12062336,
      .offset_limit = 12062400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_F_32_48_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_48_1_1,
    },
    {
      .name = "Conv2D_21_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089408,
      .offset_end = 12089536,
      .offset_limit = 12089600,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_24_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11419648,
      .offset_end = 11493376,
      .offset_limit = 11493440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_64_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_32_3_3,
    },
    {
      .name = "Conv2D_24_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12081728,
      .offset_end = 12081984,
      .offset_limit = 12082048,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_27_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11988608,
      .offset_end = 12004992,
      .offset_limit = 12005056,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_64_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_1_1,
    },
    {
      .name = "Conv2D_27_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12081984,
      .offset_end = 12082240,
      .offset_limit = 12082304,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_31_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11608064,
      .offset_end = 11644928,
      .offset_limit = 11644992,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_31_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089536,
      .offset_end = 12089664,
      .offset_limit = 12089728,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_34_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11644928,
      .offset_end = 11681792,
      .offset_limit = 11681856,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_34_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089664,
      .offset_end = 12089792,
      .offset_limit = 12089856,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_38_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11681792,
      .offset_end = 11718656,
      .offset_limit = 11718720,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_38_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089792,
      .offset_end = 12089920,
      .offset_limit = 12089984,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_41_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11718656,
      .offset_end = 11755520,
      .offset_limit = 11755584,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_41_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12089920,
      .offset_end = 12090048,
      .offset_limit = 12090112,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_47_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11862848,
      .offset_end = 11895616,
      .offset_limit = 11895680,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_F_64_128_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_128_1_1,
    },
    {
      .name = "Conv2D_47_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12082240,
      .offset_end = 12082496,
      .offset_limit = 12082560,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_50_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 7012352,
      .offset_end = 7307264,
      .offset_limit = 7307328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_128_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_64_3_3,
    },
    {
      .name = "Conv2D_50_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12076864,
      .offset_end = 12077376,
      .offset_limit = 12077440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_53_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11493376,
      .offset_end = 11558912,
      .offset_limit = 11558976,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_F_128_128_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_1_1,
    },
    {
      .name = "Conv2D_53_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12077376,
      .offset_end = 12077888,
      .offset_limit = 12077952,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_57_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8945664,
      .offset_end = 9093120,
      .offset_limit = 9093184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_57_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12084544,
      .offset_end = 12084800,
      .offset_limit = 12084864,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_60_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9093120,
      .offset_end = 9240576,
      .offset_limit = 9240640,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_60_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12084800,
      .offset_end = 12085056,
      .offset_limit = 12085120,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_64_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9240576,
      .offset_end = 9388032,
      .offset_limit = 9388096,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_64_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12085056,
      .offset_end = 12085312,
      .offset_limit = 12085376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_67_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9388032,
      .offset_end = 9535488,
      .offset_limit = 9535552,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_67_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12085312,
      .offset_end = 12085568,
      .offset_limit = 12085632,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_73_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10862592,
      .offset_end = 10993664,
      .offset_limit = 10993728,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_F_128_256_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_256_1_1,
    },
    {
      .name = "Conv2D_73_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12077888,
      .offset_end = 12078400,
      .offset_limit = 12078464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_76_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 0,
      .offset_end = 1179648,
      .offset_limit = 1179712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_256_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_128_3_3,
    },
    {
      .name = "Conv2D_76_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12068160,
      .offset_end = 12069184,
      .offset_limit = 12069248,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_79_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 7897088,
      .offset_end = 8159232,
      .offset_limit = 8159296,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_F_256_256_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_256_1_1,
    },
    {
      .name = "Conv2D_79_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12073280,
      .offset_end = 12074304,
      .offset_limit = 12074368,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_83_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 1179648,
      .offset_end = 1769472,
      .offset_limit = 1769536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_128_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_3_3,
    },
    {
      .name = "Conv2D_83_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12074304,
      .offset_end = 12074816,
      .offset_limit = 12074880,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_86_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 1769472,
      .offset_end = 2359296,
      .offset_limit = 2359360,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_128_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_3_3,
    },
    {
      .name = "Conv2D_86_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12074816,
      .offset_end = 12075328,
      .offset_limit = 12075392,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_92_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 5832704,
      .offset_end = 6225920,
      .offset_limit = 6225984,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_F_256_384_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_384_1_1,
    },
    {
      .name = "Conv2D_92_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12070208,
      .offset_end = 12071232,
      .offset_limit = 12071296,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_95_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10993664,
      .offset_end = 11124736,
      .offset_limit = 11124800,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_F_128_256_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_256_1_1,
    },
    {
      .name = "Conv2D_95_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12080448,
      .offset_end = 12080960,
      .offset_limit = 12081024,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_102_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 5308416,
      .offset_end = 5832704,
      .offset_limit = 5832768,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_F_256_512_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_512_1_1,
    },
    {
      .name = "Conv2D_102_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12069184,
      .offset_end = 12070208,
      .offset_limit = 12070272,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_107_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8159232,
      .offset_end = 8355840,
      .offset_limit = 8355904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_F_128_384_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_384_1_1,
    },
    {
      .name = "Conv2D_107_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12078400,
      .offset_end = 12078912,
      .offset_limit = 12078976,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_111_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9535488,
      .offset_end = 9682944,
      .offset_limit = 9683008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_111_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12085568,
      .offset_end = 12085824,
      .offset_limit = 12085888,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_114_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9682944,
      .offset_end = 9830400,
      .offset_limit = 9830464,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_114_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12085824,
      .offset_end = 12086080,
      .offset_limit = 12086144,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_119_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11124736,
      .offset_end = 11223040,
      .offset_limit = 11223104,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_F_128_192_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_192_1_1,
    },
    {
      .name = "Conv2D_119_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12078912,
      .offset_end = 12079424,
      .offset_limit = 12079488,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_124_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11558912,
      .offset_end = 11608064,
      .offset_limit = 11608128,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_F_64_192_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_192_1_1,
    },
    {
      .name = "Conv2D_124_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12082496,
      .offset_end = 12082752,
      .offset_limit = 12082816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_128_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11755520,
      .offset_end = 11792384,
      .offset_limit = 11792448,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_128_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090048,
      .offset_end = 12090176,
      .offset_limit = 12090240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_131_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11792384,
      .offset_end = 11829248,
      .offset_limit = 11829312,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_32_32_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32_32_3_3,
    },
    {
      .name = "Conv2D_131_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090176,
      .offset_end = 12090304,
      .offset_limit = 12090368,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_32,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_32,
    },
    {
      .name = "Conv2D_136_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11895616,
      .offset_end = 11920192,
      .offset_limit = 11920256,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_F_64_96_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_96_1_1,
    },
    {
      .name = "Conv2D_136_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12082752,
      .offset_end = 12083008,
      .offset_limit = 12083072,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_139_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8355840,
      .offset_end = 8503296,
      .offset_limit = 8503360,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_139_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12083008,
      .offset_end = 12083264,
      .offset_limit = 12083328,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_142_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8503296,
      .offset_end = 8650752,
      .offset_limit = 8650816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_142_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12083264,
      .offset_end = 12083520,
      .offset_limit = 12083584,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_145_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12088384,
      .offset_end = 12088640,
      .offset_limit = 12088704,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_1_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_1_1,
    },
    {
      .name = "Conv2D_145_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090816,
      .offset_end = 12090820,
      .offset_limit = 12090888,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_146_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8650752,
      .offset_end = 8798208,
      .offset_limit = 8798272,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_146_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12083520,
      .offset_end = 12083776,
      .offset_limit = 12083840,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_149_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 8798208,
      .offset_end = 8945664,
      .offset_limit = 8945728,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_149_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12083776,
      .offset_end = 12084032,
      .offset_limit = 12084096,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_152_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11972224,
      .offset_end = 11988608,
      .offset_limit = 11988672,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_64_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_1_1,
    },
    {
      .name = "Conv2D_152_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12081472,
      .offset_end = 12081728,
      .offset_limit = 12081792,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_155_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9830400,
      .offset_end = 9977856,
      .offset_limit = 9977920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_155_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12086080,
      .offset_end = 12086336,
      .offset_limit = 12086400,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_159_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11223040,
      .offset_end = 11321344,
      .offset_limit = 11321408,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_F_128_192_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_192_1_1,
    },
    {
      .name = "Conv2D_159_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12079424,
      .offset_end = 12079936,
      .offset_limit = 12080000,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_163_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 9977856,
      .offset_end = 10125312,
      .offset_limit = 10125376,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_163_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12086336,
      .offset_end = 12086592,
      .offset_limit = 12086656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_166_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10125312,
      .offset_end = 10272768,
      .offset_limit = 10272832,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_166_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12086592,
      .offset_end = 12086848,
      .offset_limit = 12086912,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_171_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11321344,
      .offset_end = 11419648,
      .offset_limit = 11419712,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_F_128_192_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_192_1_1,
    },
    {
      .name = "Conv2D_171_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12079936,
      .offset_end = 12080448,
      .offset_limit = 12080512,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_174_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 7307264,
      .offset_end = 7602176,
      .offset_limit = 7602240,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_64_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_128_3_3,
    },
    {
      .name = "Conv2D_174_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12084032,
      .offset_end = 12084288,
      .offset_limit = 12084352,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_177_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10272768,
      .offset_end = 10420224,
      .offset_limit = 10420288,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_177_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12086848,
      .offset_end = 12087104,
      .offset_limit = 12087168,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_180_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12088640,
      .offset_end = 12088896,
      .offset_limit = 12088960,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_1_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_1_1,
    },
    {
      .name = "Conv2D_180_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090832,
      .offset_end = 12090836,
      .offset_limit = 12090904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_181_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 7602176,
      .offset_end = 7897088,
      .offset_limit = 7897152,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_64_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_128_3_3,
    },
    {
      .name = "Conv2D_181_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12084288,
      .offset_end = 12084544,
      .offset_limit = 12084608,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_184_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10420224,
      .offset_end = 10567680,
      .offset_limit = 10567744,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_184_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12087104,
      .offset_end = 12087360,
      .offset_limit = 12087424,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_187_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12004992,
      .offset_end = 12021376,
      .offset_limit = 12021440,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_64_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_1_1,
    },
    {
      .name = "Conv2D_187_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12087872,
      .offset_end = 12088128,
      .offset_limit = 12088192,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_190_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 2359296,
      .offset_end = 2949120,
      .offset_limit = 2949184,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_128_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_3_3,
    },
    {
      .name = "Conv2D_190_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12075328,
      .offset_end = 12075840,
      .offset_limit = 12075904,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_194_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 6225920,
      .offset_end = 6619136,
      .offset_limit = 6619200,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_F_256_384_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_384_1_1,
    },
    {
      .name = "Conv2D_194_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12071232,
      .offset_end = 12072256,
      .offset_limit = 12072320,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_198_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 2949120,
      .offset_end = 3538944,
      .offset_limit = 3539008,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_128_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_3_3,
    },
    {
      .name = "Conv2D_198_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12075840,
      .offset_end = 12076352,
      .offset_limit = 12076416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_201_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 3538944,
      .offset_end = 4128768,
      .offset_limit = 4128832,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_128_128_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128_128_3_3,
    },
    {
      .name = "Conv2D_201_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12076352,
      .offset_end = 12076864,
      .offset_limit = 12076928,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_128,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_128,
    },
    {
      .name = "Conv2D_206_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 6619136,
      .offset_end = 7012352,
      .offset_limit = 7012416,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_F_256_384_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256_384_1_1,
    },
    {
      .name = "Conv2D_206_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12072256,
      .offset_end = 12073280,
      .offset_limit = 12073344,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_256,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_256,
    },
    {
      .name = "Conv2D_209_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 4128768,
      .offset_end = 4718592,
      .offset_limit = 4718656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_64_256_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_256_3_3,
    },
    {
      .name = "Conv2D_209_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12080960,
      .offset_end = 12081216,
      .offset_limit = 12081280,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_212_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10567680,
      .offset_end = 10715136,
      .offset_limit = 10715200,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_212_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12087360,
      .offset_end = 12087616,
      .offset_limit = 12087680,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_215_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12088896,
      .offset_end = 12089152,
      .offset_limit = 12089216,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_1_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_1_1,
    },
    {
      .name = "Conv2D_215_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090848,
      .offset_end = 12090852,
      .offset_limit = 12090920,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "Conv2D_216_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 4718592,
      .offset_end = 5308416,
      .offset_limit = 5308480,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_64_256_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_256_3_3,
    },
    {
      .name = "Conv2D_216_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12081216,
      .offset_end = 12081472,
      .offset_limit = 12081536,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_219_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 10715136,
      .offset_end = 10862592,
      .offset_limit = 10862656,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_64_64_3_3,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_3_3,
    },
    {
      .name = "Conv2D_219_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12087616,
      .offset_end = 12087872,
      .offset_limit = 12087936,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_222_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12021376,
      .offset_end = 12037760,
      .offset_limit = 12037824,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_F_64_64_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64_64_1_1,
    },
    {
      .name = "Conv2D_222_bias",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12088128,
      .offset_end = 12088384,
      .offset_limit = 12088448,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_64,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_64,
    },
    {
      .name = "Conv2D_232_weights",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090688,
      .offset_end = 12090752,
      .offset_limit = 12090816,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_F_1_16_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_1,
    },
    {
      .name = "Mul_237_param1",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11829248,
      .offset_end = 11862848,
      .offset_limit = 11862912,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_4_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_4_2100,
    },
    {
      .name = "Add_239_param0",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11938624,
      .offset_end = 11955424,
      .offset_limit = 11955488,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Sub_241_param0",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 11955424,
      .offset_end = 11972224,
      .offset_limit = 11972288,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Div_244_param1",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090800,
      .offset_end = 12090804,
      .offset_limit = 12090872,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "MaxPool_98_decomposed_pad_pads_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090304,
      .offset_end = 12090368,
      .offset_limit = 12090432,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "MaxPool_98_decomposed_pad_const_value_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090752,
      .offset_end = 12090756,
      .offset_limit = 12090824,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "MaxPool_99_decomposed_pad_pads_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090368,
      .offset_end = 12090432,
      .offset_limit = 12090496,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "MaxPool_99_decomposed_pad_const_value_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090768,
      .offset_end = 12090772,
      .offset_limit = 12090840,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
    {
      .name = "MaxPool_100_decomposed_pad_pads_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090432,
      .offset_end = 12090496,
      .offset_limit = 12090560,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_8,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 63,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_INT64,
      .nbits = 64,
      .ndims = 4,
      .shape = buff_info__shape_8,
    },
    {
      .name = "MaxPool_100_decomposed_pad_const_value_Out",
      .addr_base = {(unsigned char *)(0x71000000UL) /* Equivalent hex address = 0x71000000UL */},
      .offset_start = 12090784,
      .offset_end = 12090788,
      .offset_limit = 12090856,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1,
      .mem_ndims = 1,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_yolo_deer(void)
{
  static const uint32_t buff_info__shape_1_5_2100[] = { 1, 5, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_5_2100[] = { 1, 5, 2100 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Concat_245_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 42000,
      .offset_limit = 42064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 254,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_5_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_5_2100,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_yolo_deer(void)
{
  static const uint32_t buff_info__shape_1_3_320_320[] = { 1, 320, 320, 3 };
  static const uint32_t buff_info__mem_shape_L_1_3_320_320[] = { 1, 320, 320, 3 };
  static const uint32_t buff_info__shape_1_16_160_160[] = { 1, 160, 160, 16 };
  static const uint32_t buff_info__mem_shape_L_1_16_160_160[] = { 1, 160, 160, 16 };
  static const uint32_t buff_info__shape_1_32_80_80[] = { 1, 80, 80, 32 };
  static const uint32_t buff_info__mem_shape_L_1_32_80_80[] = { 1, 80, 80, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_80_80[] = { 1, 32, 80, 80 };
  static const uint32_t buff_info__shape_1_16_80_80[] = { 1, 80, 80, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_80_80[] = { 1, 16, 80, 80 };
  static const uint32_t buff_info__mem_shape_L_1_16_80_80[] = { 1, 80, 80, 16 };
  static const uint32_t buff_info__shape_1_48_80_80[] = { 1, 80, 80, 48 };
  static const uint32_t buff_info__mem_shape_L_1_48_80_80[] = { 1, 80, 80, 48 };
  static const uint32_t buff_info__shape_1_64_40_40[] = { 1, 40, 40, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_40_40[] = { 1, 40, 40, 64 };
  static const uint32_t buff_info__mem_shape_F_1_64_40_40[] = { 1, 64, 40, 40 };
  static const uint32_t buff_info__shape_1_32_40_40[] = { 1, 40, 40, 32 };
  static const uint32_t buff_info__mem_shape_F_1_32_40_40[] = { 1, 32, 40, 40 };
  static const uint32_t buff_info__mem_shape_L_1_32_40_40[] = { 1, 40, 40, 32 };
  static const uint32_t buff_info__shape_1_128_40_40[] = { 1, 40, 40, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_40_40[] = { 1, 40, 40, 128 };
  static const uint32_t buff_info__shape_1_128_20_20[] = { 1, 20, 20, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_20_20[] = { 1, 20, 20, 128 };
  static const uint32_t buff_info__mem_shape_F_1_128_20_20[] = { 1, 128, 20, 20 };
  static const uint32_t buff_info__shape_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t buff_info__mem_shape_F_1_64_20_20[] = { 1, 64, 20, 20 };
  static const uint32_t buff_info__mem_shape_L_1_64_20_20[] = { 1, 20, 20, 64 };
  static const uint32_t buff_info__shape_1_256_20_20[] = { 1, 20, 20, 256 };
  static const uint32_t buff_info__mem_shape_L_1_256_20_20[] = { 1, 20, 20, 256 };
  static const uint32_t buff_info__shape_1_256_10_10[] = { 1, 10, 10, 256 };
  static const uint32_t buff_info__mem_shape_L_1_256_10_10[] = { 1, 10, 10, 256 };
  static const uint32_t buff_info__mem_shape_F_1_256_10_10[] = { 1, 256, 10, 10 };
  static const uint32_t buff_info__shape_1_128_10_10[] = { 1, 10, 10, 128 };
  static const uint32_t buff_info__mem_shape_F_1_128_10_10[] = { 1, 128, 10, 10 };
  static const uint32_t buff_info__mem_shape_L_1_128_10_10[] = { 1, 10, 10, 128 };
  static const uint32_t buff_info__shape_1_384_10_10[] = { 1, 10, 10, 384 };
  static const uint32_t buff_info__mem_shape_L_1_384_10_10[] = { 1, 10, 10, 384 };
  static const uint32_t buff_info__shape_1_128_14_14[] = { 1, 14, 14, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_14_14[] = { 1, 14, 14, 128 };
  static const uint32_t buff_info__shape_1_128_12_12[] = { 1, 12, 12, 128 };
  static const uint32_t buff_info__mem_shape_L_1_128_12_12[] = { 1, 12, 12, 128 };
  static const uint32_t buff_info__shape_1_512_10_10[] = { 1, 10, 10, 512 };
  static const uint32_t buff_info__mem_shape_L_1_512_10_10[] = { 1, 10, 10, 512 };
  static const uint32_t buff_info__shape_1_1024_10_10[] = { 1, 10, 10, 1024 };
  static const uint32_t buff_info__mem_shape_L_1_1024_10_10[] = { 1, 10, 10, 1024 };
  static const uint32_t buff_info__shape_1_384_20_20[] = { 1, 20, 20, 384 };
  static const uint32_t buff_info__mem_shape_L_1_384_20_20[] = { 1, 20, 20, 384 };
  static const uint32_t buff_info__shape_1_192_20_20[] = { 1, 20, 20, 192 };
  static const uint32_t buff_info__mem_shape_L_1_192_20_20[] = { 1, 20, 20, 192 };
  static const uint32_t buff_info__shape_1_512_20_20[] = { 1, 20, 20, 512 };
  static const uint32_t buff_info__mem_shape_L_1_512_20_20[] = { 1, 20, 20, 512 };
  static const uint32_t buff_info__shape_1_192_40_40[] = { 1, 40, 40, 192 };
  static const uint32_t buff_info__mem_shape_L_1_192_40_40[] = { 1, 40, 40, 192 };
  static const uint32_t buff_info__shape_1_96_40_40[] = { 1, 40, 40, 96 };
  static const uint32_t buff_info__mem_shape_L_1_96_40_40[] = { 1, 40, 40, 96 };
  static const uint32_t buff_info__shape_1_1_40_40[] = { 1, 40, 40, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_40_40[] = { 1, 1, 40, 40 };
  static const uint32_t buff_info__shape_1_65_40_40[] = { 1, 40, 40, 65 };
  static const uint32_t buff_info__mem_shape_L_1_65_40_40[] = { 1, 40, 40, 65 };
  static const uint32_t buff_info__shape_1_65_1600[] = { 1, 65, 1600, 1 };
  static const uint32_t buff_info__mem_shape_F_1_65_1600[] = { 1, 65, 1600 };
  static const uint32_t buff_info__shape_1_1_20_20[] = { 1, 20, 20, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_20_20[] = { 1, 1, 20, 20 };
  static const uint32_t buff_info__shape_1_65_20_20[] = { 1, 20, 20, 65 };
  static const uint32_t buff_info__mem_shape_L_1_65_20_20[] = { 1, 20, 20, 65 };
  static const uint32_t buff_info__shape_1_65_400[] = { 1, 65, 400, 1 };
  static const uint32_t buff_info__mem_shape_F_1_65_400[] = { 1, 65, 400 };
  static const uint32_t buff_info__shape_1_64_10_10[] = { 1, 10, 10, 64 };
  static const uint32_t buff_info__mem_shape_L_1_64_10_10[] = { 1, 10, 10, 64 };
  static const uint32_t buff_info__shape_1_1_10_10[] = { 1, 10, 10, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_10_10[] = { 1, 1, 10, 10 };
  static const uint32_t buff_info__shape_1_65_10_10[] = { 1, 10, 10, 65 };
  static const uint32_t buff_info__mem_shape_L_1_65_10_10[] = { 1, 10, 10, 65 };
  static const uint32_t buff_info__shape_1_65_100[] = { 1, 65, 100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_65_100[] = { 1, 65, 100 };
  static const uint32_t buff_info__shape_1_65_2100[] = { 1, 65, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_65_2100[] = { 1, 65, 2100 };
  static const uint32_t buff_info__shape_1_64_2100[] = { 1, 64, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_64_2100[] = { 1, 64, 2100 };
  static const uint32_t buff_info__shape_1_1_2100[] = { 1, 1, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_2100[] = { 1, 1, 2100 };
  static const uint32_t buff_info__shape_1_4_16_2100[] = { 1, 16, 2100, 4 };
  static const uint32_t buff_info__mem_shape_F_1_4_16_2100[] = { 1, 4, 16, 2100 };
  static const uint32_t buff_info__shape_1_16_4_2100[] = { 1, 4, 2100, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_4_2100[] = { 1, 16, 4, 2100 };
  static const uint32_t buff_info__mem_shape_L_1_16_4_2100[] = { 1, 4, 2100, 16 };
  static const uint32_t buff_info__shape_1_1_1_257[] = { 1, 1, 257, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_1_257[] = { 1, 1, 1, 257 };
  static const uint32_t buff_info__shape_1_1_4_2100[] = { 1, 4, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_1_4_2100[] = { 1, 1, 4, 2100 };
  static const uint32_t buff_info__shape_1_4_2100[] = { 1, 4, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_4_2100[] = { 1, 4, 2100 };
  static const uint32_t buff_info__shape_1_2_2100[] = { 1, 2, 2100, 1 };
  static const uint32_t buff_info__mem_shape_F_1_2_2100[] = { 1, 2, 2100 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_1_out_0_inserted_out473",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1638400,
      .offset_end = 2867200,
      .offset_limit = 2867264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 3,
      .mem_shape = buff_info__mem_shape_L_1_3_320_320,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_3_320_320,
    },
    {
      .name = "Conv2D_2_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_160_160,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_160_160,
    },
    {
      .name = "Sigmoid_3_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_160_160,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_160_160,
    },
    {
      .name = "Mul_4_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1638400,
      .offset_end = 3276800,
      .offset_limit = 3276864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 4,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_160_160,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_160_160,
    },
    {
      .name = "Conv2D_5_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 5,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Sigmoid_6_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 819200,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 6,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Mul_7_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1638400,
      .offset_end = 2457600,
      .offset_limit = 2457664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 7,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Conv2D_8_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 8,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Sigmoid_9_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 819200,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 9,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Mul_10_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1638400,
      .offset_end = 2457600,
      .offset_limit = 2457664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 10,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Mul_10_out_0_inserted_out475",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 11,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Slice_19_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 819200,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "__ATONN_bucket_477_p1",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1228800,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 12,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Slice_19_out_0_inserted_out480",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Slice_11_out_0_inserted_out481",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 409600,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 13,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Conv2D_12_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 14,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Sigmoid_13_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 15,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Mul_14_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 16,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Conv2D_15_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 17,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Sigmoid_16_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 18,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Mul_17_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 19,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Add_18_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 819200,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 20,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_80_80,
    },
    {
      .name = "Concat_20_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 21,
      .batch = 48,
      .mem_shape = buff_info__mem_shape_L_1_48_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_48_80_80,
    },
    {
      .name = "Conv2D_21_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1228800,
      .offset_end = 2048000,
      .offset_limit = 2048064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 22,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Sigmoid_22_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2048000,
      .offset_end = 2867200,
      .offset_limit = 2867264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 23,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Mul_23_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 24,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_80_80,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_80_80,
    },
    {
      .name = "Conv2D_24_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 25,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_25_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 26,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_26_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 27,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_27_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 28,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_28_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 29,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_29_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 30,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_29_out_0_inserted_out483",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 31,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Slice_45_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "__ATONN_bucket_485_p1",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 32,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Slice_45_out_0_inserted_out488",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Slice_30_out_0_inserted_out489",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 33,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_31_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 34,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_32_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 35,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_33_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 36,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_34_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 37,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_35_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 38,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_36_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 39,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Add_37_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 409600,
      .offset_end = 614400,
      .offset_limit = 614464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 40,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_38_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 41,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_39_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 42,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_40_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 43,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_41_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 44,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_42_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 45,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_43_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 46,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Add_44_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 614400,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 47,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Concat_46_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 819200,
      .offset_end = 1638400,
      .offset_limit = 1638464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 48,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_40_40,
    },
    {
      .name = "Conv2D_47_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 49,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_48_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 50,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_49_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 51,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_50_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 52,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_51_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 53,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_52_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 54,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Conv2D_53_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 55,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_54_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 56,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_55_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 57,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_55_out_0_inserted_out491",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 58,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Slice_71_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "__ATONN_bucket_493_p1",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 59,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_71_out_0_inserted_out496",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 60,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_56_out_0_inserted_out497",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 60,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_57_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 61,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_58_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 62,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_59_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 63,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_60_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 64,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_61_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 65,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_62_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 66,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Add_63_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 67,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_64_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 68,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_65_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 69,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_66_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 70,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_67_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 71,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_68_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 72,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_69_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 73,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Add_70_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 74,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Concat_72_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 75,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_20_20,
    },
    {
      .name = "Conv2D_73_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 76,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_74_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 77,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_75_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 78,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Conv2D_76_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 79,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_77_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 80,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_78_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 81,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Conv2D_79_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 82,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_80_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 83,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_81_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 84,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_81_out_0_inserted_out499",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 85,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Slice_90_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 86,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "__ATONN_bucket_501_p1",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 86,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Slice_90_out_0_inserted_out504",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 87,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Slice_82_out_0_inserted_out505",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 87,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Conv2D_83_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 88,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_84_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 89,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_85_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 256000,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 90,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Conv2D_86_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 91,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_87_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 92,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_88_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 256000,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 93,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Add_89_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 153600,
      .offset_limit = 153664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 94,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Concat_91_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 95,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_10_10,
    },
    {
      .name = "Conv2D_92_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 96,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_93_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 97,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_94_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 98,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Conv2D_95_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 99,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_96_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 256000,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 100,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_97_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 101,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "MaxPool_98_decomposed_pad_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 305152,
      .offset_limit = 305216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 102,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_14_14,
    },
    {
      .name = "MaxPool_98_decomposed_0_out_1",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 305152,
      .offset_end = 378880,
      .offset_limit = 378944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 103,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_12_12,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_12_12,
    },
    {
      .name = "MaxPool_98_decomposed_1_out_2",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 104,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "MaxPool_99_decomposed_pad_out_3",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 305152,
      .offset_limit = 305216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 105,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_14_14,
    },
    {
      .name = "MaxPool_99_decomposed_0_out_4",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 305152,
      .offset_end = 378880,
      .offset_limit = 378944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 106,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_12_12,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_12_12,
    },
    {
      .name = "MaxPool_99_decomposed_1_out_5",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 153600,
      .offset_limit = 153664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 107,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "MaxPool_100_decomposed_pad_out_6",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 305152,
      .offset_limit = 305216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 108,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_14_14,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_14_14,
    },
    {
      .name = "MaxPool_100_decomposed_0_out_7",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 305152,
      .offset_end = 378880,
      .offset_limit = 378944,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 109,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_12_12,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_12_12,
    },
    {
      .name = "MaxPool_100_decomposed_1_out_8",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 110,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Concat_101_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 111,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_10_10,
    },
    {
      .name = "Conv2D_102_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 112,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_103_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 113,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_104_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1433600,
      .offset_end = 1536000,
      .offset_limit = 1536064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 114,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Resize_105_resize_NN_expansion_concat_9_out_10",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 115,
      .batch = 1024,
      .mem_shape = buff_info__mem_shape_L_1_1024_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1024_10_10,
    },
    {
      .name = "Resize_105_resize_NN_expansion_concat_9_out_12",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 116,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_20_20,
    },
    {
      .name = "Concat_106_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 614400,
      .offset_limit = 614464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 117,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_20_20,
    },
    {
      .name = "Conv2D_107_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 118,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_108_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 119,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_109_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 120,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_109_out_0_inserted_out507",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 121,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Slice_117_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 122,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "__ATONN_bucket_509_p1",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 122,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_117_out_0_inserted_out511",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 123,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_110_out_0_inserted_out512",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 123,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_111_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 124,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_112_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 125,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_113_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 126,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_114_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 127,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_115_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 128,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_116_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 129,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Concat_118_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 130,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_L_1_192_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_192_20_20,
    },
    {
      .name = "Conv2D_119_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 131,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_120_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 132,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_121_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 1228800,
      .offset_end = 1433600,
      .offset_limit = 1433664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 133,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Resize_122_resize_NN_expansion_concat_13_out_14",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 819200,
      .offset_limit = 819264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 134,
      .batch = 512,
      .mem_shape = buff_info__mem_shape_L_1_512_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_512_20_20,
    },
    {
      .name = "Resize_122_resize_NN_expansion_concat_13_out_16",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 1458176,
      .offset_end = 2277376,
      .offset_limit = 2277440,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 135,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_40_40,
    },
    {
      .name = "Concat_123_out_0",
      .addr_base = {(unsigned char *)(0x90000000UL) /* Equivalent hex address = 0x90000000UL */},
      .offset_start = 0,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 136,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_L_1_192_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_192_40_40,
    },
    {
      .name = "Conv2D_124_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 137,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_125_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 138,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_126_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 139,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_126_out_0_inserted_out514",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 140,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Slice_134_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 141,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "__ATONN_bucket_516_p1",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 141,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Slice_134_out_0_inserted_out518",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 142,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Slice_127_out_0_inserted_out519",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 142,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_128_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 143,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_129_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 144,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_130_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 145,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Conv2D_131_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 146,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Sigmoid_132_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 147,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Mul_133_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 409600,
      .offset_end = 614400,
      .offset_limit = 614464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 148,
      .batch = 32,
      .mem_shape = buff_info__mem_shape_L_1_32_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_32_40_40,
    },
    {
      .name = "Concat_135_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 614400,
      .offset_end = 1228800,
      .offset_limit = 1228864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 149,
      .batch = 96,
      .mem_shape = buff_info__mem_shape_L_1_96_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_96_40_40,
    },
    {
      .name = "Conv2D_136_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 150,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_137_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 151,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_138_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 152,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_139_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 153,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_140_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 154,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_141_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 155,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_142_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 156,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_143_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 157,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_144_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 158,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_145_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 416000,
      .offset_end = 422400,
      .offset_limit = 422464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 159,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_40_40,
    },
    {
      .name = "Conv2D_146_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 160,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_147_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 161,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_148_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 162,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_149_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 163,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Sigmoid_150_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 164,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Mul_151_out_0",
      .addr_base = {(unsigned char *)(0x34350000UL) /* Equivalent hex address = 0x34350000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 165,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Conv2D_152_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 166,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_40_40,
    },
    {
      .name = "Concat_153_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 416000,
      .offset_limit = 416064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 167,
      .batch = 65,
      .mem_shape = buff_info__mem_shape_L_1_65_40_40,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_40_40,
    },
    {
      .name = "Conv2D_155_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 168,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_156_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 169,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_157_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 170,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Concat_158_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 171,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_L_1_192_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_192_20_20,
    },
    {
      .name = "Conv2D_159_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 172,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_160_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 173,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_161_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 174,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Reshape_154_out_0",
      .addr_base = {(unsigned char *)(0x34270000UL) /* Equivalent hex address = 0x34270000UL */},
      .offset_start = 0,
      .offset_end = 416000,
      .offset_limit = 416064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 175,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_65_1600,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_1600,
    },
    {
      .name = "Mul_161_out_0_inserted_out521",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 175,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Slice_169_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 176,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "__ATONN_bucket_523_p1",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 176,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_169_out_0_inserted_out525",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 177,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Slice_162_out_0_inserted_out526",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 177,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_163_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 178,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_164_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 179,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_165_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 180,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_166_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 181,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_167_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 182,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_168_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 183,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Concat_170_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 184,
      .batch = 192,
      .mem_shape = buff_info__mem_shape_L_1_192_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_192_20_20,
    },
    {
      .name = "Conv2D_171_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 185,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Sigmoid_172_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 186,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Mul_173_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 187,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_20_20,
    },
    {
      .name = "Conv2D_174_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 188,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_175_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 189,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_176_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 190,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_177_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 191,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_178_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 192,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_179_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 193,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_180_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 409600,
      .offset_end = 411200,
      .offset_limit = 411264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 194,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_20_20,
    },
    {
      .name = "Conv2D_181_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 195,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_182_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 196,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_183_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 197,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_184_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 409600,
      .offset_limit = 409664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 198,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Sigmoid_185_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 199,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Mul_186_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 200,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Conv2D_187_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 201,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_20_20,
    },
    {
      .name = "Concat_188_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 257600,
      .offset_limit = 257664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 202,
      .batch = 65,
      .mem_shape = buff_info__mem_shape_L_1_65_20_20,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_20_20,
    },
    {
      .name = "Conv2D_190_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 203,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_191_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 204,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_192_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 257600,
      .offset_end = 308800,
      .offset_limit = 308864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 205,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Concat_193_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 153600,
      .offset_limit = 153664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 206,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_10_10,
    },
    {
      .name = "Conv2D_194_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 257600,
      .offset_end = 360000,
      .offset_limit = 360064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 207,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_195_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 208,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_196_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 209,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Reshape_189_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 307200,
      .offset_end = 411200,
      .offset_limit = 411264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 210,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_65_400,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_400,
    },
    {
      .name = "Mul_196_out_0_inserted_out528",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 210,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Slice_204_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 211,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "__ATONN_bucket_530_p1",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 211,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Slice_204_out_0_inserted_out532",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 212,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Slice_197_out_0_inserted_out533",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 212,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Conv2D_198_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 213,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_199_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 214,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_200_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 256000,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 215,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Conv2D_201_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 216,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Sigmoid_202_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 256000,
      .offset_limit = 256064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 217,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Mul_203_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 153600,
      .offset_limit = 153664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 218,
      .batch = 128,
      .mem_shape = buff_info__mem_shape_L_1_128_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_128_10_10,
    },
    {
      .name = "Concat_205_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 153600,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 219,
      .batch = 384,
      .mem_shape = buff_info__mem_shape_L_1_384_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_384_10_10,
    },
    {
      .name = "Conv2D_206_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 220,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Sigmoid_207_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 102400,
      .offset_end = 204800,
      .offset_limit = 204864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 221,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Mul_208_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 204800,
      .offset_end = 307200,
      .offset_limit = 307264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 222,
      .batch = 256,
      .mem_shape = buff_info__mem_shape_L_1_256_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_256_10_10,
    },
    {
      .name = "Conv2D_209_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 25600,
      .offset_limit = 25664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 223,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Sigmoid_210_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 25600,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 224,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Mul_211_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 76800,
      .offset_limit = 76864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 225,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Conv2D_212_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 25600,
      .offset_limit = 25664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 226,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Sigmoid_213_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 25600,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 227,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Mul_214_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 76800,
      .offset_limit = 76864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 228,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Conv2D_215_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 411200,
      .offset_end = 411600,
      .offset_limit = 411664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 229,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_10_10,
    },
    {
      .name = "Conv2D_216_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 25600,
      .offset_limit = 25664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 230,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Sigmoid_217_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 25600,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 231,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Mul_218_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 76800,
      .offset_limit = 76864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 232,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Conv2D_219_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 25600,
      .offset_limit = 25664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 233,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Sigmoid_220_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 25600,
      .offset_end = 51200,
      .offset_limit = 51264,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 234,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Mul_221_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 51200,
      .offset_end = 76800,
      .offset_limit = 76864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 235,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Conv2D_222_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 76800,
      .offset_end = 102400,
      .offset_limit = 102464,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 236,
      .batch = 64,
      .mem_shape = buff_info__mem_shape_L_1_64_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_10_10,
    },
    {
      .name = "Concat_223_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 26000,
      .offset_limit = 26064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 237,
      .batch = 65,
      .mem_shape = buff_info__mem_shape_L_1_65_10_10,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_10_10,
    },
    {
      .name = "Reshape_224_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 26000,
      .offset_end = 52000,
      .offset_limit = 52064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 238,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_65_100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_100,
    },
    {
      .name = "Concat_225_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 546000,
      .offset_limit = 546064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 239,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_65_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_65_2100,
    },
    {
      .name = "Slice_228_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2050080,
      .offset_end = 2587680,
      .offset_limit = 2587744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 240,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_64_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_64_2100,
    },
    {
      .name = "__ATONN_bucket_534_p1",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 75600,
      .offset_end = 84000,
      .offset_limit = 84064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 240,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_2100,
    },
    {
      .name = "Reshape_229_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2050080,
      .offset_end = 2587680,
      .offset_limit = 2587744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 240,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_4_16_2100,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_4_16_2100,
    },
    {
      .name = "Transpose_230_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 537600,
      .offset_limit = 537664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 241,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_16_4_2100,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_4_2100,
    },
    {
      .name = "Sigmoid_227_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 75600,
      .offset_end = 84000,
      .offset_limit = 84064,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 242,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_2100,
    },
    {
      .name = "Transpose_230_out_0_inserted_out536",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 2050080,
      .offset_end = 2587680,
      .offset_limit = 2587744,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 243,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_4_2100,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_4_2100,
    },
    {
      .name = "Softmax_231_out_0",
      .addr_base = {(unsigned char *)(0x34100000UL) /* Equivalent hex address = 0x34100000UL */},
      .offset_start = 0,
      .offset_end = 537600,
      .offset_limit = 537664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 244,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_L_1_16_4_2100,
      .mem_ndims = 4,
      .chpos = CHPos_Last,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_4_2100,
    },
    {
      .name = "SCRATCH_Softmax_231_PORT_OUT",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 33600,
      .offset_end = 34628,
      .offset_limit = 34696,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 244,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_1_257,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 31,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FXP,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_1_257,
    },
    {
      .name = "Conv2D_232_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 33600,
      .offset_limit = 33664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 245,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_1_4_2100,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_1_4_2100,
    },
    {
      .name = "Reshape_233_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 33600,
      .offset_limit = 33664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 246,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_4_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_4_2100,
    },
    {
      .name = "Mul_237_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 84000,
      .offset_end = 117600,
      .offset_limit = 117664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 247,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_4_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_4_2100,
    },
    {
      .name = "Slice_240_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 16800,
      .offset_limit = 16864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 248,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "__ATONN_bucket_537_p1",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 16800,
      .offset_end = 33600,
      .offset_limit = 33664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 248,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Add_239_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 84000,
      .offset_end = 100800,
      .offset_limit = 100864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 249,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Sub_241_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 100800,
      .offset_end = 117600,
      .offset_limit = 117664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 250,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Add_243_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 0,
      .offset_end = 16800,
      .offset_limit = 16864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 251,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Div_244_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 42000,
      .offset_end = 58800,
      .offset_limit = 58864,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 252,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = "Sub_242_out_0",
      .addr_base = {(unsigned char *)(0x342e0000UL) /* Equivalent hex address = 0x342e0000UL */},
      .offset_start = 58800,
      .offset_end = 75600,
      .offset_limit = 75664,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 253,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_F_1_2_2100,
      .mem_ndims = 3,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_2_2100,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}


